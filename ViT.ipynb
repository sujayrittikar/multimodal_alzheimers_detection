{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51807170-1d84-4c68-b8ff-6827b6599159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import scipy.ndimage\n",
    "from monai.networks.nets import resnet18\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55fcc886-2bed-428c-b812-0950f5390e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_nifti(nifti_path, target_shape=(128, 128, 128)):\n",
    "    # Normalize intensity to [0,1]\n",
    "    img = (img - np.min(img)) / (np.max(img) - np.min(img) + 1e-8)\n",
    "    # Resize to target shape: \n",
    "    img_resized = scipy.ndimage.zoom(img, np.array(target_shape) / np.array(img.shape), order=1)\n",
    "    return img_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e579606-2ae8-43dd-95f9-9f9f25530b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_files_with_substring(directory, substring):\n",
    "    matching_files = [f for f in os.listdir(directory) if substring in f]\n",
    "    return matching_files\n",
    "\n",
    "def get_nib_image(adni_file_name):\n",
    "    return nib.load(adni_file_name).get_fdata()\n",
    "\n",
    "def visualize_image(nib_image):\n",
    "    plt.imshow(nib_image[:,:,nib_image.shape[2]//2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52f021e5-6aab-4e1c-89d1-6453028f7d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_file_names_for_subject(subject_id, date=None):\n",
    "    os.path.expanduser(\"~/adni_flat_dataset\")\n",
    "    dir_ = \"/home/rittikar-s/adni_flat_dataset\"\n",
    "    files = find_files_with_substring(dir_, subject_id)\n",
    "    if date:\n",
    "        files = [file for file in files if date in file]\n",
    "    file_paths = [f\"{dir_}/{file}\" for file in files]\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f01308a7-4fa8-4e4a-b42a-ccb401de8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"ADNI1_Complete_1Yr_1.5T_1_26_2025.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c4be2a0-6cf7-422c-bb27-23a77776d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets.vitautoenc import ViTAutoEnc\n",
    "\n",
    "vit_model = ViTAutoEnc(in_channels=1, patch_size=(16,16,16), img_size=(128,128,128))\n",
    "\n",
    "def get_vit_embedding(img):\n",
    "    return vit_model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9723c9f4-ec96-41be-b7c8-9f24de00ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NiftiDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, target_shape=(128, 128, 128)):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def preprocess_nifti(self, nifti_path):\n",
    "        img = nib.load(nifti_path).get_fdata()\n",
    "        \n",
    "        # Normalize intensity to [0,1]\n",
    "        img = (img - np.min(img)) / (np.max(img) - np.min(img) + 1e-8)\n",
    "        \n",
    "        # Resize to target shape\n",
    "        img_resized = scipy.ndimage.zoom(img, np.array(self.target_shape) / np.array(img.shape), order=1)\n",
    "        \n",
    "        return torch.tensor(img_resized, dtype=torch.float32).unsqueeze(0)  # Add channel dim\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.preprocess_nifti(self.image_paths[idx])\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        embedding = get_vit_embedding(image.reshape(1,1,128,128,128))\n",
    "        return embedding, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62036a9f-e976-4fd0-aef5-4ea71b88072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_label = {\n",
    "    \"CN\": 0,\n",
    "    \"MCI\": 1,\n",
    "    \"AD\": 2\n",
    "}\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    subject = row[\"Subject\"]\n",
    "    date = row[\"Acq Date\"]\n",
    "    date = date.replace(\"/\", \"-\")\n",
    "    image_path = get_image_file_names_for_subject(subject, date)[0]\n",
    "    image_paths.append(image_path)\n",
    "    labels.append(class_to_label[row[\"Group\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc86ce3e-ef67-4492-b81e-4174b4f8084f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2294"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac5306e8-85f3-4a47-9e2d-048b87bdb7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2294"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ff88cf5-1448-4d1f-aa93-964640bfa9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(image_paths, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6107f004-c758-45c2-a4c9-6c352027cb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Batches: 402, Test Batches: 173\n"
     ]
    }
   ],
   "source": [
    "# Create train & test datasets\n",
    "train_dataset = NiftiDataset(train_paths, train_labels)\n",
    "test_dataset = NiftiDataset(test_paths, test_labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, pin_memory=True)\n",
    "\n",
    "print(f\"Train Batches: {len(train_loader)}, Test Batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "175d94f2-5922-4e14-9b95-f428d1555e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def save_embeddings_hdf5(dataloader, filename):\n",
    "    \"\"\"Save embeddings (from list format) and labels incrementally to an HDF5 file.\"\"\"\n",
    "    with h5py.File(filename, \"w\") as f:\n",
    "        first_batch = True\n",
    "        for i, (embedding_list, label) in enumerate(dataloader):\n",
    "            # Extract the last layer embeddings\n",
    "            embedding_tensor = embedding_list[1][-1]  # Extract final layer embeddings\n",
    "            embedding_tensor = embedding_tensor.cpu().detach()  # Move to CPU\n",
    "            \n",
    "            embedding_numpy = embedding_tensor.numpy()  # Convert to NumPy\n",
    "            label_numpy = label.cpu().numpy()\n",
    "\n",
    "            # Reshape embeddings if needed\n",
    "            embedding_numpy = embedding_numpy.reshape(embedding_numpy.shape[0], -1)  # (4, 1, 512, 768) â†’ (4, 512 * 768)\n",
    "\n",
    "            if first_batch:\n",
    "                # Create expandable datasets with correct shape\n",
    "                f.create_dataset(\"embeddings\", data=embedding_numpy, \n",
    "                                 maxshape=(None, embedding_numpy.shape[1]))  # Now 2D\n",
    "                f.create_dataset(\"labels\", data=label_numpy, maxshape=(None,))\n",
    "                first_batch = False\n",
    "            else:\n",
    "                # Resize and append new embeddings\n",
    "                f[\"embeddings\"].resize((f[\"embeddings\"].shape[0] + embedding_numpy.shape[0]), axis=0)\n",
    "                f[\"embeddings\"][-embedding_numpy.shape[0]:] = embedding_numpy\n",
    "\n",
    "                f[\"labels\"].resize((f[\"labels\"].shape[0] + label_numpy.shape[0]), axis=0)\n",
    "                f[\"labels\"][-label_numpy.shape[0]:] = label_numpy\n",
    "            \n",
    "            print(f\"Saved embeddings for batch: {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a120898e-b143-40a4-a31f-3fabcb857139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings for batch: 1\n",
      "Saved embeddings for batch: 2\n",
      "Saved embeddings for batch: 3\n",
      "Saved embeddings for batch: 4\n",
      "Saved embeddings for batch: 5\n",
      "Saved embeddings for batch: 6\n",
      "Saved embeddings for batch: 7\n",
      "Saved embeddings for batch: 8\n",
      "Saved embeddings for batch: 9\n",
      "Saved embeddings for batch: 10\n",
      "Saved embeddings for batch: 11\n",
      "Saved embeddings for batch: 12\n",
      "Saved embeddings for batch: 13\n",
      "Saved embeddings for batch: 14\n",
      "Saved embeddings for batch: 15\n",
      "Saved embeddings for batch: 16\n",
      "Saved embeddings for batch: 17\n",
      "Saved embeddings for batch: 18\n",
      "Saved embeddings for batch: 19\n",
      "Saved embeddings for batch: 20\n",
      "Saved embeddings for batch: 21\n",
      "Saved embeddings for batch: 22\n",
      "Saved embeddings for batch: 23\n",
      "Saved embeddings for batch: 24\n",
      "Saved embeddings for batch: 25\n",
      "Saved embeddings for batch: 26\n",
      "Saved embeddings for batch: 27\n",
      "Saved embeddings for batch: 28\n",
      "Saved embeddings for batch: 29\n",
      "Saved embeddings for batch: 30\n",
      "Saved embeddings for batch: 31\n",
      "Saved embeddings for batch: 32\n",
      "Saved embeddings for batch: 33\n",
      "Saved embeddings for batch: 34\n",
      "Saved embeddings for batch: 35\n",
      "Saved embeddings for batch: 36\n",
      "Saved embeddings for batch: 37\n",
      "Saved embeddings for batch: 38\n",
      "Saved embeddings for batch: 39\n",
      "Saved embeddings for batch: 40\n",
      "Saved embeddings for batch: 41\n",
      "Saved embeddings for batch: 42\n",
      "Saved embeddings for batch: 43\n",
      "Saved embeddings for batch: 44\n",
      "Saved embeddings for batch: 45\n",
      "Saved embeddings for batch: 46\n",
      "Saved embeddings for batch: 47\n",
      "Saved embeddings for batch: 48\n",
      "Saved embeddings for batch: 49\n",
      "Saved embeddings for batch: 50\n",
      "Saved embeddings for batch: 51\n",
      "Saved embeddings for batch: 52\n",
      "Saved embeddings for batch: 53\n",
      "Saved embeddings for batch: 54\n",
      "Saved embeddings for batch: 55\n",
      "Saved embeddings for batch: 56\n",
      "Saved embeddings for batch: 57\n",
      "Saved embeddings for batch: 58\n",
      "Saved embeddings for batch: 59\n",
      "Saved embeddings for batch: 60\n",
      "Saved embeddings for batch: 61\n",
      "Saved embeddings for batch: 62\n",
      "Saved embeddings for batch: 63\n",
      "Saved embeddings for batch: 64\n",
      "Saved embeddings for batch: 65\n",
      "Saved embeddings for batch: 66\n",
      "Saved embeddings for batch: 67\n",
      "Saved embeddings for batch: 68\n",
      "Saved embeddings for batch: 69\n",
      "Saved embeddings for batch: 70\n",
      "Saved embeddings for batch: 71\n",
      "Saved embeddings for batch: 72\n",
      "Saved embeddings for batch: 73\n",
      "Saved embeddings for batch: 74\n",
      "Saved embeddings for batch: 75\n",
      "Saved embeddings for batch: 76\n",
      "Saved embeddings for batch: 77\n",
      "Saved embeddings for batch: 78\n",
      "Saved embeddings for batch: 79\n",
      "Saved embeddings for batch: 80\n",
      "Saved embeddings for batch: 81\n",
      "Saved embeddings for batch: 82\n",
      "Saved embeddings for batch: 83\n",
      "Saved embeddings for batch: 84\n",
      "Saved embeddings for batch: 85\n",
      "Saved embeddings for batch: 86\n",
      "Saved embeddings for batch: 87\n",
      "Saved embeddings for batch: 88\n",
      "Saved embeddings for batch: 89\n",
      "Saved embeddings for batch: 90\n",
      "Saved embeddings for batch: 91\n",
      "Saved embeddings for batch: 92\n",
      "Saved embeddings for batch: 93\n",
      "Saved embeddings for batch: 94\n",
      "Saved embeddings for batch: 95\n",
      "Saved embeddings for batch: 96\n",
      "Saved embeddings for batch: 97\n",
      "Saved embeddings for batch: 98\n",
      "Saved embeddings for batch: 99\n",
      "Saved embeddings for batch: 100\n",
      "Saved embeddings for batch: 101\n",
      "Saved embeddings for batch: 102\n",
      "Saved embeddings for batch: 103\n",
      "Saved embeddings for batch: 104\n",
      "Saved embeddings for batch: 105\n",
      "Saved embeddings for batch: 106\n",
      "Saved embeddings for batch: 107\n",
      "Saved embeddings for batch: 108\n",
      "Saved embeddings for batch: 109\n",
      "Saved embeddings for batch: 110\n",
      "Saved embeddings for batch: 111\n",
      "Saved embeddings for batch: 112\n",
      "Saved embeddings for batch: 113\n",
      "Saved embeddings for batch: 114\n",
      "Saved embeddings for batch: 115\n",
      "Saved embeddings for batch: 116\n",
      "Saved embeddings for batch: 117\n",
      "Saved embeddings for batch: 118\n",
      "Saved embeddings for batch: 119\n",
      "Saved embeddings for batch: 120\n",
      "Saved embeddings for batch: 121\n",
      "Saved embeddings for batch: 122\n",
      "Saved embeddings for batch: 123\n",
      "Saved embeddings for batch: 124\n",
      "Saved embeddings for batch: 125\n",
      "Saved embeddings for batch: 126\n",
      "Saved embeddings for batch: 127\n",
      "Saved embeddings for batch: 128\n",
      "Saved embeddings for batch: 129\n",
      "Saved embeddings for batch: 130\n",
      "Saved embeddings for batch: 131\n",
      "Saved embeddings for batch: 132\n",
      "Saved embeddings for batch: 133\n",
      "Saved embeddings for batch: 134\n",
      "Saved embeddings for batch: 135\n",
      "Saved embeddings for batch: 136\n",
      "Saved embeddings for batch: 137\n",
      "Saved embeddings for batch: 138\n",
      "Saved embeddings for batch: 139\n",
      "Saved embeddings for batch: 140\n",
      "Saved embeddings for batch: 141\n",
      "Saved embeddings for batch: 142\n",
      "Saved embeddings for batch: 143\n",
      "Saved embeddings for batch: 144\n",
      "Saved embeddings for batch: 145\n",
      "Saved embeddings for batch: 146\n",
      "Saved embeddings for batch: 147\n",
      "Saved embeddings for batch: 148\n",
      "Saved embeddings for batch: 149\n",
      "Saved embeddings for batch: 150\n",
      "Saved embeddings for batch: 151\n",
      "Saved embeddings for batch: 152\n",
      "Saved embeddings for batch: 153\n",
      "Saved embeddings for batch: 154\n",
      "Saved embeddings for batch: 155\n",
      "Saved embeddings for batch: 156\n",
      "Saved embeddings for batch: 157\n",
      "Saved embeddings for batch: 158\n",
      "Saved embeddings for batch: 159\n",
      "Saved embeddings for batch: 160\n",
      "Saved embeddings for batch: 161\n",
      "Saved embeddings for batch: 162\n",
      "Saved embeddings for batch: 163\n",
      "Saved embeddings for batch: 164\n",
      "Saved embeddings for batch: 165\n",
      "Saved embeddings for batch: 166\n",
      "Saved embeddings for batch: 167\n",
      "Saved embeddings for batch: 168\n",
      "Saved embeddings for batch: 169\n",
      "Saved embeddings for batch: 170\n",
      "Saved embeddings for batch: 171\n",
      "Saved embeddings for batch: 172\n",
      "Saved embeddings for batch: 173\n",
      "Saved embeddings for batch: 174\n",
      "Saved embeddings for batch: 175\n",
      "Saved embeddings for batch: 176\n",
      "Saved embeddings for batch: 177\n",
      "Saved embeddings for batch: 178\n",
      "Saved embeddings for batch: 179\n",
      "Saved embeddings for batch: 180\n",
      "Saved embeddings for batch: 181\n",
      "Saved embeddings for batch: 182\n",
      "Saved embeddings for batch: 183\n",
      "Saved embeddings for batch: 184\n",
      "Saved embeddings for batch: 185\n",
      "Saved embeddings for batch: 186\n",
      "Saved embeddings for batch: 187\n",
      "Saved embeddings for batch: 188\n",
      "Saved embeddings for batch: 189\n",
      "Saved embeddings for batch: 190\n",
      "Saved embeddings for batch: 191\n",
      "Saved embeddings for batch: 192\n",
      "Saved embeddings for batch: 193\n",
      "Saved embeddings for batch: 194\n",
      "Saved embeddings for batch: 195\n",
      "Saved embeddings for batch: 196\n",
      "Saved embeddings for batch: 197\n",
      "Saved embeddings for batch: 198\n",
      "Saved embeddings for batch: 199\n",
      "Saved embeddings for batch: 200\n",
      "Saved embeddings for batch: 201\n",
      "Saved embeddings for batch: 202\n",
      "Saved embeddings for batch: 203\n",
      "Saved embeddings for batch: 204\n",
      "Saved embeddings for batch: 205\n",
      "Saved embeddings for batch: 206\n",
      "Saved embeddings for batch: 207\n",
      "Saved embeddings for batch: 208\n",
      "Saved embeddings for batch: 209\n",
      "Saved embeddings for batch: 210\n",
      "Saved embeddings for batch: 211\n",
      "Saved embeddings for batch: 212\n",
      "Saved embeddings for batch: 213\n",
      "Saved embeddings for batch: 214\n",
      "Saved embeddings for batch: 215\n",
      "Saved embeddings for batch: 216\n",
      "Saved embeddings for batch: 217\n",
      "Saved embeddings for batch: 218\n",
      "Saved embeddings for batch: 219\n",
      "Saved embeddings for batch: 220\n",
      "Saved embeddings for batch: 221\n",
      "Saved embeddings for batch: 222\n",
      "Saved embeddings for batch: 223\n",
      "Saved embeddings for batch: 224\n",
      "Saved embeddings for batch: 225\n",
      "Saved embeddings for batch: 226\n",
      "Saved embeddings for batch: 227\n",
      "Saved embeddings for batch: 228\n",
      "Saved embeddings for batch: 229\n",
      "Saved embeddings for batch: 230\n",
      "Saved embeddings for batch: 231\n",
      "Saved embeddings for batch: 232\n",
      "Saved embeddings for batch: 233\n",
      "Saved embeddings for batch: 234\n",
      "Saved embeddings for batch: 235\n",
      "Saved embeddings for batch: 236\n",
      "Saved embeddings for batch: 237\n",
      "Saved embeddings for batch: 238\n",
      "Saved embeddings for batch: 239\n",
      "Saved embeddings for batch: 240\n",
      "Saved embeddings for batch: 241\n",
      "Saved embeddings for batch: 242\n",
      "Saved embeddings for batch: 243\n",
      "Saved embeddings for batch: 244\n",
      "Saved embeddings for batch: 245\n",
      "Saved embeddings for batch: 246\n",
      "Saved embeddings for batch: 247\n",
      "Saved embeddings for batch: 248\n",
      "Saved embeddings for batch: 249\n",
      "Saved embeddings for batch: 250\n",
      "Saved embeddings for batch: 251\n",
      "Saved embeddings for batch: 252\n",
      "Saved embeddings for batch: 253\n",
      "Saved embeddings for batch: 254\n",
      "Saved embeddings for batch: 255\n",
      "Saved embeddings for batch: 256\n",
      "Saved embeddings for batch: 257\n",
      "Saved embeddings for batch: 258\n",
      "Saved embeddings for batch: 259\n",
      "Saved embeddings for batch: 260\n",
      "Saved embeddings for batch: 261\n",
      "Saved embeddings for batch: 262\n",
      "Saved embeddings for batch: 263\n",
      "Saved embeddings for batch: 264\n",
      "Saved embeddings for batch: 265\n",
      "Saved embeddings for batch: 266\n",
      "Saved embeddings for batch: 267\n",
      "Saved embeddings for batch: 268\n",
      "Saved embeddings for batch: 269\n",
      "Saved embeddings for batch: 270\n",
      "Saved embeddings for batch: 271\n",
      "Saved embeddings for batch: 272\n",
      "Saved embeddings for batch: 273\n",
      "Saved embeddings for batch: 274\n",
      "Saved embeddings for batch: 275\n",
      "Saved embeddings for batch: 276\n",
      "Saved embeddings for batch: 277\n",
      "Saved embeddings for batch: 278\n",
      "Saved embeddings for batch: 279\n",
      "Saved embeddings for batch: 280\n",
      "Saved embeddings for batch: 281\n",
      "Saved embeddings for batch: 282\n",
      "Saved embeddings for batch: 283\n",
      "Saved embeddings for batch: 284\n",
      "Saved embeddings for batch: 285\n",
      "Saved embeddings for batch: 286\n",
      "Saved embeddings for batch: 287\n",
      "Saved embeddings for batch: 288\n",
      "Saved embeddings for batch: 289\n",
      "Saved embeddings for batch: 290\n",
      "Saved embeddings for batch: 291\n",
      "Saved embeddings for batch: 292\n",
      "Saved embeddings for batch: 293\n",
      "Saved embeddings for batch: 294\n",
      "Saved embeddings for batch: 295\n",
      "Saved embeddings for batch: 296\n",
      "Saved embeddings for batch: 297\n",
      "Saved embeddings for batch: 298\n",
      "Saved embeddings for batch: 299\n",
      "Saved embeddings for batch: 300\n",
      "Saved embeddings for batch: 301\n",
      "Saved embeddings for batch: 302\n",
      "Saved embeddings for batch: 303\n",
      "Saved embeddings for batch: 304\n",
      "Saved embeddings for batch: 305\n",
      "Saved embeddings for batch: 306\n",
      "Saved embeddings for batch: 307\n",
      "Saved embeddings for batch: 308\n",
      "Saved embeddings for batch: 309\n",
      "Saved embeddings for batch: 310\n",
      "Saved embeddings for batch: 311\n",
      "Saved embeddings for batch: 312\n",
      "Saved embeddings for batch: 313\n",
      "Saved embeddings for batch: 314\n",
      "Saved embeddings for batch: 315\n",
      "Saved embeddings for batch: 316\n",
      "Saved embeddings for batch: 317\n",
      "Saved embeddings for batch: 318\n",
      "Saved embeddings for batch: 319\n",
      "Saved embeddings for batch: 320\n",
      "Saved embeddings for batch: 321\n",
      "Saved embeddings for batch: 322\n",
      "Saved embeddings for batch: 323\n",
      "Saved embeddings for batch: 324\n",
      "Saved embeddings for batch: 325\n",
      "Saved embeddings for batch: 326\n",
      "Saved embeddings for batch: 327\n",
      "Saved embeddings for batch: 328\n",
      "Saved embeddings for batch: 329\n",
      "Saved embeddings for batch: 330\n",
      "Saved embeddings for batch: 331\n",
      "Saved embeddings for batch: 332\n",
      "Saved embeddings for batch: 333\n",
      "Saved embeddings for batch: 334\n",
      "Saved embeddings for batch: 335\n",
      "Saved embeddings for batch: 336\n",
      "Saved embeddings for batch: 337\n",
      "Saved embeddings for batch: 338\n",
      "Saved embeddings for batch: 339\n",
      "Saved embeddings for batch: 340\n",
      "Saved embeddings for batch: 341\n",
      "Saved embeddings for batch: 342\n",
      "Saved embeddings for batch: 343\n",
      "Saved embeddings for batch: 344\n",
      "Saved embeddings for batch: 345\n",
      "Saved embeddings for batch: 346\n",
      "Saved embeddings for batch: 347\n",
      "Saved embeddings for batch: 348\n",
      "Saved embeddings for batch: 349\n",
      "Saved embeddings for batch: 350\n",
      "Saved embeddings for batch: 351\n",
      "Saved embeddings for batch: 352\n",
      "Saved embeddings for batch: 353\n",
      "Saved embeddings for batch: 354\n",
      "Saved embeddings for batch: 355\n",
      "Saved embeddings for batch: 356\n",
      "Saved embeddings for batch: 357\n",
      "Saved embeddings for batch: 358\n",
      "Saved embeddings for batch: 359\n",
      "Saved embeddings for batch: 360\n",
      "Saved embeddings for batch: 361\n",
      "Saved embeddings for batch: 362\n",
      "Saved embeddings for batch: 363\n",
      "Saved embeddings for batch: 364\n",
      "Saved embeddings for batch: 365\n",
      "Saved embeddings for batch: 366\n",
      "Saved embeddings for batch: 367\n",
      "Saved embeddings for batch: 368\n",
      "Saved embeddings for batch: 369\n",
      "Saved embeddings for batch: 370\n",
      "Saved embeddings for batch: 371\n",
      "Saved embeddings for batch: 372\n",
      "Saved embeddings for batch: 373\n",
      "Saved embeddings for batch: 374\n",
      "Saved embeddings for batch: 375\n",
      "Saved embeddings for batch: 376\n",
      "Saved embeddings for batch: 377\n",
      "Saved embeddings for batch: 378\n",
      "Saved embeddings for batch: 379\n",
      "Saved embeddings for batch: 380\n",
      "Saved embeddings for batch: 381\n",
      "Saved embeddings for batch: 382\n",
      "Saved embeddings for batch: 383\n",
      "Saved embeddings for batch: 384\n",
      "Saved embeddings for batch: 385\n",
      "Saved embeddings for batch: 386\n",
      "Saved embeddings for batch: 387\n",
      "Saved embeddings for batch: 388\n",
      "Saved embeddings for batch: 389\n",
      "Saved embeddings for batch: 390\n",
      "Saved embeddings for batch: 391\n",
      "Saved embeddings for batch: 392\n",
      "Saved embeddings for batch: 393\n",
      "Saved embeddings for batch: 394\n",
      "Saved embeddings for batch: 395\n",
      "Saved embeddings for batch: 396\n",
      "Saved embeddings for batch: 397\n",
      "Saved embeddings for batch: 398\n",
      "Saved embeddings for batch: 399\n",
      "Saved embeddings for batch: 400\n",
      "Saved embeddings for batch: 401\n",
      "Saved embeddings for batch: 402\n"
     ]
    }
   ],
   "source": [
    "save_embeddings_hdf5(train_loader, \"train_embeddings.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c104a2e5-1e5a-4ee2-acbf-93ccd27e064a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings for batch: 1\n",
      "Saved embeddings for batch: 2\n",
      "Saved embeddings for batch: 3\n",
      "Saved embeddings for batch: 4\n",
      "Saved embeddings for batch: 5\n",
      "Saved embeddings for batch: 6\n",
      "Saved embeddings for batch: 7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msave_embeddings_hdf5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_embeddings.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m, in \u001b[0;36msave_embeddings_hdf5\u001b[0;34m(dataloader, filename)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      6\u001b[0m     first_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (embedding_list, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# Extract the last layer embeddings\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         embedding_tensor \u001b[38;5;241m=\u001b[39m embedding_list[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Extract final layer embeddings\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         embedding_tensor \u001b[38;5;241m=\u001b[39m embedding_tensor\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()  \u001b[38;5;66;03m# Move to CPU\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m, in \u001b[0;36mNiftiDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 22\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_nifti\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     24\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m get_vit_embedding(image\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m128\u001b[39m))\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mNiftiDataset.preprocess_nifti\u001b[0;34m(self, nifti_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m img \u001b[38;5;241m=\u001b[39m (img \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(img)) \u001b[38;5;241m/\u001b[39m (np\u001b[38;5;241m.\u001b[39mmax(img) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(img) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Resize to target shape\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m img_resized \u001b[38;5;241m=\u001b[39m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzoom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(img_resized, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/scipy/ndimage/_interpolation.py:862\u001b[0m, in \u001b[0;36mzoom\u001b[0;34m(input, zoom, output, order, mode, cval, prefilter, grid_mode)\u001b[0m\n\u001b[1;32m    858\u001b[0m zoom \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdivide(zoom_nominator, zoom_div,\n\u001b[1;32m    859\u001b[0m                  out\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mones_like(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64),\n\u001b[1;32m    860\u001b[0m                  where\u001b[38;5;241m=\u001b[39mzoom_div \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    861\u001b[0m zoom \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(zoom)\n\u001b[0;32m--> 862\u001b[0m \u001b[43m_nd_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzoom_shift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzoom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnpad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mgrid_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_embeddings_hdf5(test_loader, \"test_embeddings.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90199ebb-4c12-4e7b-bd36-1aed95925a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.16\n"
     ]
    }
   ],
   "source": [
    "!python3 -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "504d8865-b072-44d8-bec1-2d1af005019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_hdf5(filename, batch_size=32):\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        num_samples = f[\"embeddings\"].shape[0]  # Total samples\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            X_batch = torch.tensor(f[\"embeddings\"][i : i + batch_size], dtype=torch.float32)\n",
    "            y_batch = torch.tensor(f[\"labels\"][i : i + batch_size], dtype=torch.long)\n",
    "            yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f45a8e92-2b2b-4bb7-a3b1-fd72cc042dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca0a001e-95b6-45f1-8384-f0011a35cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_batch, y_batch in load_embeddings_hdf5(\"train_embeddings.h5\"):\n",
    "    clf.fit(X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ad2ea1c-2f09-49cf-bfba-1fa1c81fb06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = [], []\n",
    "for X_batch, y_batch in load_embeddings_hdf5(\"test_embeddings.h5\"):\n",
    "    y_pred.extend(clf.predict(X_batch))\n",
    "    y_true.extend(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8328ba9-a49f-460f-85fe-5e8fb1d431b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       213\n",
      "           1     0.4822    0.9790    0.6462       333\n",
      "           2     0.3077    0.0280    0.0513       143\n",
      "\n",
      "    accuracy                         0.4790       689\n",
      "   macro avg     0.2633    0.3357    0.2325       689\n",
      "weighted avg     0.2969    0.4790    0.3230       689\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ebd750f-ea7f-4567-a9ba-681ba901667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9af434c0-713f-4747-add1-0dab59e5a131",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = next(load_embeddings_hdf5(\"train_embeddings.h5\"))[0].shape[1]  # Get feature size\n",
    "num_classes = 3  # Adjust based on labels\n",
    "model = MLPClassifier(input_dim, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53eb668e-d4e1-4963-b601-55afd0f07f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a8a0a0e-c841-46df-9644-88bf0b8a4420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.7554\n",
      "Epoch [2/10], Loss: 0.7504\n",
      "Epoch [3/10], Loss: 0.7506\n",
      "Epoch [4/10], Loss: 0.7511\n",
      "Epoch [5/10], Loss: 0.7516\n",
      "Epoch [6/10], Loss: 0.7522\n",
      "Epoch [7/10], Loss: 0.7527\n",
      "Epoch [8/10], Loss: 0.7532\n",
      "Epoch [9/10], Loss: 0.7537\n",
      "Epoch [10/10], Loss: 0.7542\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for X_batch, y_batch in load_embeddings_hdf5(\"train_embeddings.h5\", batch_size=4):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{10}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d024b1eb-13bb-41ee-8713-81f9db73ccc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.4286\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in load_embeddings_hdf5(\"test_embeddings.h5\", batch_size=32):\n",
    "        outputs = model(X_batch)\n",
    "        predicted_labels = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        y_true.extend(y_batch.numpy())\n",
    "        y_pred.extend(predicted_labels.numpy())\n",
    "\n",
    "# Compute accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3274333-b94e-4eed-9ca4-c394e983121c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
