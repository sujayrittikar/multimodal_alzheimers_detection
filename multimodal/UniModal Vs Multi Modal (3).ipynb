{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd16bdcb-1310-450c-88dd-eb99b6beb334",
   "metadata": {},
   "source": [
    "# UniModal Images Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94b23760-3ca8-477d-86db-b9c530bea46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of 'flattened_images_2':\n",
      "  84489294400 bytes\n",
      "  80575.27 MB\n",
      "  78.69 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_folder_size(folder):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(folder):\n",
    "        for file in filenames:\n",
    "            file_path = os.path.join(dirpath, file)\n",
    "            \n",
    "            if not os.path.islink(file_path):\n",
    "                total_size += os.path.getsize(file_path)\n",
    "    return total_size\n",
    "\n",
    "folder = \"flattened_images_2\"\n",
    "size_bytes = get_folder_size(folder)\n",
    "size_mb = size_bytes / (1024 * 1024)\n",
    "size_gb = size_bytes / (1024 * 1024 * 1024)\n",
    "\n",
    "print(f\"Total size of '{folder}':\")\n",
    "print(f\"  {size_bytes} bytes\")\n",
    "print(f\"  {size_mb:.2f} MB\")\n",
    "print(f\"  {size_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfb2c5d5-90b2-4f64-b189-a4edb4da53ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 15 14:13:00 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.04             Driver Version: 570.124.04     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A4000               Off |   00000000:01:00.0 Off |                  Off |\n",
      "| 41%   41C    P8             16W /  140W |     431MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A4000               Off |   00000000:21:00.0 Off |                  Off |\n",
      "| 70%   87C    P0            125W /  140W |   11237MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A          143101      C   ...conda3/envs/my_env/bin/python        422MiB |\n",
      "|    1   N/A  N/A          155681      C   ...olia-h/miniconda3/bin/python3      11228MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c8b5986-81f8-40b6-b190-a2acfedc1317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 1.0638, Train Acc: 0.4623 | Val Loss: 1.0175, Val Acc: 0.4971\n",
      "Best model saved.\n",
      "Epoch 2/10 | Train Loss: 0.9520, Train Acc: 0.5439 | Val Loss: 0.8992, Val Acc: 0.6192\n",
      "Best model saved.\n",
      "Epoch 3/10 | Train Loss: 0.4299, Train Acc: 0.8243 | Val Loss: 1.0173, Val Acc: 0.5610\n",
      "Epoch 4/10 | Train Loss: 0.0396, Train Acc: 0.9938 | Val Loss: 1.1181, Val Acc: 0.6337\n",
      "Epoch 5/10 | Train Loss: 0.0086, Train Acc: 0.9988 | Val Loss: 1.1097, Val Acc: 0.6773\n",
      "Early stopping triggered.\n",
      "ðŸ”¥ Test Accuracy: 0.6522\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CN       0.61      0.59      0.60       106\n",
      "         MCI       0.69      0.76      0.72       167\n",
      "          AD       0.62      0.49      0.55        72\n",
      "\n",
      "    accuracy                           0.65       345\n",
      "   macro avg       0.64      0.61      0.62       345\n",
      "weighted avg       0.65      0.65      0.65       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, DeiTForImageClassification\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#  Check CUDA availability\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths to datasets and preprocessed images\n",
    "train_csv = \"train_data_3.csv\"\n",
    "val_csv = \"val_data_3.csv\"\n",
    "test_csv = \"test_data_3.csv\"\n",
    "image_dir = \"preprocessed_images_3\"  \n",
    "\n",
    "# Load Data Splits\n",
    "train_data = pd.read_csv(train_csv)\n",
    "val_data = pd.read_csv(val_csv)\n",
    "test_data = pd.read_csv(test_csv)\n",
    "\n",
    "#  Load DeiT-Base Model and Image Processor with classifier head\n",
    "checkpoint = \"facebook/deit-base-distilled-patch16-224\"  \n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "model = DeiTForImageClassification.from_pretrained(\n",
    "    checkpoint, num_labels=3, ignore_mismatched_sizes=True  \n",
    ").to(device)\n",
    "\n",
    "# Define label mapping \n",
    "label_map = {\"CN\": 0, \"MCI\": 1, \"AD\": 2}\n",
    "\n",
    "class MRIImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        \n",
    "        img_path = os.path.join(self.image_dir, f\"{row['Image Data ID']}.png\")\n",
    "        \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = label_map[row[\"Group\"]]\n",
    "        return image, label\n",
    "\n",
    "#  Define Transformations for DeiT-Base\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "])\n",
    "\n",
    "#  Create PyTorch Dataloaders\n",
    "batch_size = 8\n",
    "train_dataset = MRIImageDataset(train_data, image_dir, transform=transform)\n",
    "val_dataset = MRIImageDataset(val_data, image_dir, transform=transform)\n",
    "test_dataset = MRIImageDataset(test_data, image_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "#  Define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "#  Training Loop with Early Stopping\n",
    "epochs = 10\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_state = None\n",
    "patience = 3\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct_val += predicted.eq(labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    val_loss = running_val_loss / len(val_dataset)\n",
    "    val_acc = correct_val / total_val\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        wait = 0\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "if best_model_state:\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "#  Evaluate Model on Test Set and Compute Classification Report\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images).logits\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = correct / total\n",
    "print(f\"ðŸ”¥ Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Compute and print classification report \n",
    "report = classification_report(all_labels, all_preds, target_names=list(label_map.keys()))\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4bb056c-9471-4874-be26-02de0091ea10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 05:59:31.817615: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-19 05:59:31.856938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-19 05:59:32.474935: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 1.0688, Train Acc: 0.4579 | Val Loss: 1.0299, Val Acc: 0.4855\n",
      "Best model saved.\n",
      "Epoch 2/10 | Train Loss: 0.9702, Train Acc: 0.5445 | Val Loss: 0.9788, Val Acc: 0.5174\n",
      "Best model saved.\n",
      "Epoch 3/10 | Train Loss: 0.4673, Train Acc: 0.8187 | Val Loss: 0.8478, Val Acc: 0.6570\n",
      "Best model saved.\n",
      "Epoch 4/10 | Train Loss: 0.0413, Train Acc: 0.9963 | Val Loss: 0.9522, Val Acc: 0.6453\n",
      "Epoch 5/10 | Train Loss: 0.0045, Train Acc: 1.0000 | Val Loss: 1.0168, Val Acc: 0.6599\n",
      "Epoch 6/10 | Train Loss: 0.0017, Train Acc: 1.0000 | Val Loss: 1.0938, Val Acc: 0.6628\n",
      "Early stopping triggered.\n",
      "ðŸ”¥ Test Accuracy: 0.6435\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CN       0.62      0.52      0.56       106\n",
      "         MCI       0.66      0.80      0.72       167\n",
      "          AD       0.63      0.46      0.53        72\n",
      "\n",
      "    accuracy                           0.64       345\n",
      "   macro avg       0.64      0.59      0.61       345\n",
      "weighted avg       0.64      0.64      0.63       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, DeiTForImageClassification\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#  Check CUDA availability\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths to datasets and preprocessed PNG images\n",
    "train_csv = \"train_data_3.csv\"\n",
    "val_csv = \"val_data_3.csv\"\n",
    "test_csv = \"test_data_3.csv\"\n",
    "image_dir = \"preprocessed_images_3\"  \n",
    "\n",
    "# Load Data Splits\n",
    "train_data = pd.read_csv(train_csv)\n",
    "val_data = pd.read_csv(val_csv)\n",
    "test_data = pd.read_csv(test_csv)\n",
    "\n",
    "#  Load DeiT-Base Model and Image Processor with modified classifier head\n",
    "checkpoint = \"facebook/deit-base-distilled-patch16-224\"  \n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "model = DeiTForImageClassification.from_pretrained(\n",
    "    checkpoint, num_labels=3, ignore_mismatched_sizes=True  \n",
    ").to(device)\n",
    "\n",
    "# Define label mapping \n",
    "label_map = {\"CN\": 0, \"MCI\": 1, \"AD\": 2}\n",
    "\n",
    "class MRIImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        # Build the file path assuming PNG images\n",
    "        img_path = os.path.join(self.image_dir, f\"{row['Image Data ID']}.png\")\n",
    "        # Open the PNG image using PIL and convert it to RGB\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = label_map[row[\"Group\"]]\n",
    "        return image, label\n",
    "\n",
    "#  Define Transformations for DeiT-Base\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "])\n",
    "\n",
    "#  Create PyTorch Dataloaders\n",
    "batch_size = 8\n",
    "train_dataset = MRIImageDataset(train_data, image_dir, transform=transform)\n",
    "val_dataset = MRIImageDataset(val_data, image_dir, transform=transform)\n",
    "test_dataset = MRIImageDataset(test_data, image_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "#  Define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "#  Training Loop with Early Stopping\n",
    "epochs = 10\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_state = None\n",
    "patience = 3\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct_val += predicted.eq(labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    val_loss = running_val_loss / len(val_dataset)\n",
    "    val_acc = correct_val / total_val\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        wait = 0\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "if best_model_state:\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "#  Evaluate Model on Test Set and Compute Classification Report\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images).logits\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = correct / total\n",
    "print(f\"ðŸ”¥ Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Compute and print classification report using scikit-learn\n",
    "report = classification_report(all_labels, all_preds, target_names=list(label_map.keys()))\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6a2d50-c364-446c-b4d6-5fefae19bc05",
   "metadata": {},
   "source": [
    "## DEIT Model End to end training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fd897b4-9d30-49b7-b048-d9ec408594cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 05:45:45.859845: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-17 05:45:45.897912: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-17 05:45:46.446734: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 1.0628, Train Acc: 0.4654 | Val Loss: 1.0162, Val Acc: 0.5174\n",
      "Best model saved.\n",
      "Epoch 2/10 | Train Loss: 0.9562, Train Acc: 0.5340 | Val Loss: 0.9823, Val Acc: 0.5291\n",
      "Best model saved.\n",
      "Epoch 3/10 | Train Loss: 0.5160, Train Acc: 0.8062 | Val Loss: 0.8068, Val Acc: 0.6512\n",
      "Best model saved.\n",
      "Epoch 4/10 | Train Loss: 0.0677, Train Acc: 0.9931 | Val Loss: 0.9239, Val Acc: 0.6657\n",
      "Epoch 5/10 | Train Loss: 0.0105, Train Acc: 1.0000 | Val Loss: 0.9995, Val Acc: 0.6628\n",
      "Epoch 6/10 | Train Loss: 0.0044, Train Acc: 1.0000 | Val Loss: 1.0697, Val Acc: 0.6541\n",
      "Early stopping triggered.\n",
      "ðŸ”¥ Test Accuracy: 0.6348\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, DeiTForImageClassification\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths to datasets and preprocessed images\n",
    "train_csv = \"train_data_3.csv\"\n",
    "val_csv = \"val_data_3.csv\"\n",
    "test_csv = \"test_data_3.csv\"\n",
    "image_dir = \"preprocessed_images_3\"\n",
    "\n",
    "# Load Data Splits\n",
    "train_data = pd.read_csv(train_csv)\n",
    "val_data = pd.read_csv(val_csv)\n",
    "test_data = pd.read_csv(test_csv)\n",
    "\n",
    "# Load DeiT-Base Model and Image Processor with modified classifier head\n",
    "checkpoint = \"facebook/deit-base-distilled-patch16-224\"  \n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "model = DeiTForImageClassification.from_pretrained(\n",
    "    checkpoint, num_labels=3, ignore_mismatched_sizes=True  \n",
    ").to(device)\n",
    "\n",
    "# Define label mapping \n",
    "label_map = {\"CN\": 0, \"MCI\": 1, \"AD\": 2}\n",
    "\n",
    "class MRIImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        img_path = os.path.join(self.image_dir, f\"{row['Image Data ID']}.png\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = label_map[row[\"Group\"]]\n",
    "        return image, label\n",
    "\n",
    "# Define Transformations for DeiT-Base\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "])\n",
    "\n",
    "# Create PyTorch Dataloaders\n",
    "batch_size = 8\n",
    "train_dataset = MRIImageDataset(train_data, image_dir, transform=transform)\n",
    "val_dataset = MRIImageDataset(val_data, image_dir, transform=transform)\n",
    "test_dataset = MRIImageDataset(test_data, image_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "# Training Loop with Early Stopping\n",
    "epochs = 10\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_state = None\n",
    "patience = 3\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct_val += predicted.eq(labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    val_loss = running_val_loss / len(val_dataset)\n",
    "    val_acc = correct_val / total_val\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        wait = 0\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "if best_model_state:\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "# Evaluate Model on Test Set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images).logits\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_acc = correct / total\n",
    "print(f\"ðŸ”¥ Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4643949-4a56-45cd-a05c-d50d0a5658f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 1.0613, Train Acc: 0.4698 | Val Loss: 1.0142, Val Acc: 0.4884\n",
      "Best model saved.\n",
      "Epoch 2/10 | Train Loss: 0.9641, Train Acc: 0.5240 | Val Loss: 0.9699, Val Acc: 0.5465\n",
      "Best model saved.\n",
      "Epoch 3/10 | Train Loss: 0.6085, Train Acc: 0.7639 | Val Loss: 0.9055, Val Acc: 0.6250\n",
      "Best model saved.\n",
      "Epoch 4/10 | Train Loss: 0.1029, Train Acc: 0.9863 | Val Loss: 0.9356, Val Acc: 0.6744\n",
      "Epoch 5/10 | Train Loss: 0.0150, Train Acc: 1.0000 | Val Loss: 1.0320, Val Acc: 0.6657\n",
      "Epoch 6/10 | Train Loss: 0.0048, Train Acc: 1.0000 | Val Loss: 1.1095, Val Acc: 0.6628\n",
      "Early stopping triggered.\n",
      "ðŸ”¥ Test Accuracy: 0.6551\n",
      "\n",
      "Direct Calculation:\n",
      "Standard Error: 0.0256\n",
      "Margin of Error (95% CI): 0.0502\n",
      "95% Confidence Interval for Accuracy: [0.6049, 0.7052]\n",
      "\n",
      "Bootstrapping:\n",
      "Bootstrapped Mean Accuracy: 0.6551\n",
      "Standard Error (Bootstrap): 0.0269\n",
      "Margin of Error (Bootstrap, 95% CI): 0.0527\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, DeiTForImageClassification\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# âœ… Check CUDA availability\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths to datasets and preprocessed images\n",
    "train_csv = \"train_data_3.csv\"\n",
    "val_csv = \"val_data_3.csv\"\n",
    "test_csv = \"test_data_3.csv\"\n",
    "image_dir = \"preprocessed_images_3\"\n",
    "\n",
    "# Load Data Splits\n",
    "train_data = pd.read_csv(train_csv)\n",
    "val_data = pd.read_csv(val_csv)\n",
    "test_data = pd.read_csv(test_csv)\n",
    "\n",
    "# âœ… Load DeiT-Base Model and Image Processor with modified classifier head\n",
    "checkpoint = \"facebook/deit-base-distilled-patch16-224\"  # Using DeiT base distilled model\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "model = DeiTForImageClassification.from_pretrained(\n",
    "    checkpoint, num_labels=3, ignore_mismatched_sizes=True  # Set num_labels=3\n",
    ").to(device)\n",
    "\n",
    "# Define label mapping (example: \"CN\": Cognitively Normal, \"MCI\": Mild Cognitive Impairment, \"AD\": Alzheimerâ€™s Disease)\n",
    "label_map = {\"CN\": 0, \"MCI\": 1, \"AD\": 2}\n",
    "\n",
    "class MRIImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        img_path = os.path.join(self.image_dir, f\"{row['Image Data ID']}.png\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = label_map[row[\"Group\"]]\n",
    "        return image, label\n",
    "\n",
    "# âœ… Define Transformations for DeiT-Base\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224 as expected by DeiT-Base\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "])\n",
    "\n",
    "# âœ… Create PyTorch Dataloaders\n",
    "batch_size = 8\n",
    "train_dataset = MRIImageDataset(train_data, image_dir, transform=transform)\n",
    "val_dataset = MRIImageDataset(val_data, image_dir, transform=transform)\n",
    "test_dataset = MRIImageDataset(test_data, image_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# âœ… Define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "# âœ… Training Loop with Early Stopping\n",
    "epochs = 10\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_state = None\n",
    "patience = 3\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct_val += predicted.eq(labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    val_loss = running_val_loss / len(val_dataset)\n",
    "    val_acc = correct_val / total_val\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        wait = 0\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "if best_model_state:\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "# âœ… Evaluate Model on Test Set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images).logits\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = correct / total\n",
    "print(f\"ðŸ”¥ Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Calculate Margin of Error for Test Accuracy\n",
    "# ---------------------------\n",
    "\n",
    "# Approach 1: Direct Calculation using the Binomial Formula.\n",
    "n_test = len(test_dataset)\n",
    "se_direct = math.sqrt(test_acc * (1 - test_acc) / n_test)\n",
    "moe_direct = 1.96 * se_direct\n",
    "print(\"\\nDirect Calculation:\")\n",
    "print(\"Standard Error: {:.4f}\".format(se_direct))\n",
    "print(\"Margin of Error (95% CI): {:.4f}\".format(moe_direct))\n",
    "print(\"95% Confidence Interval for Accuracy: [{:.4f}, {:.4f}]\".format(test_acc - moe_direct, test_acc + moe_direct))\n",
    "\n",
    "# Approach 2: Bootstrapping.\n",
    "n_bootstrap = 1000\n",
    "boot_accs = []\n",
    "all_preds = np.array(all_preds)\n",
    "all_targets = np.array(all_targets)\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_test), n_test, replace=True)\n",
    "    acc_bs = accuracy_score(all_targets[indices], all_preds[indices])\n",
    "    boot_accs.append(acc_bs)\n",
    "boot_accs = np.array(boot_accs)\n",
    "se_bootstrap = np.std(boot_accs)\n",
    "moe_bootstrap = 1.96 * se_bootstrap\n",
    "print(\"\\nBootstrapping:\")\n",
    "print(\"Bootstrapped Mean Accuracy: {:.4f}\".format(np.mean(boot_accs)))\n",
    "print(\"Standard Error (Bootstrap): {:.4f}\".format(se_bootstrap))\n",
    "print(\"Margin of Error (Bootstrap, 95% CI): {:.4f}\".format(moe_bootstrap))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "674b5b3b-bee3-4ef0-bbea-4de9d4565a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.6492\n",
      "Bootstrapped Weighted F1 Mean: 0.6498\n",
      "Standard Error (Bootstrap) for Weighted F1: 0.0269\n",
      "Margin of Error (Bootstrap, 95% CI) for Weighted F1: 0.0528\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Compute the weighted F1 score on the test set.\n",
    "weighted_f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "print(\"Weighted F1 Score: {:.4f}\".format(weighted_f1))\n",
    "\n",
    "# Bootstrapping to estimate the margin of error for the weighted F1 score.\n",
    "n_bootstrap = 1000\n",
    "boot_f1s = []\n",
    "n_samples = len(all_targets)\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_samples), n_samples, replace=True)\n",
    "    f1_bs = f1_score(all_targets[indices], all_preds[indices], average='weighted')\n",
    "    boot_f1s.append(f1_bs)\n",
    "boot_f1s = np.array(boot_f1s)\n",
    "\n",
    "std_f1 = np.std(boot_f1s)\n",
    "moe_f1 = 1.96 * std_f1\n",
    "\n",
    "print(\"Bootstrapped Weighted F1 Mean: {:.4f}\".format(np.mean(boot_f1s)))\n",
    "print(\"Standard Error (Bootstrap) for Weighted F1: {:.4f}\".format(std_f1))\n",
    "print(\"Margin of Error (Bootstrap, 95% CI) for Weighted F1: {:.4f}\".format(moe_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aac7ec4b-c2e7-466a-b317-1884c7c75c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 1.0635, Train Acc: 0.4741 | Val Loss: 1.0187, Val Acc: 0.4855\n",
      "Best model saved.\n",
      "Epoch 2/10 | Train Loss: 0.9260, Train Acc: 0.5657 | Val Loss: 0.9279, Val Acc: 0.5233\n",
      "Best model saved.\n",
      "Epoch 3/10 | Train Loss: 0.3414, Train Acc: 0.8729 | Val Loss: 0.9068, Val Acc: 0.5930\n",
      "Best model saved.\n",
      "Epoch 4/10 | Train Loss: 0.0400, Train Acc: 0.9907 | Val Loss: 0.9134, Val Acc: 0.6541\n",
      "Epoch 5/10 | Train Loss: 0.0072, Train Acc: 0.9994 | Val Loss: 0.9495, Val Acc: 0.7122\n",
      "Epoch 6/10 | Train Loss: 0.0014, Train Acc: 1.0000 | Val Loss: 1.0481, Val Acc: 0.6977\n",
      "Early stopping triggered.\n",
      "ðŸ”¥ Test Accuracy: 0.6638\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CN       0.70      0.57      0.62       106\n",
      "         MCI       0.66      0.83      0.73       167\n",
      "          AD       0.62      0.43      0.51        72\n",
      "\n",
      "    accuracy                           0.66       345\n",
      "   macro avg       0.66      0.61      0.62       345\n",
      "weighted avg       0.66      0.66      0.65       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, DeiTForImageClassification\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths to datasets and preprocessed PNG images\n",
    "train_csv = \"train_data_3.csv\"\n",
    "val_csv = \"val_data_3.csv\"\n",
    "test_csv = \"test_data_3.csv\"\n",
    "image_dir = \"preprocessed_images_3\"  \n",
    "\n",
    "# Load Data Splits\n",
    "train_data = pd.read_csv(train_csv)\n",
    "val_data = pd.read_csv(val_csv)\n",
    "test_data = pd.read_csv(test_csv)\n",
    "\n",
    "# Load DeiT-Base Model and Image Processor with modified classifier head\n",
    "checkpoint = \"facebook/deit-base-distilled-patch16-224\"  \n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "model = DeiTForImageClassification.from_pretrained(\n",
    "    checkpoint, num_labels=3, ignore_mismatched_sizes=True  \n",
    ").to(device)\n",
    "\n",
    "# Define label mapping \n",
    "label_map = {\"CN\": 0, \"MCI\": 1, \"AD\": 2}\n",
    "\n",
    "class MRIImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        # Build the file path assuming PNG images\n",
    "        img_path = os.path.join(self.image_dir, f\"{row['Image Data ID']}.png\")\n",
    "        # Open the PNG image using PIL and convert it to RGB\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = label_map[row[\"Group\"]]\n",
    "        return image, label\n",
    "\n",
    "# Define Transformations for DeiT-Base\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "])\n",
    "\n",
    "# Create PyTorch Dataloaders\n",
    "batch_size = 8\n",
    "train_dataset = MRIImageDataset(train_data, image_dir, transform=transform)\n",
    "val_dataset = MRIImageDataset(val_data, image_dir, transform=transform)\n",
    "test_dataset = MRIImageDataset(test_data, image_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "#  Define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "#  Training Loop with Early Stopping\n",
    "epochs = 10\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_state = None\n",
    "patience = 3\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct_val += predicted.eq(labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    val_loss = running_val_loss / len(val_dataset)\n",
    "    val_acc = correct_val / total_val\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        wait = 0\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "if best_model_state:\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "# Evaluate Model on Test Set and Compute Classification Report\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images).logits\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = correct / total\n",
    "print(f\"ðŸ”¥ Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Compute and print classification report using scikit-learn\n",
    "report = classification_report(all_labels, all_preds, target_names=list(label_map.keys()))\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b6ca9c-2b6e-4574-bb1d-59d78981777a",
   "metadata": {},
   "source": [
    "## Using MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e774a54-1f2f-44cf-a42f-b9ea14752cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape: (1605, 768)\n",
      "Val features shape: (344, 768)\n",
      "Test features shape: (345, 768)\n",
      "Epoch 1/200: Train Loss = 1.0683, Val Loss = 1.0389, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 2/200: Train Loss = 1.0490, Val Loss = 1.0353, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 3/200: Train Loss = 1.0480, Val Loss = 1.0324, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 4/200: Train Loss = 1.0396, Val Loss = 1.0306, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 5/200: Train Loss = 1.0326, Val Loss = 1.0289, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 6/200: Train Loss = 1.0371, Val Loss = 1.0265, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 7/200: Train Loss = 1.0277, Val Loss = 1.0248, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 8/200: Train Loss = 1.0270, Val Loss = 1.0230, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 9/200: Train Loss = 1.0305, Val Loss = 1.0210, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 10/200: Train Loss = 1.0224, Val Loss = 1.0199, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 11/200: Train Loss = 1.0237, Val Loss = 1.0176, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 12/200: Train Loss = 1.0174, Val Loss = 1.0162, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 13/200: Train Loss = 1.0205, Val Loss = 1.0139, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 14/200: Train Loss = 1.0142, Val Loss = 1.0125, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 15/200: Train Loss = 1.0134, Val Loss = 1.0103, Val Acc = 0.4913\n",
      "  -> New best model saved.\n",
      "Epoch 16/200: Train Loss = 1.0059, Val Loss = 1.0081, Val Acc = 0.4942\n",
      "  -> New best model saved.\n",
      "Epoch 17/200: Train Loss = 1.0050, Val Loss = 1.0081, Val Acc = 0.4913\n",
      "  -> New best model saved.\n",
      "Epoch 18/200: Train Loss = 1.0022, Val Loss = 1.0048, Val Acc = 0.4913\n",
      "  -> New best model saved.\n",
      "Epoch 19/200: Train Loss = 0.9976, Val Loss = 1.0057, Val Acc = 0.4884\n",
      "Epoch 20/200: Train Loss = 0.9949, Val Loss = 1.0010, Val Acc = 0.5174\n",
      "  -> New best model saved.\n",
      "Epoch 21/200: Train Loss = 0.9899, Val Loss = 1.0046, Val Acc = 0.4884\n",
      "Epoch 22/200: Train Loss = 0.9879, Val Loss = 0.9992, Val Acc = 0.5000\n",
      "  -> New best model saved.\n",
      "Epoch 23/200: Train Loss = 0.9913, Val Loss = 0.9976, Val Acc = 0.5174\n",
      "  -> New best model saved.\n",
      "Epoch 24/200: Train Loss = 0.9835, Val Loss = 0.9955, Val Acc = 0.5349\n",
      "  -> New best model saved.\n",
      "Epoch 25/200: Train Loss = 0.9803, Val Loss = 0.9930, Val Acc = 0.5291\n",
      "  -> New best model saved.\n",
      "Epoch 26/200: Train Loss = 0.9700, Val Loss = 0.9969, Val Acc = 0.5058\n",
      "Epoch 27/200: Train Loss = 0.9696, Val Loss = 0.9883, Val Acc = 0.5378\n",
      "  -> New best model saved.\n",
      "Epoch 28/200: Train Loss = 0.9749, Val Loss = 0.9860, Val Acc = 0.5349\n",
      "  -> New best model saved.\n",
      "Epoch 29/200: Train Loss = 0.9589, Val Loss = 0.9845, Val Acc = 0.5291\n",
      "  -> New best model saved.\n",
      "Epoch 30/200: Train Loss = 0.9592, Val Loss = 0.9831, Val Acc = 0.5291\n",
      "  -> New best model saved.\n",
      "Epoch 31/200: Train Loss = 0.9577, Val Loss = 0.9805, Val Acc = 0.5320\n",
      "  -> New best model saved.\n",
      "Epoch 32/200: Train Loss = 0.9514, Val Loss = 0.9798, Val Acc = 0.5320\n",
      "  -> New best model saved.\n",
      "Epoch 33/200: Train Loss = 0.9535, Val Loss = 0.9772, Val Acc = 0.5378\n",
      "  -> New best model saved.\n",
      "Epoch 34/200: Train Loss = 0.9406, Val Loss = 0.9782, Val Acc = 0.5320\n",
      "Epoch 35/200: Train Loss = 0.9349, Val Loss = 0.9739, Val Acc = 0.5465\n",
      "  -> New best model saved.\n",
      "Epoch 36/200: Train Loss = 0.9283, Val Loss = 0.9749, Val Acc = 0.5494\n",
      "Epoch 37/200: Train Loss = 0.9324, Val Loss = 0.9717, Val Acc = 0.5552\n",
      "  -> New best model saved.\n",
      "Epoch 38/200: Train Loss = 0.9316, Val Loss = 0.9696, Val Acc = 0.5552\n",
      "  -> New best model saved.\n",
      "Epoch 39/200: Train Loss = 0.9209, Val Loss = 0.9684, Val Acc = 0.5552\n",
      "  -> New best model saved.\n",
      "Epoch 40/200: Train Loss = 0.9166, Val Loss = 0.9670, Val Acc = 0.5610\n",
      "  -> New best model saved.\n",
      "Epoch 41/200: Train Loss = 0.9132, Val Loss = 0.9661, Val Acc = 0.5465\n",
      "  -> New best model saved.\n",
      "Epoch 42/200: Train Loss = 0.9061, Val Loss = 0.9657, Val Acc = 0.5581\n",
      "  -> New best model saved.\n",
      "Epoch 43/200: Train Loss = 0.9011, Val Loss = 0.9663, Val Acc = 0.5669\n",
      "Epoch 44/200: Train Loss = 0.8980, Val Loss = 0.9583, Val Acc = 0.5581\n",
      "  -> New best model saved.\n",
      "Epoch 45/200: Train Loss = 0.8946, Val Loss = 0.9587, Val Acc = 0.5552\n",
      "Epoch 46/200: Train Loss = 0.8891, Val Loss = 0.9564, Val Acc = 0.5581\n",
      "  -> New best model saved.\n",
      "Epoch 47/200: Train Loss = 0.8865, Val Loss = 0.9542, Val Acc = 0.5669\n",
      "  -> New best model saved.\n",
      "Epoch 48/200: Train Loss = 0.8822, Val Loss = 0.9552, Val Acc = 0.5610\n",
      "Epoch 49/200: Train Loss = 0.8739, Val Loss = 0.9509, Val Acc = 0.5669\n",
      "  -> New best model saved.\n",
      "Epoch 50/200: Train Loss = 0.8669, Val Loss = 0.9520, Val Acc = 0.5610\n",
      "Epoch 51/200: Train Loss = 0.8564, Val Loss = 0.9513, Val Acc = 0.5465\n",
      "Epoch 52/200: Train Loss = 0.8575, Val Loss = 0.9475, Val Acc = 0.5698\n",
      "  -> New best model saved.\n",
      "Epoch 53/200: Train Loss = 0.8445, Val Loss = 0.9468, Val Acc = 0.5669\n",
      "  -> New best model saved.\n",
      "Epoch 54/200: Train Loss = 0.8402, Val Loss = 0.9492, Val Acc = 0.5698\n",
      "Epoch 55/200: Train Loss = 0.8433, Val Loss = 0.9455, Val Acc = 0.5669\n",
      "  -> New best model saved.\n",
      "Epoch 56/200: Train Loss = 0.8263, Val Loss = 0.9467, Val Acc = 0.5669\n",
      "Epoch 57/200: Train Loss = 0.8297, Val Loss = 0.9469, Val Acc = 0.5610\n",
      "Epoch 58/200: Train Loss = 0.8273, Val Loss = 0.9400, Val Acc = 0.5640\n",
      "  -> New best model saved.\n",
      "Epoch 59/200: Train Loss = 0.8195, Val Loss = 0.9390, Val Acc = 0.5610\n",
      "  -> New best model saved.\n",
      "Epoch 60/200: Train Loss = 0.8121, Val Loss = 0.9427, Val Acc = 0.5640\n",
      "Epoch 61/200: Train Loss = 0.8083, Val Loss = 0.9374, Val Acc = 0.5756\n",
      "  -> New best model saved.\n",
      "Epoch 62/200: Train Loss = 0.7967, Val Loss = 0.9370, Val Acc = 0.5756\n",
      "  -> New best model saved.\n",
      "Epoch 63/200: Train Loss = 0.7917, Val Loss = 0.9303, Val Acc = 0.5785\n",
      "  -> New best model saved.\n",
      "Epoch 64/200: Train Loss = 0.7862, Val Loss = 0.9309, Val Acc = 0.5756\n",
      "Epoch 65/200: Train Loss = 0.7885, Val Loss = 0.9266, Val Acc = 0.5843\n",
      "  -> New best model saved.\n",
      "Epoch 66/200: Train Loss = 0.7737, Val Loss = 0.9258, Val Acc = 0.5872\n",
      "  -> New best model saved.\n",
      "Epoch 67/200: Train Loss = 0.7599, Val Loss = 0.9415, Val Acc = 0.5698\n",
      "Epoch 68/200: Train Loss = 0.7576, Val Loss = 0.9231, Val Acc = 0.5843\n",
      "  -> New best model saved.\n",
      "Epoch 69/200: Train Loss = 0.7500, Val Loss = 0.9235, Val Acc = 0.5814\n",
      "Epoch 70/200: Train Loss = 0.7521, Val Loss = 0.9298, Val Acc = 0.5814\n",
      "Epoch 71/200: Train Loss = 0.7429, Val Loss = 0.9185, Val Acc = 0.5959\n",
      "  -> New best model saved.\n",
      "Epoch 72/200: Train Loss = 0.7301, Val Loss = 0.9150, Val Acc = 0.5988\n",
      "  -> New best model saved.\n",
      "Epoch 73/200: Train Loss = 0.7255, Val Loss = 0.9133, Val Acc = 0.6076\n",
      "  -> New best model saved.\n",
      "Epoch 74/200: Train Loss = 0.7233, Val Loss = 0.9134, Val Acc = 0.6105\n",
      "Epoch 75/200: Train Loss = 0.7128, Val Loss = 0.9118, Val Acc = 0.6047\n",
      "  -> New best model saved.\n",
      "Epoch 76/200: Train Loss = 0.7055, Val Loss = 0.9189, Val Acc = 0.6017\n",
      "Epoch 77/200: Train Loss = 0.7066, Val Loss = 0.9147, Val Acc = 0.5988\n",
      "Epoch 78/200: Train Loss = 0.6934, Val Loss = 0.9084, Val Acc = 0.5959\n",
      "  -> New best model saved.\n",
      "Epoch 79/200: Train Loss = 0.6820, Val Loss = 0.9062, Val Acc = 0.5959\n",
      "  -> New best model saved.\n",
      "Epoch 80/200: Train Loss = 0.6859, Val Loss = 0.9019, Val Acc = 0.6105\n",
      "  -> New best model saved.\n",
      "Epoch 81/200: Train Loss = 0.6691, Val Loss = 0.9105, Val Acc = 0.6017\n",
      "Epoch 82/200: Train Loss = 0.6637, Val Loss = 0.9033, Val Acc = 0.6047\n",
      "Epoch 83/200: Train Loss = 0.6663, Val Loss = 0.8982, Val Acc = 0.6076\n",
      "  -> New best model saved.\n",
      "Epoch 84/200: Train Loss = 0.6586, Val Loss = 0.8964, Val Acc = 0.6192\n",
      "  -> New best model saved.\n",
      "Epoch 85/200: Train Loss = 0.6408, Val Loss = 0.8956, Val Acc = 0.6221\n",
      "  -> New best model saved.\n",
      "Epoch 86/200: Train Loss = 0.6391, Val Loss = 0.9010, Val Acc = 0.6134\n",
      "Epoch 87/200: Train Loss = 0.6203, Val Loss = 0.8953, Val Acc = 0.6134\n",
      "  -> New best model saved.\n",
      "Epoch 88/200: Train Loss = 0.6169, Val Loss = 0.8944, Val Acc = 0.6134\n",
      "  -> New best model saved.\n",
      "Epoch 89/200: Train Loss = 0.6139, Val Loss = 0.8916, Val Acc = 0.6279\n",
      "  -> New best model saved.\n",
      "Epoch 90/200: Train Loss = 0.6098, Val Loss = 0.9009, Val Acc = 0.6105\n",
      "Epoch 91/200: Train Loss = 0.6077, Val Loss = 0.8932, Val Acc = 0.6105\n",
      "Epoch 92/200: Train Loss = 0.5996, Val Loss = 0.8860, Val Acc = 0.6250\n",
      "  -> New best model saved.\n",
      "Epoch 93/200: Train Loss = 0.5882, Val Loss = 0.8940, Val Acc = 0.6279\n",
      "Epoch 94/200: Train Loss = 0.5878, Val Loss = 0.8802, Val Acc = 0.6308\n",
      "  -> New best model saved.\n",
      "Epoch 95/200: Train Loss = 0.5811, Val Loss = 0.8817, Val Acc = 0.6250\n",
      "Epoch 96/200: Train Loss = 0.5644, Val Loss = 0.8861, Val Acc = 0.6250\n",
      "Epoch 97/200: Train Loss = 0.5640, Val Loss = 0.8861, Val Acc = 0.6221\n",
      "Epoch 98/200: Train Loss = 0.5554, Val Loss = 0.8921, Val Acc = 0.6308\n",
      "Epoch 99/200: Train Loss = 0.5377, Val Loss = 0.8801, Val Acc = 0.6250\n",
      "  -> New best model saved.\n",
      "Epoch 100/200: Train Loss = 0.5409, Val Loss = 0.8775, Val Acc = 0.6337\n",
      "  -> New best model saved.\n",
      "Epoch 101/200: Train Loss = 0.5351, Val Loss = 0.8732, Val Acc = 0.6366\n",
      "  -> New best model saved.\n",
      "Epoch 102/200: Train Loss = 0.5234, Val Loss = 0.8774, Val Acc = 0.6250\n",
      "Epoch 103/200: Train Loss = 0.5195, Val Loss = 0.8690, Val Acc = 0.6308\n",
      "  -> New best model saved.\n",
      "Epoch 104/200: Train Loss = 0.5169, Val Loss = 0.8704, Val Acc = 0.6453\n",
      "Epoch 105/200: Train Loss = 0.5094, Val Loss = 0.8887, Val Acc = 0.6424\n",
      "Epoch 106/200: Train Loss = 0.4932, Val Loss = 0.8635, Val Acc = 0.6424\n",
      "  -> New best model saved.\n",
      "Epoch 107/200: Train Loss = 0.4913, Val Loss = 0.8668, Val Acc = 0.6395\n",
      "Epoch 108/200: Train Loss = 0.4861, Val Loss = 0.8771, Val Acc = 0.6512\n",
      "Epoch 109/200: Train Loss = 0.4847, Val Loss = 0.8621, Val Acc = 0.6512\n",
      "  -> New best model saved.\n",
      "Epoch 110/200: Train Loss = 0.4900, Val Loss = 0.8680, Val Acc = 0.6424\n",
      "Epoch 111/200: Train Loss = 0.4612, Val Loss = 0.8719, Val Acc = 0.6424\n",
      "Epoch 112/200: Train Loss = 0.4570, Val Loss = 0.8673, Val Acc = 0.6512\n",
      "Epoch 113/200: Train Loss = 0.4460, Val Loss = 0.8625, Val Acc = 0.6512\n",
      "Epoch 114/200: Train Loss = 0.4408, Val Loss = 0.8570, Val Acc = 0.6424\n",
      "  -> New best model saved.\n",
      "Epoch 115/200: Train Loss = 0.4411, Val Loss = 0.8614, Val Acc = 0.6541\n",
      "Epoch 116/200: Train Loss = 0.4308, Val Loss = 0.8591, Val Acc = 0.6715\n",
      "Epoch 117/200: Train Loss = 0.4192, Val Loss = 0.8711, Val Acc = 0.6483\n",
      "Epoch 118/200: Train Loss = 0.4168, Val Loss = 0.8719, Val Acc = 0.6657\n",
      "Epoch 119/200: Train Loss = 0.4199, Val Loss = 0.8620, Val Acc = 0.6570\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 0.6144927536231884\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.59      0.46      0.52        72\n",
      "          CN       0.58      0.62      0.60       106\n",
      "         MCI       0.64      0.68      0.66       167\n",
      "\n",
      "    accuracy                           0.61       345\n",
      "   macro avg       0.61      0.59      0.59       345\n",
      "weighted avg       0.61      0.61      0.61       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load Features\n",
    "##########################################\n",
    "# Load DeiT features.\n",
    "train_features = np.load(\"Deit_train_features_1.npy\")  \n",
    "val_features   = np.load(\"Deit_val_features_1.npy\")      \n",
    "test_features  = np.load(\"Deit_test_features_1.npy\")     \n",
    "\n",
    "print(\"Train features shape:\", train_features.shape)\n",
    "print(\"Val features shape:\", val_features.shape)\n",
    "print(\"Test features shape:\", test_features.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "# Read the CSV files \n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "\n",
    "label_col = \"Group\"\n",
    "\n",
    "# Encode labels using LabelEncoder.\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create PyTorch Datasets and DataLoaders\n",
    "##########################################\n",
    "# Convert features and labels to tensors.\n",
    "train_features = torch.tensor(train_features, dtype=torch.float32)\n",
    "val_features   = torch.tensor(val_features, dtype=torch.float32)\n",
    "test_features  = torch.tensor(test_features, dtype=torch.float32)\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels   = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_labels  = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets.\n",
    "train_dataset = TensorDataset(train_features, train_labels)\n",
    "val_dataset   = TensorDataset(val_features, val_labels)\n",
    "test_dataset  = TensorDataset(test_features, test_labels)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define an MLP Classifier \n",
    "##########################################\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),  # First hidden layer with 256 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),        # Second hidden layer with 128 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes) # Output layer with num_classes neurons\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Example usage:\n",
    "input_dim = train_features.shape[1]  \n",
    "model_mlp = MLPClassifier(input_dim, num_classes)\n",
    "model_mlp.to(device)\n",
    "\n",
    "\n",
    "##########################################\n",
    "# 5. Train the MLP Classifier with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_mlp.parameters(), lr=2e-5)\n",
    "num_epochs = 200\n",
    "patience = 5  # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_mlp.train()\n",
    "    train_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_mlp(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Evaluate on the validation set.\n",
    "    model_mlp.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model_mlp(x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    val_acc = accuracy_score(all_targets, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping check.\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model_mlp.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model state.\n",
    "model_mlp.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model_mlp.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model_mlp(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585529c9-7c4d-467a-b492-13b482db26c4",
   "metadata": {},
   "source": [
    "# Uni Modal Tabular Data performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4969a38e-8b4c-408c-a036-526503a0f2a3",
   "metadata": {},
   "source": [
    "## Ft Transformer end to end training and evalaution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfba5e74-f629-48f5-8f6d-8619b84edfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  \n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = 42  # choose your seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e66305b4-d952-47ac-aca0-b30d0147cfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.0557 | Val Loss: 1.0274\n",
      "Epoch 2: Train Loss: 1.0233 | Val Loss: 0.9525\n",
      "Epoch 3: Train Loss: 0.7375 | Val Loss: 0.4822\n",
      "Epoch 4: Train Loss: 0.4742 | Val Loss: 0.3990\n",
      "Epoch 5: Train Loss: 0.4257 | Val Loss: 0.3785\n",
      "Epoch 6: Train Loss: 0.3891 | Val Loss: 0.3898\n",
      "Epoch 7: Train Loss: 0.3681 | Val Loss: 0.3462\n",
      "Epoch 8: Train Loss: 0.3403 | Val Loss: 0.3471\n",
      "Epoch 9: Train Loss: 0.3667 | Val Loss: 0.3960\n",
      "Epoch 10: Train Loss: 0.3602 | Val Loss: 0.3369\n",
      "Epoch 11: Train Loss: 0.3360 | Val Loss: 0.3430\n",
      "Epoch 12: Train Loss: 0.3377 | Val Loss: 0.3402\n",
      "Epoch 13: Train Loss: 0.3413 | Val Loss: 0.3201\n",
      "Epoch 14: Train Loss: 0.3366 | Val Loss: 0.3385\n",
      "Epoch 15: Train Loss: 0.3308 | Val Loss: 0.3481\n",
      "Epoch 16: Train Loss: 0.3233 | Val Loss: 0.3290\n",
      "Epoch 17: Train Loss: 0.3308 | Val Loss: 0.3324\n",
      "Epoch 18: Train Loss: 0.3194 | Val Loss: 0.3325\n",
      "Early stopping triggered.\n",
      "Trained model saved to best_ft_transformer_classification.pt\n",
      "Test Accuracy: 0.9043478260869565\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.83      0.81      0.82        72\n",
      "          CN       0.98      0.95      0.97       106\n",
      "         MCI       0.89      0.92      0.90       167\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.90      0.89      0.90       345\n",
      "weighted avg       0.90      0.90      0.90       345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_789196/921975057.py:191: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(save_model_path))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from rtdl_revisiting_models import FTTransformer\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Preprocess the Data\n",
    "##########################################\n",
    "# Load CSV files for train, validation, and test splits.\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "# Define feature columns and target label.\n",
    "numerical_features = [\"Age\", \"CDGLOBAL\", \"CDRSB\", \"MMSCORE\", \"HMSCORE\", \"NPISCORE\", \"GDTOTAL\"]\n",
    "categorical_features = [\"GENOTYPE\"]\n",
    "label = \"Group\"\n",
    "\n",
    "# Subset dataframes to desired columns.\n",
    "cols = numerical_features + categorical_features + [label]\n",
    "train_data = train_data[cols]\n",
    "val_data   = val_data[cols]\n",
    "test_data  = test_data[cols]\n",
    "\n",
    "# Handle missingness for numerical features.\n",
    "cols_with_missing = [\"CDRSB\", \"MMSCORE\", \"HMSCORE\", \"NPISCORE\", \"GDTOTAL\"]\n",
    "for col in cols_with_missing:\n",
    "    for df in [train_data, val_data, test_data]:\n",
    "        df[col + \"_is_missing\"] = df[col].isnull().astype(int)\n",
    "        df[col] = df[col].fillna(-999)\n",
    "\n",
    "# Extend continuous features to include missing indicators.\n",
    "numerical_features_extended = numerical_features + [col + \"_is_missing\" for col in cols_with_missing]\n",
    "\n",
    "# Encode categorical features using LabelEncoder.\n",
    "cat_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    train_data[col] = le.fit_transform(train_data[col].astype(str))\n",
    "    val_data[col]   = le.transform(val_data[col].astype(str))\n",
    "    test_data[col]  = le.transform(test_data[col].astype(str))\n",
    "    cat_encoders[col] = le\n",
    "\n",
    "# Encode the target.\n",
    "label_encoder = LabelEncoder()\n",
    "train_data[label] = label_encoder.fit_transform(train_data[label])\n",
    "val_data[label]   = label_encoder.transform(val_data[label])\n",
    "test_data[label]  = label_encoder.transform(test_data[label])\n",
    "num_classes = len(label_encoder.classes_)  \n",
    "\n",
    "##########################################\n",
    "# 2. Prepare NumPy Arrays and Create Dataset\n",
    "##########################################\n",
    "# Continuous features.\n",
    "X_train_cont = train_data[numerical_features_extended].values.astype(np.float32)\n",
    "X_val_cont   = val_data[numerical_features_extended].values.astype(np.float32)\n",
    "X_test_cont  = test_data[numerical_features_extended].values.astype(np.float32)\n",
    "\n",
    "# Categorical features.\n",
    "X_train_cat = train_data[categorical_features].values.astype(np.int64)\n",
    "X_val_cat   = val_data[categorical_features].values.astype(np.int64)\n",
    "X_test_cat  = test_data[categorical_features].values.astype(np.int64)\n",
    "\n",
    "# Labels.\n",
    "y_train = train_data[label].values.astype(np.int64)\n",
    "y_val   = val_data[label].values.astype(np.int64)\n",
    "y_test  = test_data[label].values.astype(np.int64)\n",
    "\n",
    "# Create a simple PyTorch Dataset.\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, cont, cat, labels):\n",
    "        self.cont = cont\n",
    "        self.cat = cat\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"cont\": torch.tensor(self.cont[idx], dtype=torch.float32),\n",
    "            \"cat\": torch.tensor(self.cat[idx], dtype=torch.long),\n",
    "            \"target\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = TabularDataset(X_train_cont, X_train_cat, y_train)\n",
    "val_dataset   = TabularDataset(X_val_cont, X_val_cat, y_val)\n",
    "test_dataset  = TabularDataset(X_test_cont, X_test_cat, y_test)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 3. Initialize and Train the FTTransformer Classifier\n",
    "##########################################\n",
    "# Get the number of continuous features.\n",
    "n_cont_features = X_train_cont.shape[1]\n",
    "cat_cardinalities = [int(train_data[col].nunique()) for col in categorical_features]\n",
    "\n",
    "# For classification, set d_out = number of classes.\n",
    "d_out = num_classes\n",
    "\n",
    "# Instantiate the FTTransformer.\n",
    "model = FTTransformer(\n",
    "    n_cont_features=n_cont_features,\n",
    "    cat_cardinalities=cat_cardinalities,\n",
    "    d_out=d_out,\n",
    "    n_blocks=3,\n",
    "    d_block=192,                # Backbone (hidden) dimension\n",
    "    attention_n_heads=8,\n",
    "    attention_dropout=0.2,\n",
    "    ffn_d_hidden=None,          \n",
    "    ffn_d_hidden_multiplier=4/3,\n",
    "    ffn_dropout=0.1,\n",
    "    residual_dropout=0.0\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set up optimizer, loss function, and scheduler.\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# Training loop with early stopping.\n",
    "max_epochs = 100\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        cont = batch[\"cont\"].to(device)\n",
    "        cat = batch[\"cat\"].to(device)\n",
    "        targets = batch[\"target\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(cont, cat) \n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * cont.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            cont = batch[\"cont\"].to(device)\n",
    "            cat = batch[\"cat\"].to(device)\n",
    "            targets = batch[\"target\"].to(device)\n",
    "            \n",
    "            logits = model(cont, cat)\n",
    "            loss = criterion(logits, targets)\n",
    "            val_loss += loss.item() * cont.size(0)\n",
    "    val_loss /= len(val_dataset)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Save the best model.\n",
    "save_model_path = \"best_ft_transformer_classification.pt\"\n",
    "torch.save(best_model_state, save_model_path)\n",
    "print(\"Trained model saved to\", save_model_path)\n",
    "\n",
    "# Load the best model.\n",
    "model.load_state_dict(torch.load(save_model_path))\n",
    "\n",
    "# Evaluate classification performance on the test set.\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        cont = batch[\"cont\"].to(device)\n",
    "        cat = batch[\"cat\"].to(device)\n",
    "        targets = batch[\"target\"].to(device)\n",
    "        \n",
    "        logits = model(cont, cat)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Classification Report (Test):\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d06fc855-97f0-48b0-8df1-352106a5f32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.0941 | Val Loss: 1.0624\n",
      "Epoch 2: Train Loss: 1.0491 | Val Loss: 1.0481\n",
      "Epoch 3: Train Loss: 1.0447 | Val Loss: 1.0407\n",
      "Epoch 4: Train Loss: 1.0392 | Val Loss: 1.0390\n",
      "Epoch 5: Train Loss: 1.0342 | Val Loss: 1.0369\n",
      "Epoch 6: Train Loss: 1.0341 | Val Loss: 1.0315\n",
      "Epoch 7: Train Loss: 1.0240 | Val Loss: 1.0204\n",
      "Epoch 8: Train Loss: 1.0204 | Val Loss: 1.0066\n",
      "Epoch 9: Train Loss: 1.0051 | Val Loss: 0.9841\n",
      "Epoch 10: Train Loss: 0.9869 | Val Loss: 0.9368\n",
      "Epoch 11: Train Loss: 0.9405 | Val Loss: 0.8596\n",
      "Epoch 12: Train Loss: 0.8710 | Val Loss: 0.7485\n",
      "Epoch 13: Train Loss: 0.7629 | Val Loss: 0.6240\n",
      "Epoch 14: Train Loss: 0.6741 | Val Loss: 0.5347\n",
      "Epoch 15: Train Loss: 0.6215 | Val Loss: 0.4861\n",
      "Epoch 16: Train Loss: 0.5702 | Val Loss: 0.4439\n",
      "Epoch 17: Train Loss: 0.5395 | Val Loss: 0.4257\n",
      "Epoch 18: Train Loss: 0.4969 | Val Loss: 0.4071\n",
      "Epoch 19: Train Loss: 0.4943 | Val Loss: 0.3988\n",
      "Epoch 20: Train Loss: 0.4719 | Val Loss: 0.3869\n",
      "Epoch 21: Train Loss: 0.4724 | Val Loss: 0.3813\n",
      "Epoch 22: Train Loss: 0.4293 | Val Loss: 0.3817\n",
      "Epoch 23: Train Loss: 0.4283 | Val Loss: 0.3675\n",
      "Epoch 24: Train Loss: 0.4261 | Val Loss: 0.3597\n",
      "Epoch 25: Train Loss: 0.4251 | Val Loss: 0.3637\n",
      "Epoch 26: Train Loss: 0.4212 | Val Loss: 0.3555\n",
      "Epoch 27: Train Loss: 0.4070 | Val Loss: 0.3595\n",
      "Epoch 28: Train Loss: 0.4079 | Val Loss: 0.3557\n",
      "Epoch 29: Train Loss: 0.3834 | Val Loss: 0.3619\n",
      "Early stopping triggered.\n",
      "Trained model saved to best_ft_transformer_classification.pt\n",
      "Test Accuracy: 0.9043478260869565\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.84      0.79      0.81        72\n",
      "          CN       0.98      0.95      0.97       106\n",
      "         MCI       0.89      0.92      0.90       167\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.90      0.89      0.89       345\n",
      "weighted avg       0.90      0.90      0.90       345\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 57   0  15]\n",
      " [  0 101   5]\n",
      " [ 11   2 154]]\n",
      "\n",
      "Direct Calculation for Accuracy:\n",
      "Standard Error: 0.0158\n",
      "Margin of Error (95% CI): 0.0310\n",
      "95% Confidence Interval for Accuracy: [0.8733, 0.9354]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_160858/3551740628.py:192: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(save_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bootstrapping for Accuracy:\n",
      "Bootstrapped Mean Accuracy: 0.9036\n",
      "Standard Error (Bootstrap): 0.0160\n",
      "Margin of Error (Bootstrap, 95% CI): 0.0314\n",
      "\n",
      "Weighted F1 Score: 0.9041073677435971\n",
      "\n",
      "Bootstrapping for Weighted F1 Score:\n",
      "Bootstrapped Mean Weighted F1: 0.9038\n",
      "Standard Error (Bootstrap) for Weighted F1: 0.0158\n",
      "Margin of Error (Bootstrap, 95% CI) for Weighted F1: 0.0311\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from rtdl_revisiting_models import FTTransformer  # Ensure this package is installed\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "import math\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Preprocess the Data\n",
    "##########################################\n",
    "# Load CSV files for train, validation, and test splits.\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "# Define feature columns and target label.\n",
    "numerical_features = [\"Age\", \"CDGLOBAL\", \"CDRSB\", \"MMSCORE\", \"HMSCORE\", \"NPISCORE\", \"GDTOTAL\"]\n",
    "categorical_features = [\"GENOTYPE\"]\n",
    "label = \"Group\"\n",
    "\n",
    "# Subset dataframes to desired columns.\n",
    "cols = numerical_features + categorical_features + [label]\n",
    "train_data = train_data[cols]\n",
    "val_data   = val_data[cols]\n",
    "test_data  = test_data[cols]\n",
    "\n",
    "# Handle missingness for numerical features.\n",
    "cols_with_missing = [\"CDRSB\", \"MMSCORE\", \"HMSCORE\", \"NPISCORE\", \"GDTOTAL\"]\n",
    "for col in cols_with_missing:\n",
    "    for df in [train_data, val_data, test_data]:\n",
    "        # Create a missing indicator\n",
    "        df[col + \"_is_missing\"] = df[col].isnull().astype(int)\n",
    "        # Use a sentinel value (-999) for missing data\n",
    "        df[col] = df[col].fillna(-999)\n",
    "\n",
    "# Extend the list of continuous features to include missing indicators.\n",
    "numerical_features_extended = numerical_features + [col + \"_is_missing\" for col in cols_with_missing]\n",
    "\n",
    "# Encode categorical features.\n",
    "cat_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    train_data[col] = le.fit_transform(train_data[col].astype(str))\n",
    "    val_data[col]   = le.transform(val_data[col].astype(str))\n",
    "    test_data[col]  = le.transform(test_data[col].astype(str))\n",
    "    cat_encoders[col] = le\n",
    "\n",
    "# Encode the target labels.\n",
    "label_encoder = LabelEncoder()\n",
    "train_data[label] = label_encoder.fit_transform(train_data[label])\n",
    "val_data[label]   = label_encoder.transform(val_data[label])\n",
    "test_data[label]  = label_encoder.transform(test_data[label])\n",
    "num_classes = len(label_encoder.classes_)  \n",
    "\n",
    "##########################################\n",
    "# 2. Prepare NumPy Arrays for Model Inputs\n",
    "##########################################\n",
    "# Continuous features.\n",
    "X_train_cont = train_data[numerical_features_extended].values.astype(np.float32)\n",
    "X_val_cont   = val_data[numerical_features_extended].values.astype(np.float32)\n",
    "X_test_cont  = test_data[numerical_features_extended].values.astype(np.float32)\n",
    "\n",
    "# Categorical features.\n",
    "X_train_cat = train_data[categorical_features].values.astype(np.int64)\n",
    "X_val_cat   = val_data[categorical_features].values.astype(np.int64)\n",
    "X_test_cat  = test_data[categorical_features].values.astype(np.int64)\n",
    "\n",
    "# Labels.\n",
    "y_train = train_data[label].values.astype(np.int64)\n",
    "y_val   = val_data[label].values.astype(np.int64)\n",
    "y_test  = test_data[label].values.astype(np.int64)\n",
    "\n",
    "##########################################\n",
    "# 3. Create a PyTorch Dataset and DataLoaders\n",
    "##########################################\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, cont, cat, labels):\n",
    "        self.cont = cont\n",
    "        self.cat = cat\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"cont\": torch.tensor(self.cont[idx], dtype=torch.float32),\n",
    "            \"cat\": torch.tensor(self.cat[idx], dtype=torch.long),\n",
    "            \"target\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = TabularDataset(X_train_cont, X_train_cat, y_train)\n",
    "val_dataset   = TabularDataset(X_val_cont, X_val_cat, y_val)\n",
    "test_dataset  = TabularDataset(X_test_cont, X_test_cat, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Initialize the FTTransformer Model\n",
    "##########################################\n",
    "n_cont_features = X_train_cont.shape[1]\n",
    "cat_cardinalities = [int(train_data[col].nunique()) for col in categorical_features]\n",
    "d_out = num_classes  # Number of classes for classification\n",
    "\n",
    "model = FTTransformer(\n",
    "    n_cont_features=n_cont_features,\n",
    "    cat_cardinalities=cat_cardinalities,\n",
    "    d_out=d_out,\n",
    "    n_blocks=3,\n",
    "    d_block=192,                # Backbone (hidden) dimension\n",
    "    attention_n_heads=8,\n",
    "    attention_dropout=0.2,\n",
    "    ffn_d_hidden=None,\n",
    "    ffn_d_hidden_multiplier=4/3,\n",
    "    ffn_dropout=0.1,\n",
    "    residual_dropout=0.0\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "##########################################\n",
    "# 5. Set Up Optimizer, Loss, and Scheduler\n",
    "##########################################\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "##########################################\n",
    "# 6. Training Loop with Early Stopping\n",
    "##########################################\n",
    "max_epochs = 100\n",
    "patience = 3\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        cont = batch[\"cont\"].to(device)\n",
    "        cat = batch[\"cat\"].to(device)\n",
    "        targets = batch[\"target\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(cont, cat)\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * cont.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            cont = batch[\"cont\"].to(device)\n",
    "            cat = batch[\"cat\"].to(device)\n",
    "            targets = batch[\"target\"].to(device)\n",
    "            \n",
    "            logits = model(cont, cat)\n",
    "            loss = criterion(logits, targets)\n",
    "            val_loss += loss.item() * cont.size(0)\n",
    "    val_loss /= len(val_dataset)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Save the best model.\n",
    "save_model_path = \"best_ft_transformer_classification.pt\"\n",
    "torch.save(best_model_state, save_model_path)\n",
    "print(\"Trained model saved to\", save_model_path)\n",
    "\n",
    "# Load the best model.\n",
    "model.load_state_dict(torch.load(save_model_path))\n",
    "\n",
    "##########################################\n",
    "# 7. Evaluate on the Test Set\n",
    "##########################################\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        cont = batch[\"cont\"].to(device)\n",
    "        cat = batch[\"cat\"].to(device)\n",
    "        targets = batch[\"target\"].to(device)\n",
    "        \n",
    "        logits = model(cont, cat)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Classification Report (Test):\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))\n",
    "\n",
    "conf_matrix = confusion_matrix(all_targets, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "##########################################\n",
    "# 8. Compute Margin of Error for Test Accuracy and Weighted F1 Score\n",
    "##########################################\n",
    "\n",
    "# -- Accuracy Margin of Error --\n",
    "\n",
    "# Approach 1: Direct Calculation (Binomial Formula)\n",
    "n_test = len(test_dataset)\n",
    "se_direct = math.sqrt(test_acc * (1 - test_acc) / n_test)\n",
    "moe_direct = 1.96 * se_direct\n",
    "print(\"\\nDirect Calculation for Accuracy:\")\n",
    "print(\"Standard Error: {:.4f}\".format(se_direct))\n",
    "print(\"Margin of Error (95% CI): {:.4f}\".format(moe_direct))\n",
    "print(\"95% Confidence Interval for Accuracy: [{:.4f}, {:.4f}]\".format(test_acc - moe_direct, test_acc + moe_direct))\n",
    "\n",
    "# Approach 2: Bootstrapping for Accuracy\n",
    "n_bootstrap = 1000\n",
    "boot_accs = []\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_test), n_test, replace=True)\n",
    "    acc_bs = accuracy_score(all_targets[indices], all_preds[indices])\n",
    "    boot_accs.append(acc_bs)\n",
    "boot_accs = np.array(boot_accs)\n",
    "se_bootstrap = np.std(boot_accs)\n",
    "moe_bootstrap = 1.96 * se_bootstrap\n",
    "print(\"\\nBootstrapping for Accuracy:\")\n",
    "print(\"Bootstrapped Mean Accuracy: {:.4f}\".format(np.mean(boot_accs)))\n",
    "print(\"Standard Error (Bootstrap): {:.4f}\".format(se_bootstrap))\n",
    "print(\"Margin of Error (Bootstrap, 95% CI): {:.4f}\".format(moe_bootstrap))\n",
    "\n",
    "# -- Weighted F1 Score Margin of Error --\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "weighted_f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "print(\"\\nWeighted F1 Score:\", weighted_f1)\n",
    "\n",
    "# Bootstrapping for Weighted F1 Score\n",
    "boot_f1s = []\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_test), n_test, replace=True)\n",
    "    f1_bs = f1_score(all_targets[indices], all_preds[indices], average='weighted')\n",
    "    boot_f1s.append(f1_bs)\n",
    "boot_f1s = np.array(boot_f1s)\n",
    "se_boot_f1 = np.std(boot_f1s)\n",
    "moe_boot_f1 = 1.96 * se_boot_f1\n",
    "print(\"\\nBootstrapping for Weighted F1 Score:\")\n",
    "print(\"Bootstrapped Mean Weighted F1: {:.4f}\".format(np.mean(boot_f1s)))\n",
    "print(\"Standard Error (Bootstrap) for Weighted F1: {:.4f}\".format(se_boot_f1))\n",
    "print(\"Margin of Error (Bootstrap, 95% CI) for Weighted F1: {:.4f}\".format(moe_boot_f1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "604b5511-e6cf-485f-8b40-4037e8395c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.0654 | Val Loss: 1.0424\n",
      "Epoch 2: Train Loss: 1.0517 | Val Loss: 1.0434\n",
      "Epoch 3: Train Loss: 1.0511 | Val Loss: 1.0405\n",
      "Epoch 4: Train Loss: 1.0488 | Val Loss: 1.0365\n",
      "Epoch 5: Train Loss: 1.0410 | Val Loss: 1.0308\n",
      "Epoch 6: Train Loss: 1.0443 | Val Loss: 1.0216\n",
      "Epoch 7: Train Loss: 1.0344 | Val Loss: 1.0134\n",
      "Epoch 8: Train Loss: 1.0237 | Val Loss: 0.9850\n",
      "Epoch 9: Train Loss: 1.0094 | Val Loss: 0.9264\n",
      "Epoch 10: Train Loss: 0.9603 | Val Loss: 0.7648\n",
      "Epoch 11: Train Loss: 0.8468 | Val Loss: 0.5637\n",
      "Epoch 12: Train Loss: 0.6800 | Val Loss: 0.4848\n",
      "Epoch 13: Train Loss: 0.6010 | Val Loss: 0.4825\n",
      "Epoch 14: Train Loss: 0.5670 | Val Loss: 0.4736\n",
      "Epoch 15: Train Loss: 0.5469 | Val Loss: 0.4538\n",
      "Epoch 16: Train Loss: 0.5398 | Val Loss: 0.4465\n",
      "Epoch 17: Train Loss: 0.5258 | Val Loss: 0.4279\n",
      "Epoch 18: Train Loss: 0.5109 | Val Loss: 0.4256\n",
      "Epoch 19: Train Loss: 0.4954 | Val Loss: 0.4160\n",
      "Epoch 20: Train Loss: 0.4683 | Val Loss: 0.4091\n",
      "Epoch 21: Train Loss: 0.4589 | Val Loss: 0.4136\n",
      "Epoch 22: Train Loss: 0.4524 | Val Loss: 0.4090\n",
      "Epoch 23: Train Loss: 0.4428 | Val Loss: 0.4210\n",
      "Epoch 24: Train Loss: 0.4338 | Val Loss: 0.3665\n",
      "Epoch 25: Train Loss: 0.4200 | Val Loss: 0.3787\n",
      "Epoch 26: Train Loss: 0.4034 | Val Loss: 0.3745\n",
      "Epoch 27: Train Loss: 0.3804 | Val Loss: 0.3664\n",
      "Epoch 28: Train Loss: 0.3912 | Val Loss: 0.3895\n",
      "Epoch 29: Train Loss: 0.3707 | Val Loss: 0.3830\n",
      "Epoch 30: Train Loss: 0.3774 | Val Loss: 0.3960\n",
      "Early stopping triggered.\n",
      "Trained model saved to best_ft_transformer_classification.pt\n",
      "Test Accuracy: 0.8869565217391304\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.71      0.93      0.81        72\n",
      "          CN       0.98      0.95      0.97       106\n",
      "         MCI       0.93      0.83      0.88       167\n",
      "\n",
      "    accuracy                           0.89       345\n",
      "   macro avg       0.88      0.90      0.88       345\n",
      "weighted avg       0.90      0.89      0.89       345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_160858/3081910025.py:191: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(save_model_path))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Import the FTTransformer model from rtdl_revisiting_models.\n",
    "from rtdl_revisiting_models import FTTransformer\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Preprocess the Data\n",
    "##########################################\n",
    "# Load CSV files for train, validation, and test splits.\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "# Define feature columns and target label.\n",
    "numerical_features = [\"Age\", \"CDGLOBAL\", \"CDRSB\", \"MMSCORE\", \"HMSCORE\", \"NPISCORE\", \"GDTOTAL\"]\n",
    "categorical_features = [\"GENOTYPE\"]\n",
    "label = \"Group\"\n",
    "\n",
    "# Subset dataframes to desired columns.\n",
    "cols = numerical_features + categorical_features + [label]\n",
    "train_data = train_data[cols]\n",
    "val_data   = val_data[cols]\n",
    "test_data  = test_data[cols]\n",
    "\n",
    "# Handle missingness for numerical features.\n",
    "cols_with_missing = [\"CDRSB\", \"MMSCORE\", \"HMSCORE\", \"NPISCORE\", \"GDTOTAL\"]\n",
    "for col in cols_with_missing:\n",
    "    for df in [train_data, val_data, test_data]:\n",
    "        df[col + \"_is_missing\"] = df[col].isnull().astype(int)\n",
    "        df[col] = df[col].fillna(-999)\n",
    "\n",
    "# Extend continuous features to include missing indicators.\n",
    "numerical_features_extended = numerical_features + [col + \"_is_missing\" for col in cols_with_missing]\n",
    "\n",
    "# Encode categorical features using LabelEncoder.\n",
    "cat_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    train_data[col] = le.fit_transform(train_data[col].astype(str))\n",
    "    val_data[col]   = le.transform(val_data[col].astype(str))\n",
    "    test_data[col]  = le.transform(test_data[col].astype(str))\n",
    "    cat_encoders[col] = le\n",
    "\n",
    "# Encode the target.\n",
    "label_encoder = LabelEncoder()\n",
    "train_data[label] = label_encoder.fit_transform(train_data[label])\n",
    "val_data[label]   = label_encoder.transform(val_data[label])\n",
    "test_data[label]  = label_encoder.transform(test_data[label])\n",
    "num_classes = len(label_encoder.classes_)  # e.g., 3 for classification\n",
    "\n",
    "##########################################\n",
    "# 2. Prepare NumPy Arrays and Create Dataset\n",
    "##########################################\n",
    "# Continuous features (including missing indicators).\n",
    "X_train_cont = train_data[numerical_features_extended].values.astype(np.float32)\n",
    "X_val_cont   = val_data[numerical_features_extended].values.astype(np.float32)\n",
    "X_test_cont  = test_data[numerical_features_extended].values.astype(np.float32)\n",
    "\n",
    "# Categorical features.\n",
    "X_train_cat = train_data[categorical_features].values.astype(np.int64)\n",
    "X_val_cat   = val_data[categorical_features].values.astype(np.int64)\n",
    "X_test_cat  = test_data[categorical_features].values.astype(np.int64)\n",
    "\n",
    "# Labels.\n",
    "y_train = train_data[label].values.astype(np.int64)\n",
    "y_val   = val_data[label].values.astype(np.int64)\n",
    "y_test  = test_data[label].values.astype(np.int64)\n",
    "\n",
    "# Create a simple PyTorch Dataset.\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, cont, cat, labels):\n",
    "        self.cont = cont\n",
    "        self.cat = cat\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"cont\": torch.tensor(self.cont[idx], dtype=torch.float32),\n",
    "            \"cat\": torch.tensor(self.cat[idx], dtype=torch.long),\n",
    "            \"target\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = TabularDataset(X_train_cont, X_train_cat, y_train)\n",
    "val_dataset   = TabularDataset(X_val_cont, X_val_cat, y_val)\n",
    "test_dataset  = TabularDataset(X_test_cont, X_test_cat, y_test)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 3. Initialize and Train the FTTransformer Classifier\n",
    "##########################################\n",
    "# Get the number of continuous features.\n",
    "n_cont_features = X_train_cont.shape[1]\n",
    "# Determine the cardinalities for each categorical feature.\n",
    "cat_cardinalities = [int(train_data[col].nunique()) for col in categorical_features]\n",
    "\n",
    "# For classification, set d_out = number of classes.\n",
    "d_out = num_classes\n",
    "\n",
    "model = FTTransformer(\n",
    "    n_cont_features=n_cont_features,\n",
    "    cat_cardinalities=cat_cardinalities,\n",
    "    d_out=d_out,\n",
    "    n_blocks=2,                # default is 3 blocks\n",
    "    d_block=192,               # default backbone dimension is 192 (kept unchanged)\n",
    "    attention_n_heads=8,       # default is 8 heads\n",
    "    attention_dropout=0.2,     # default dropout in attention\n",
    "    ffn_d_hidden=None,         # will be set internally as int(192 * 4/3)\n",
    "    ffn_d_hidden_multiplier=4/3,\n",
    "    ffn_dropout=0.3,           # default FFN dropout\n",
    "    residual_dropout=0.2       # default residual dropout\n",
    ")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set up optimizer, loss function, and scheduler.\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# Training loop with early stopping.\n",
    "max_epochs = 100\n",
    "patience = 3\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        cont = batch[\"cont\"].to(device)\n",
    "        cat = batch[\"cat\"].to(device)\n",
    "        targets = batch[\"target\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(cont, cat)  # Forward pass returns logits (shape: [batch_size, d_out])\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * cont.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            cont = batch[\"cont\"].to(device)\n",
    "            cat = batch[\"cat\"].to(device)\n",
    "            targets = batch[\"target\"].to(device)\n",
    "            \n",
    "            logits = model(cont, cat)\n",
    "            loss = criterion(logits, targets)\n",
    "            val_loss += loss.item() * cont.size(0)\n",
    "    val_loss /= len(val_dataset)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Save the best model.\n",
    "save_model_path = \"best_ft_transformer_classification.pt\"\n",
    "torch.save(best_model_state, save_model_path)\n",
    "print(\"Trained model saved to\", save_model_path)\n",
    "\n",
    "# Load the best model (optional).\n",
    "model.load_state_dict(torch.load(save_model_path))\n",
    "\n",
    "# Evaluate classification performance on the test set.\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        cont = batch[\"cont\"].to(device)\n",
    "        cat = batch[\"cat\"].to(device)\n",
    "        targets = batch[\"target\"].to(device)\n",
    "        \n",
    "        logits = model(cont, cat)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Classification Report (Test):\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20f6340c-ba46-4d79-9596-0d644c25ec18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape: (1605, 192)\n",
      "Val features shape: (344, 192)\n",
      "Test features shape: (345, 192)\n",
      "Epoch 1/200: Train Loss = 1.0474, Val Loss = 0.9516, Val Acc = 0.8488\n",
      "  -> New best model saved.\n",
      "Epoch 2/200: Train Loss = 0.8777, Val Loss = 0.7740, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 3/200: Train Loss = 0.7044, Val Loss = 0.6027, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 4/200: Train Loss = 0.5542, Val Loss = 0.4837, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 5/200: Train Loss = 0.4558, Val Loss = 0.4125, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 6/200: Train Loss = 0.3964, Val Loss = 0.3738, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 7/200: Train Loss = 0.3639, Val Loss = 0.3547, Val Acc = 0.8605\n",
      "  -> New best model saved.\n",
      "Epoch 8/200: Train Loss = 0.3322, Val Loss = 0.3444, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 9/200: Train Loss = 0.3292, Val Loss = 0.3389, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 10/200: Train Loss = 0.3188, Val Loss = 0.3368, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 11/200: Train Loss = 0.3175, Val Loss = 0.3357, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 12/200: Train Loss = 0.3085, Val Loss = 0.3345, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 13/200: Train Loss = 0.3160, Val Loss = 0.3348, Val Acc = 0.8547\n",
      "Epoch 14/200: Train Loss = 0.3166, Val Loss = 0.3348, Val Acc = 0.8547\n",
      "Epoch 15/200: Train Loss = 0.3116, Val Loss = 0.3349, Val Acc = 0.8576\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 0.9130434782608695\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.84      0.85      0.84        72\n",
      "          CN       0.98      0.95      0.97       106\n",
      "         MCI       0.91      0.92      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.91      0.91      0.91       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Concatenate Features\n",
    "##########################################\n",
    "# Load DeiT features.\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")        \n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")          \n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")   \n",
    "\n",
    "\n",
    "# Concatenate features along the last axis.\n",
    "train_features = ft_train\n",
    "val_features   = ft_val\n",
    "test_features  = ft_test\n",
    "\n",
    "print(\"Train features shape:\", train_features.shape)\n",
    "print(\"Val features shape:\", val_features.shape)\n",
    "print(\"Test features shape:\", test_features.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "# Read the CSV files (adjust filenames if needed)\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "# We assume the label column is named \"Group\"\n",
    "label_col = \"Group\"\n",
    "\n",
    "# Encode labels using LabelEncoder.\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create PyTorch Datasets and DataLoaders\n",
    "##########################################\n",
    "# Convert features and labels to tensors.\n",
    "train_features = torch.tensor(train_features, dtype=torch.float32)\n",
    "val_features   = torch.tensor(val_features, dtype=torch.float32)\n",
    "test_features  = torch.tensor(test_features, dtype=torch.float32)\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels   = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_labels  = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets.\n",
    "train_dataset = TensorDataset(train_features, train_labels)\n",
    "val_dataset   = TensorDataset(val_features, val_labels)\n",
    "test_dataset  = TensorDataset(test_features, test_labels)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define an MLP Classifier for Fused Features\n",
    "##########################################\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),  # First hidden layer with 256 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),        # Second hidden layer with 128 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes) # Output layer with num_classes neurons\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Example usage:\n",
    "input_dim = train_features.shape[1]  # Assuming train_features is defined and extracted\n",
    "model_mlp = MLPClassifier(input_dim, num_classes)\n",
    "model_mlp.to(device)\n",
    "\n",
    "\n",
    "##########################################\n",
    "# 5. Train the MLP Classifier with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_mlp.parameters(), lr=2e-5)\n",
    "num_epochs = 200\n",
    "patience = 3  # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_mlp.train()\n",
    "    train_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_mlp(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Evaluate on the validation set.\n",
    "    model_mlp.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model_mlp(x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    val_acc = accuracy_score(all_targets, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping check.\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model_mlp.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model state.\n",
    "model_mlp.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model_mlp.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model_mlp(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7e0049d-04a8-45f4-8eca-d017091822a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused train features shape: (1605, 960)\n",
      "Fused val features shape: (344, 960)\n",
      "Fused test features shape: (345, 960)\n",
      "Epoch 1/100: Train Loss = 0.9981, Val Loss = 0.8910, Val Acc = 0.5174\n",
      "  -> New best model saved.\n",
      "Epoch 2/100: Train Loss = 0.8369, Val Loss = 0.7198, Val Acc = 0.7791\n",
      "  -> New best model saved.\n",
      "Epoch 3/100: Train Loss = 0.6735, Val Loss = 0.5631, Val Acc = 0.7965\n",
      "  -> New best model saved.\n",
      "Epoch 4/100: Train Loss = 0.5425, Val Loss = 0.4538, Val Acc = 0.8372\n",
      "  -> New best model saved.\n",
      "Epoch 5/100: Train Loss = 0.4564, Val Loss = 0.3931, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 6/100: Train Loss = 0.3984, Val Loss = 0.3550, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 7/100: Train Loss = 0.3706, Val Loss = 0.3340, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 8/100: Train Loss = 0.3359, Val Loss = 0.3237, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 9/100: Train Loss = 0.3238, Val Loss = 0.3141, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 10/100: Train Loss = 0.3146, Val Loss = 0.3097, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 11/100: Train Loss = 0.3060, Val Loss = 0.3074, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 12/100: Train Loss = 0.2967, Val Loss = 0.3059, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 13/100: Train Loss = 0.3038, Val Loss = 0.3057, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 14/100: Train Loss = 0.2963, Val Loss = 0.3036, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 15/100: Train Loss = 0.2844, Val Loss = 0.3030, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 16/100: Train Loss = 0.2831, Val Loss = 0.3023, Val Acc = 0.8866\n",
      "  -> New best model saved.\n",
      "Epoch 17/100: Train Loss = 0.2845, Val Loss = 0.3011, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 18/100: Train Loss = 0.2787, Val Loss = 0.3020, Val Acc = 0.8866\n",
      "Epoch 19/100: Train Loss = 0.2811, Val Loss = 0.3023, Val Acc = 0.8721\n",
      "Epoch 20/100: Train Loss = 0.2741, Val Loss = 0.3011, Val Acc = 0.8808\n",
      "Epoch 21/100: Train Loss = 0.2700, Val Loss = 0.3004, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 22/100: Train Loss = 0.2708, Val Loss = 0.3033, Val Acc = 0.8663\n",
      "Epoch 23/100: Train Loss = 0.2729, Val Loss = 0.2984, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 24/100: Train Loss = 0.2641, Val Loss = 0.2984, Val Acc = 0.8808\n",
      "Epoch 25/100: Train Loss = 0.2669, Val Loss = 0.2995, Val Acc = 0.8779\n",
      "Epoch 26/100: Train Loss = 0.2672, Val Loss = 0.2958, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 27/100: Train Loss = 0.2523, Val Loss = 0.2945, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 28/100: Train Loss = 0.2586, Val Loss = 0.2978, Val Acc = 0.8779\n",
      "Epoch 29/100: Train Loss = 0.2539, Val Loss = 0.3016, Val Acc = 0.8517\n",
      "Epoch 30/100: Train Loss = 0.2558, Val Loss = 0.2975, Val Acc = 0.8779\n",
      "Epoch 31/100: Train Loss = 0.2508, Val Loss = 0.2949, Val Acc = 0.8750\n",
      "Epoch 32/100: Train Loss = 0.2498, Val Loss = 0.2937, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 33/100: Train Loss = 0.2378, Val Loss = 0.2931, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 34/100: Train Loss = 0.2465, Val Loss = 0.2933, Val Acc = 0.8779\n",
      "Epoch 35/100: Train Loss = 0.2479, Val Loss = 0.2929, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 36/100: Train Loss = 0.2397, Val Loss = 0.2935, Val Acc = 0.8750\n",
      "Epoch 37/100: Train Loss = 0.2441, Val Loss = 0.2916, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 38/100: Train Loss = 0.2318, Val Loss = 0.2948, Val Acc = 0.8779\n",
      "Epoch 39/100: Train Loss = 0.2363, Val Loss = 0.2956, Val Acc = 0.8779\n",
      "Epoch 40/100: Train Loss = 0.2323, Val Loss = 0.2984, Val Acc = 0.8750\n",
      "Epoch 41/100: Train Loss = 0.2343, Val Loss = 0.2917, Val Acc = 0.8779\n",
      "Epoch 42/100: Train Loss = 0.2308, Val Loss = 0.2935, Val Acc = 0.8721\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 0.9130434782608695\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.85      0.83      0.84        72\n",
      "          CN       0.98      0.95      0.97       106\n",
      "         MCI       0.90      0.92      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.91      0.90      0.91       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Concatenate Features\n",
    "##########################################\n",
    "# Load DeiT features.\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")  \n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")      \n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")     \n",
    "\n",
    "# Load FTTransformer features.\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")        \n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")          \n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")         \n",
    "\n",
    "# Concatenate features along the last axis.\n",
    "train_features = np.concatenate((deit_train, ft_train), axis=1)\n",
    "val_features   = np.concatenate((deit_val, ft_val), axis=1)\n",
    "test_features  = np.concatenate((deit_test, ft_test), axis=1)\n",
    "\n",
    "print(\"Fused train features shape:\", train_features.shape)\n",
    "print(\"Fused val features shape:\", val_features.shape)\n",
    "print(\"Fused test features shape:\", test_features.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "# Read the CSV files \n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "\n",
    "label_col = \"Group\"\n",
    "\n",
    "# Encode labels using LabelEncoder.\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create PyTorch Datasets and DataLoaders\n",
    "##########################################\n",
    "# Convert features and labels to tensors.\n",
    "train_features = torch.tensor(train_features, dtype=torch.float32)\n",
    "val_features   = torch.tensor(val_features, dtype=torch.float32)\n",
    "test_features  = torch.tensor(test_features, dtype=torch.float32)\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels   = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_labels  = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets.\n",
    "train_dataset = TensorDataset(train_features, train_labels)\n",
    "val_dataset   = TensorDataset(val_features, val_labels)\n",
    "test_dataset  = TensorDataset(test_features, test_labels)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define an MLP Classifier for Fused Features\n",
    "##########################################\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),  # First hidden layer with 256 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),        # Second hidden layer with 128 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes) # Output layer with num_classes neurons\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "input_dim = train_features.shape[1]  \n",
    "model_mlp = MLPClassifier(input_dim, num_classes)\n",
    "model_mlp.to(device)\n",
    "\n",
    "\n",
    "##########################################\n",
    "# 5. Train the MLP Classifier with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_mlp.parameters(), lr=2e-5)\n",
    "num_epochs = 100\n",
    "patience = 5  # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_mlp.train()\n",
    "    train_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_mlp(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Evaluate on the validation set.\n",
    "    model_mlp.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model_mlp(x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    val_acc = accuracy_score(all_targets, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping check.\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model_mlp.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model state.\n",
    "model_mlp.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model_mlp.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model_mlp(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c294f1e-e08f-4605-8e50-2efcfcd2b4cd",
   "metadata": {},
   "source": [
    "## Early Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b6dfd4d-21bf-4f58-a411-dd0c0ab290e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused train features shape: (1605, 960)\n",
      "Fused val features shape: (344, 960)\n",
      "Fused test features shape: (345, 960)\n",
      "Epoch 1/100: Train Loss = 1.0067, Val Loss = 0.9130, Val Acc = 0.6017\n",
      "  -> New best model saved.\n",
      "Epoch 2/100: Train Loss = 0.8532, Val Loss = 0.7509, Val Acc = 0.7674\n",
      "  -> New best model saved.\n",
      "Epoch 3/100: Train Loss = 0.7080, Val Loss = 0.5965, Val Acc = 0.7791\n",
      "  -> New best model saved.\n",
      "Epoch 4/100: Train Loss = 0.5748, Val Loss = 0.4862, Val Acc = 0.8052\n",
      "  -> New best model saved.\n",
      "Epoch 5/100: Train Loss = 0.4803, Val Loss = 0.4129, Val Acc = 0.8459\n",
      "  -> New best model saved.\n",
      "Epoch 6/100: Train Loss = 0.4219, Val Loss = 0.3661, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 7/100: Train Loss = 0.3768, Val Loss = 0.3411, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 8/100: Train Loss = 0.3560, Val Loss = 0.3246, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 9/100: Train Loss = 0.3415, Val Loss = 0.3150, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 10/100: Train Loss = 0.3240, Val Loss = 0.3082, Val Acc = 0.8866\n",
      "  -> New best model saved.\n",
      "Epoch 11/100: Train Loss = 0.3108, Val Loss = 0.3057, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 12/100: Train Loss = 0.3130, Val Loss = 0.3087, Val Acc = 0.8721\n",
      "Epoch 13/100: Train Loss = 0.2930, Val Loss = 0.3009, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 14/100: Train Loss = 0.2933, Val Loss = 0.2998, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 15/100: Train Loss = 0.2959, Val Loss = 0.3020, Val Acc = 0.8721\n",
      "Epoch 16/100: Train Loss = 0.2783, Val Loss = 0.3040, Val Acc = 0.8721\n",
      "Epoch 17/100: Train Loss = 0.2815, Val Loss = 0.2991, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 18/100: Train Loss = 0.2733, Val Loss = 0.3000, Val Acc = 0.8866\n",
      "Epoch 19/100: Train Loss = 0.2835, Val Loss = 0.2989, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 20/100: Train Loss = 0.2700, Val Loss = 0.2977, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 21/100: Train Loss = 0.2724, Val Loss = 0.2961, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 22/100: Train Loss = 0.2652, Val Loss = 0.2954, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 23/100: Train Loss = 0.2602, Val Loss = 0.2966, Val Acc = 0.8779\n",
      "Epoch 24/100: Train Loss = 0.2678, Val Loss = 0.2951, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 25/100: Train Loss = 0.2618, Val Loss = 0.2947, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 26/100: Train Loss = 0.2653, Val Loss = 0.2960, Val Acc = 0.8750\n",
      "Epoch 27/100: Train Loss = 0.2721, Val Loss = 0.3029, Val Acc = 0.8721\n",
      "Epoch 28/100: Train Loss = 0.2544, Val Loss = 0.2928, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 29/100: Train Loss = 0.2570, Val Loss = 0.2940, Val Acc = 0.8808\n",
      "Epoch 30/100: Train Loss = 0.2538, Val Loss = 0.2907, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 31/100: Train Loss = 0.2476, Val Loss = 0.2911, Val Acc = 0.8779\n",
      "Epoch 32/100: Train Loss = 0.2541, Val Loss = 0.2903, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 33/100: Train Loss = 0.2437, Val Loss = 0.2901, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 34/100: Train Loss = 0.2404, Val Loss = 0.2911, Val Acc = 0.8779\n",
      "Epoch 35/100: Train Loss = 0.2455, Val Loss = 0.2908, Val Acc = 0.8779\n",
      "Epoch 36/100: Train Loss = 0.2493, Val Loss = 0.2909, Val Acc = 0.8750\n",
      "Epoch 37/100: Train Loss = 0.2372, Val Loss = 0.2914, Val Acc = 0.8750\n",
      "Epoch 38/100: Train Loss = 0.2372, Val Loss = 0.2872, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 39/100: Train Loss = 0.2350, Val Loss = 0.2879, Val Acc = 0.8779\n",
      "Epoch 40/100: Train Loss = 0.2279, Val Loss = 0.2883, Val Acc = 0.8750\n",
      "Epoch 41/100: Train Loss = 0.2259, Val Loss = 0.2885, Val Acc = 0.8779\n",
      "Epoch 42/100: Train Loss = 0.2286, Val Loss = 0.2874, Val Acc = 0.8779\n",
      "Epoch 43/100: Train Loss = 0.2226, Val Loss = 0.2855, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 44/100: Train Loss = 0.2269, Val Loss = 0.2852, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 45/100: Train Loss = 0.2220, Val Loss = 0.2842, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 46/100: Train Loss = 0.2232, Val Loss = 0.2850, Val Acc = 0.8837\n",
      "Epoch 47/100: Train Loss = 0.2188, Val Loss = 0.2837, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 48/100: Train Loss = 0.2255, Val Loss = 0.2839, Val Acc = 0.8895\n",
      "Epoch 49/100: Train Loss = 0.2176, Val Loss = 0.2859, Val Acc = 0.8837\n",
      "Epoch 50/100: Train Loss = 0.2162, Val Loss = 0.2825, Val Acc = 0.8866\n",
      "  -> New best model saved.\n",
      "Epoch 51/100: Train Loss = 0.2109, Val Loss = 0.2856, Val Acc = 0.8779\n",
      "Epoch 52/100: Train Loss = 0.2118, Val Loss = 0.2802, Val Acc = 0.8924\n",
      "  -> New best model saved.\n",
      "Epoch 53/100: Train Loss = 0.2064, Val Loss = 0.2820, Val Acc = 0.8924\n",
      "Epoch 54/100: Train Loss = 0.2078, Val Loss = 0.2851, Val Acc = 0.8837\n",
      "Epoch 55/100: Train Loss = 0.2046, Val Loss = 0.2856, Val Acc = 0.8837\n",
      "Epoch 56/100: Train Loss = 0.2048, Val Loss = 0.2839, Val Acc = 0.8837\n",
      "Epoch 57/100: Train Loss = 0.2089, Val Loss = 0.2787, Val Acc = 0.8924\n",
      "  -> New best model saved.\n",
      "Epoch 58/100: Train Loss = 0.1993, Val Loss = 0.2876, Val Acc = 0.8895\n",
      "Epoch 59/100: Train Loss = 0.1934, Val Loss = 0.2837, Val Acc = 0.8866\n",
      "Epoch 60/100: Train Loss = 0.1902, Val Loss = 0.2790, Val Acc = 0.8924\n",
      "Epoch 61/100: Train Loss = 0.1977, Val Loss = 0.2809, Val Acc = 0.8924\n",
      "Epoch 62/100: Train Loss = 0.1910, Val Loss = 0.2802, Val Acc = 0.8924\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 0.9159420289855073\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.85      0.83      0.84        72\n",
      "          CN       0.99      0.95      0.97       106\n",
      "         MCI       0.90      0.93      0.91       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.91      0.90      0.91       345\n",
      "weighted avg       0.92      0.92      0.92       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Concatenate Features\n",
    "##########################################\n",
    "# Load DeiT features.\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")  \n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")      \n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")     \n",
    "\n",
    "# Load FTTransformer features.\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")        \n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")          \n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")         \n",
    "\n",
    "# Concatenate features along the last axis.\n",
    "train_features = np.concatenate((deit_train, ft_train), axis=1)\n",
    "val_features   = np.concatenate((deit_val, ft_val), axis=1)\n",
    "test_features  = np.concatenate((deit_test, ft_test), axis=1)\n",
    "\n",
    "print(\"Fused train features shape:\", train_features.shape)\n",
    "print(\"Fused val features shape:\", val_features.shape)\n",
    "print(\"Fused test features shape:\", test_features.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "# Read the CSV files \n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "\n",
    "label_col = \"Group\"\n",
    "\n",
    "# Encode labels using LabelEncoder.\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create PyTorch Datasets and DataLoaders\n",
    "##########################################\n",
    "# Convert features and labels to tensors.\n",
    "train_features = torch.tensor(train_features, dtype=torch.float32)\n",
    "val_features   = torch.tensor(val_features, dtype=torch.float32)\n",
    "test_features  = torch.tensor(test_features, dtype=torch.float32)\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels   = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_labels  = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets.\n",
    "train_dataset = TensorDataset(train_features, train_labels)\n",
    "val_dataset   = TensorDataset(val_features, val_labels)\n",
    "test_dataset  = TensorDataset(test_features, test_labels)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define an MLP Classifier for Fused Features\n",
    "##########################################\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),  \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),        \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes) \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "input_dim = train_features.shape[1]  \n",
    "model_mlp = MLPClassifier(input_dim, num_classes)\n",
    "model_mlp.to(device)\n",
    "\n",
    "\n",
    "##########################################\n",
    "# 5. Train the MLP Classifier with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_mlp.parameters(), lr=2e-5)\n",
    "num_epochs = 100\n",
    "patience = 5  # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_mlp.train()\n",
    "    train_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_mlp(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Evaluate on the validation set.\n",
    "    model_mlp.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model_mlp(x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    val_acc = accuracy_score(all_targets, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping check.\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model_mlp.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model state.\n",
    "model_mlp.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model_mlp.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model_mlp(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd374b3c-c0de-4b5c-84b8-05bece44558e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused train features shape: (1605, 960)\n",
      "Fused val features shape: (344, 960)\n",
      "Fused test features shape: (345, 960)\n",
      "Epoch 1/100: Train Loss = 1.0306, Val Loss = 0.9219, Val Acc = 0.5756\n",
      "  -> New best model saved.\n",
      "Epoch 2/100: Train Loss = 0.8552, Val Loss = 0.7465, Val Acc = 0.7645\n",
      "  -> New best model saved.\n",
      "Epoch 3/100: Train Loss = 0.7010, Val Loss = 0.5857, Val Acc = 0.8430\n",
      "  -> New best model saved.\n",
      "Epoch 4/100: Train Loss = 0.5641, Val Loss = 0.4683, Val Acc = 0.8605\n",
      "  -> New best model saved.\n",
      "Epoch 5/100: Train Loss = 0.4694, Val Loss = 0.4010, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 6/100: Train Loss = 0.4083, Val Loss = 0.3609, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 7/100: Train Loss = 0.3731, Val Loss = 0.3385, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 8/100: Train Loss = 0.3448, Val Loss = 0.3254, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 9/100: Train Loss = 0.3264, Val Loss = 0.3183, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 10/100: Train Loss = 0.3139, Val Loss = 0.3102, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 11/100: Train Loss = 0.3032, Val Loss = 0.3056, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 12/100: Train Loss = 0.2943, Val Loss = 0.3037, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 13/100: Train Loss = 0.2930, Val Loss = 0.3016, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 14/100: Train Loss = 0.2922, Val Loss = 0.3022, Val Acc = 0.8808\n",
      "Epoch 15/100: Train Loss = 0.2957, Val Loss = 0.2998, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 16/100: Train Loss = 0.2875, Val Loss = 0.3005, Val Acc = 0.8779\n",
      "Epoch 17/100: Train Loss = 0.2806, Val Loss = 0.2985, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 18/100: Train Loss = 0.2787, Val Loss = 0.3011, Val Acc = 0.8779\n",
      "Epoch 19/100: Train Loss = 0.2843, Val Loss = 0.2976, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 20/100: Train Loss = 0.2689, Val Loss = 0.2985, Val Acc = 0.8837\n",
      "Epoch 21/100: Train Loss = 0.2755, Val Loss = 0.2979, Val Acc = 0.8837\n",
      "Epoch 22/100: Train Loss = 0.2740, Val Loss = 0.2958, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 23/100: Train Loss = 0.2722, Val Loss = 0.2997, Val Acc = 0.8750\n",
      "Epoch 24/100: Train Loss = 0.2656, Val Loss = 0.2955, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 25/100: Train Loss = 0.2644, Val Loss = 0.2949, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 26/100: Train Loss = 0.2521, Val Loss = 0.2954, Val Acc = 0.8837\n",
      "Epoch 27/100: Train Loss = 0.2618, Val Loss = 0.2984, Val Acc = 0.8721\n",
      "Epoch 28/100: Train Loss = 0.2589, Val Loss = 0.2970, Val Acc = 0.8750\n",
      "Epoch 29/100: Train Loss = 0.2526, Val Loss = 0.2946, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 30/100: Train Loss = 0.2548, Val Loss = 0.2943, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 31/100: Train Loss = 0.2494, Val Loss = 0.2926, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 32/100: Train Loss = 0.2525, Val Loss = 0.2924, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 33/100: Train Loss = 0.2461, Val Loss = 0.2977, Val Acc = 0.8721\n",
      "Epoch 34/100: Train Loss = 0.2534, Val Loss = 0.2922, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 35/100: Train Loss = 0.2478, Val Loss = 0.2909, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 36/100: Train Loss = 0.2444, Val Loss = 0.2911, Val Acc = 0.8808\n",
      "Epoch 37/100: Train Loss = 0.2395, Val Loss = 0.2917, Val Acc = 0.8750\n",
      "Epoch 38/100: Train Loss = 0.2342, Val Loss = 0.2910, Val Acc = 0.8837\n",
      "Epoch 39/100: Train Loss = 0.2414, Val Loss = 0.2897, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 40/100: Train Loss = 0.2366, Val Loss = 0.2896, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 41/100: Train Loss = 0.2431, Val Loss = 0.2901, Val Acc = 0.8750\n",
      "Epoch 42/100: Train Loss = 0.2304, Val Loss = 0.2882, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 43/100: Train Loss = 0.2359, Val Loss = 0.2875, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 44/100: Train Loss = 0.2286, Val Loss = 0.2904, Val Acc = 0.8808\n",
      "Epoch 45/100: Train Loss = 0.2345, Val Loss = 0.2874, Val Acc = 0.8866\n",
      "  -> New best model saved.\n",
      "Epoch 46/100: Train Loss = 0.2312, Val Loss = 0.2866, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 47/100: Train Loss = 0.2272, Val Loss = 0.2868, Val Acc = 0.8837\n",
      "Epoch 48/100: Train Loss = 0.2262, Val Loss = 0.2868, Val Acc = 0.8750\n",
      "Epoch 49/100: Train Loss = 0.2180, Val Loss = 0.2871, Val Acc = 0.8779\n",
      "Epoch 50/100: Train Loss = 0.2306, Val Loss = 0.2863, Val Acc = 0.8866\n",
      "  -> New best model saved.\n",
      "Epoch 51/100: Train Loss = 0.2259, Val Loss = 0.2859, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 52/100: Train Loss = 0.2185, Val Loss = 0.2840, Val Acc = 0.8866\n",
      "  -> New best model saved.\n",
      "Epoch 53/100: Train Loss = 0.2139, Val Loss = 0.2852, Val Acc = 0.8866\n",
      "Epoch 54/100: Train Loss = 0.2074, Val Loss = 0.2841, Val Acc = 0.8895\n",
      "Epoch 55/100: Train Loss = 0.2164, Val Loss = 0.2830, Val Acc = 0.8895\n",
      "  -> New best model saved.\n",
      "Epoch 56/100: Train Loss = 0.2115, Val Loss = 0.2837, Val Acc = 0.8866\n",
      "Epoch 57/100: Train Loss = 0.2136, Val Loss = 0.2865, Val Acc = 0.8808\n",
      "Epoch 58/100: Train Loss = 0.2043, Val Loss = 0.2837, Val Acc = 0.8895\n",
      "Epoch 59/100: Train Loss = 0.2002, Val Loss = 0.2832, Val Acc = 0.8866\n",
      "Epoch 60/100: Train Loss = 0.2016, Val Loss = 0.2823, Val Acc = 0.8895\n",
      "  -> New best model saved.\n",
      "Epoch 61/100: Train Loss = 0.2083, Val Loss = 0.2827, Val Acc = 0.8866\n",
      "Epoch 62/100: Train Loss = 0.2032, Val Loss = 0.2820, Val Acc = 0.8866\n",
      "  -> New best model saved.\n",
      "Epoch 63/100: Train Loss = 0.1954, Val Loss = 0.2817, Val Acc = 0.8895\n",
      "  -> New best model saved.\n",
      "Epoch 64/100: Train Loss = 0.2096, Val Loss = 0.2798, Val Acc = 0.8895\n",
      "  -> New best model saved.\n",
      "Epoch 65/100: Train Loss = 0.1938, Val Loss = 0.2811, Val Acc = 0.8895\n",
      "Epoch 66/100: Train Loss = 0.1849, Val Loss = 0.2829, Val Acc = 0.8895\n",
      "Epoch 67/100: Train Loss = 0.1946, Val Loss = 0.2802, Val Acc = 0.8895\n",
      "Epoch 68/100: Train Loss = 0.1897, Val Loss = 0.2814, Val Acc = 0.8895\n",
      "Epoch 69/100: Train Loss = 0.1824, Val Loss = 0.2799, Val Acc = 0.8895\n",
      "Early stopping triggered.\n",
      "\n",
      "Test Accuracy: 0.9043478260869565\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.80      0.85      0.82        72\n",
      "          CN       0.98      0.95      0.97       106\n",
      "         MCI       0.90      0.90      0.90       167\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.90      0.90      0.90       345\n",
      "weighted avg       0.91      0.90      0.91       345\n",
      "\n",
      "\n",
      "Direct Calculation:\n",
      "Standard Error: 0.015834546303385672\n",
      "Margin of Error (95% CI): 0.031035710754635917\n",
      "95% Confidence Interval for Accuracy: [0.8733, 0.9354]\n",
      "\n",
      "Bootstrapping:\n",
      "Bootstrapped Mean Accuracy: 0.9041130434782608\n",
      "Standard Error (Bootstrap): 0.015918411755204514\n",
      "Margin of Error (Bootstrap, 95% CI): 0.031200087040200847\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import math\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Concatenate Features\n",
    "##########################################\n",
    "# Load DeiT features.\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")  \n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")      \n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")     \n",
    "\n",
    "# Load FTTransformer features.\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")        \n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")          \n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")         \n",
    "\n",
    "# Concatenate features along the last axis.\n",
    "train_features = np.concatenate((deit_train, ft_train), axis=1)\n",
    "val_features   = np.concatenate((deit_val, ft_val), axis=1)\n",
    "test_features  = np.concatenate((deit_test, ft_test), axis=1)\n",
    "\n",
    "print(\"Fused train features shape:\", train_features.shape)\n",
    "print(\"Fused val features shape:\", val_features.shape)\n",
    "print(\"Fused test features shape:\", test_features.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "# Read the CSV files \n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "\n",
    "# Encode labels using LabelEncoder.\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create PyTorch Datasets and DataLoaders\n",
    "##########################################\n",
    "# Convert features and labels to tensors.\n",
    "train_features = torch.tensor(train_features, dtype=torch.float32)\n",
    "val_features   = torch.tensor(val_features, dtype=torch.float32)\n",
    "test_features  = torch.tensor(test_features, dtype=torch.float32)\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels   = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_labels  = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets.\n",
    "train_dataset = TensorDataset(train_features, train_labels)\n",
    "val_dataset   = TensorDataset(val_features, val_labels)\n",
    "test_dataset  = TensorDataset(test_features, test_labels)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define an MLP Classifier for Fused Features\n",
    "##########################################\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),  # First hidden layer with 256 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),        # Second hidden layer with 128 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes) # Output layer with num_classes neurons\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "input_dim = train_features.shape[1]\n",
    "model_mlp = MLPClassifier(input_dim, num_classes)\n",
    "model_mlp.to(device)\n",
    "\n",
    "##########################################\n",
    "# 5. Train the MLP Classifier with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_mlp.parameters(), lr=2e-5)\n",
    "num_epochs = 100\n",
    "patience = 5  # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_mlp.train()\n",
    "    train_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_mlp(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Evaluate on the validation set.\n",
    "    model_mlp.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds_val = []\n",
    "    all_targets_val = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model_mlp(x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds_val.append(preds.cpu().numpy())\n",
    "            all_targets_val.append(y.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds_val = np.concatenate(all_preds_val)\n",
    "    all_targets_val = np.concatenate(all_targets_val)\n",
    "    val_acc = accuracy_score(all_targets_val, all_preds_val)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping check.\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model_mlp.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model state.\n",
    "model_mlp.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model_mlp.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model_mlp(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", test_acc)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))\n",
    "\n",
    "##########################################\n",
    "# 7. Calculate the Margin of Error for Test Accuracy\n",
    "##########################################\n",
    "\n",
    "# Approach 1: Direct Calculation using the binomial formula.\n",
    "n_test = len(test_dataset)  # Number of test samples.\n",
    "se = math.sqrt(test_acc * (1 - test_acc) / n_test)\n",
    "moe = 1.96 * se  # For a 95% confidence interval.\n",
    "print(\"\\nDirect Calculation:\")\n",
    "print(\"Standard Error:\", se)\n",
    "print(\"Margin of Error (95% CI):\", moe)\n",
    "print(\"95% Confidence Interval for Accuracy: [{:.4f}, {:.4f}]\".format(test_acc - moe, test_acc + moe))\n",
    "\n",
    "# Approach 2: Bootstrapping.\n",
    "n_bootstrap = 1000\n",
    "boot_acc = []\n",
    "n_samples = len(all_targets)\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_samples), n_samples, replace=True)\n",
    "    boot_accuracy = accuracy_score(all_targets[indices], all_preds[indices])\n",
    "    boot_acc.append(boot_accuracy)\n",
    "boot_acc = np.array(boot_acc)\n",
    "se_boot = np.std(boot_acc)\n",
    "moe_boot = 1.96 * se_boot  # Using the normal approximation.\n",
    "print(\"\\nBootstrapping:\")\n",
    "print(\"Bootstrapped Mean Accuracy:\", np.mean(boot_acc))\n",
    "print(\"Standard Error (Bootstrap):\", se_boot)\n",
    "print(\"Margin of Error (Bootstrap, 95% CI):\", moe_boot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecdc92a-ce5d-4d85-9e3a-731a53d623e6",
   "metadata": {},
   "source": [
    "## Margin of error for early fusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24ea4273-55eb-4d3b-bc38-fea2f60ade75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused train features shape: (1605, 960)\n",
      "Fused val features shape: (344, 960)\n",
      "Fused test features shape: (345, 960)\n",
      "Epoch 1/100: Train Loss = 1.0096, Val Loss = 0.9144, Val Acc = 0.6570\n",
      "  -> New best model saved.\n",
      "Epoch 2/100: Train Loss = 0.8693, Val Loss = 0.7573, Val Acc = 0.7703\n",
      "  -> New best model saved.\n",
      "Epoch 3/100: Train Loss = 0.7163, Val Loss = 0.5992, Val Acc = 0.7791\n",
      "  -> New best model saved.\n",
      "Epoch 4/100: Train Loss = 0.5696, Val Loss = 0.4813, Val Acc = 0.8169\n",
      "  -> New best model saved.\n",
      "Epoch 5/100: Train Loss = 0.4849, Val Loss = 0.4096, Val Acc = 0.8605\n",
      "  -> New best model saved.\n",
      "Epoch 6/100: Train Loss = 0.4269, Val Loss = 0.3689, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 7/100: Train Loss = 0.3693, Val Loss = 0.3392, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 8/100: Train Loss = 0.3436, Val Loss = 0.3270, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 9/100: Train Loss = 0.3286, Val Loss = 0.3166, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 10/100: Train Loss = 0.3235, Val Loss = 0.3114, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 11/100: Train Loss = 0.3071, Val Loss = 0.3094, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 12/100: Train Loss = 0.3097, Val Loss = 0.3096, Val Acc = 0.8663\n",
      "Epoch 13/100: Train Loss = 0.2981, Val Loss = 0.3064, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 14/100: Train Loss = 0.2861, Val Loss = 0.3124, Val Acc = 0.8605\n",
      "Epoch 15/100: Train Loss = 0.2907, Val Loss = 0.3033, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 16/100: Train Loss = 0.2888, Val Loss = 0.3018, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 17/100: Train Loss = 0.2820, Val Loss = 0.3017, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 18/100: Train Loss = 0.2733, Val Loss = 0.3004, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 19/100: Train Loss = 0.2764, Val Loss = 0.3019, Val Acc = 0.8750\n",
      "Epoch 20/100: Train Loss = 0.2654, Val Loss = 0.3073, Val Acc = 0.8605\n",
      "Epoch 21/100: Train Loss = 0.2727, Val Loss = 0.2981, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 22/100: Train Loss = 0.2696, Val Loss = 0.2994, Val Acc = 0.8721\n",
      "Epoch 23/100: Train Loss = 0.2632, Val Loss = 0.3046, Val Acc = 0.8605\n",
      "Epoch 24/100: Train Loss = 0.2623, Val Loss = 0.2978, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 25/100: Train Loss = 0.2576, Val Loss = 0.2960, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 26/100: Train Loss = 0.2621, Val Loss = 0.2966, Val Acc = 0.8808\n",
      "Epoch 27/100: Train Loss = 0.2545, Val Loss = 0.2967, Val Acc = 0.8750\n",
      "Epoch 28/100: Train Loss = 0.2574, Val Loss = 0.2947, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 29/100: Train Loss = 0.2573, Val Loss = 0.2939, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 30/100: Train Loss = 0.2560, Val Loss = 0.2980, Val Acc = 0.8721\n",
      "Epoch 31/100: Train Loss = 0.2500, Val Loss = 0.2952, Val Acc = 0.8721\n",
      "Epoch 32/100: Train Loss = 0.2515, Val Loss = 0.2934, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 33/100: Train Loss = 0.2390, Val Loss = 0.2928, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 34/100: Train Loss = 0.2495, Val Loss = 0.2918, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 35/100: Train Loss = 0.2484, Val Loss = 0.2934, Val Acc = 0.8779\n",
      "Epoch 36/100: Train Loss = 0.2374, Val Loss = 0.2941, Val Acc = 0.8750\n",
      "Epoch 37/100: Train Loss = 0.2390, Val Loss = 0.2926, Val Acc = 0.8837\n",
      "Epoch 38/100: Train Loss = 0.2295, Val Loss = 0.2927, Val Acc = 0.8779\n",
      "Epoch 39/100: Train Loss = 0.2391, Val Loss = 0.2906, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 40/100: Train Loss = 0.2338, Val Loss = 0.2933, Val Acc = 0.8721\n",
      "Epoch 41/100: Train Loss = 0.2366, Val Loss = 0.2903, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 42/100: Train Loss = 0.2347, Val Loss = 0.2917, Val Acc = 0.8779\n",
      "Epoch 43/100: Train Loss = 0.2244, Val Loss = 0.2878, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 44/100: Train Loss = 0.2286, Val Loss = 0.2882, Val Acc = 0.8837\n",
      "Epoch 45/100: Train Loss = 0.2249, Val Loss = 0.2879, Val Acc = 0.8837\n",
      "Epoch 46/100: Train Loss = 0.2198, Val Loss = 0.2873, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 47/100: Train Loss = 0.2248, Val Loss = 0.2881, Val Acc = 0.8808\n",
      "Epoch 48/100: Train Loss = 0.2248, Val Loss = 0.2882, Val Acc = 0.8779\n",
      "Epoch 49/100: Train Loss = 0.2190, Val Loss = 0.2844, Val Acc = 0.8866\n",
      "  -> New best model saved.\n",
      "Epoch 50/100: Train Loss = 0.2162, Val Loss = 0.2841, Val Acc = 0.8866\n",
      "  -> New best model saved.\n",
      "Epoch 51/100: Train Loss = 0.2170, Val Loss = 0.2838, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 52/100: Train Loss = 0.2154, Val Loss = 0.2830, Val Acc = 0.8866\n",
      "  -> New best model saved.\n",
      "Epoch 53/100: Train Loss = 0.2044, Val Loss = 0.2864, Val Acc = 0.8837\n",
      "Epoch 54/100: Train Loss = 0.2046, Val Loss = 0.2828, Val Acc = 0.8866\n",
      "  -> New best model saved.\n",
      "Epoch 55/100: Train Loss = 0.2085, Val Loss = 0.2846, Val Acc = 0.8895\n",
      "Epoch 56/100: Train Loss = 0.2026, Val Loss = 0.2842, Val Acc = 0.8866\n",
      "Epoch 57/100: Train Loss = 0.2038, Val Loss = 0.2850, Val Acc = 0.8924\n",
      "Epoch 58/100: Train Loss = 0.2002, Val Loss = 0.2814, Val Acc = 0.8866\n",
      "  -> New best model saved.\n",
      "Epoch 59/100: Train Loss = 0.1998, Val Loss = 0.2835, Val Acc = 0.8866\n",
      "Epoch 60/100: Train Loss = 0.1980, Val Loss = 0.2799, Val Acc = 0.8924\n",
      "  -> New best model saved.\n",
      "Epoch 61/100: Train Loss = 0.2007, Val Loss = 0.2780, Val Acc = 0.8895\n",
      "  -> New best model saved.\n",
      "Epoch 62/100: Train Loss = 0.1972, Val Loss = 0.2792, Val Acc = 0.8895\n",
      "Epoch 63/100: Train Loss = 0.1935, Val Loss = 0.2793, Val Acc = 0.8895\n",
      "Epoch 64/100: Train Loss = 0.1925, Val Loss = 0.2803, Val Acc = 0.8895\n",
      "Epoch 65/100: Train Loss = 0.1850, Val Loss = 0.2811, Val Acc = 0.8924\n",
      "Epoch 66/100: Train Loss = 0.1978, Val Loss = 0.2772, Val Acc = 0.8895\n",
      "  -> New best model saved.\n",
      "Epoch 67/100: Train Loss = 0.1785, Val Loss = 0.2794, Val Acc = 0.8895\n",
      "Epoch 68/100: Train Loss = 0.1836, Val Loss = 0.2776, Val Acc = 0.8895\n",
      "Epoch 69/100: Train Loss = 0.1883, Val Loss = 0.2784, Val Acc = 0.8924\n",
      "Epoch 70/100: Train Loss = 0.1749, Val Loss = 0.2833, Val Acc = 0.9012\n",
      "Epoch 71/100: Train Loss = 0.1827, Val Loss = 0.2780, Val Acc = 0.8895\n",
      "Early stopping triggered.\n",
      "\n",
      "Test Accuracy: 0.9014492753623189\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.79      0.85      0.82        72\n",
      "          CN       0.98      0.95      0.97       106\n",
      "         MCI       0.90      0.89      0.90       167\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.89      0.90      0.89       345\n",
      "weighted avg       0.90      0.90      0.90       345\n",
      "\n",
      "\n",
      "Direct Calculation:\n",
      "Standard Error: 0.016046894982606724\n",
      "Margin of Error (95% CI): 0.03145191416590918\n",
      "95% Confidence Interval for Accuracy: [0.8700, 0.9329]\n",
      "\n",
      "Bootstrapping:\n",
      "Bootstrapped Mean Accuracy: 0.9012144927536233\n",
      "Standard Error (Bootstrap): 0.015500185037278612\n",
      "Margin of Error (Bootstrap, 95% CI): 0.03038036267306608\n",
      "\n",
      "Weighted F1 Score: 0.9023199166992005\n",
      "\n",
      "Bootstrapping for Weighted F1 Score:\n",
      "Bootstrapped Mean Weighted F1: 0.9022109799453788\n",
      "Standard Error (Bootstrap) for Weighted F1: 0.01609568137122737\n",
      "Margin of Error (Bootstrap, 95% CI) for Weighted F1: 0.031547535487605645\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import math\n",
    "\n",
    "# Set device.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Concatenate Features\n",
    "##########################################\n",
    "# Load DeiT features.\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")  \n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")      \n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")     \n",
    "\n",
    "# Load FTTransformer features.\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")        \n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")          \n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")         \n",
    "\n",
    "# Concatenate features along the last axis.\n",
    "train_features = np.concatenate((deit_train, ft_train), axis=1)\n",
    "val_features   = np.concatenate((deit_val, ft_val), axis=1)\n",
    "test_features  = np.concatenate((deit_test, ft_test), axis=1)\n",
    "\n",
    "print(\"Fused train features shape:\", train_features.shape)\n",
    "print(\"Fused val features shape:\", val_features.shape)\n",
    "print(\"Fused test features shape:\", test_features.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "# Read the CSV files \n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "\n",
    "# Encode labels using LabelEncoder.\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create PyTorch Datasets and DataLoaders\n",
    "##########################################\n",
    "# Convert features and labels to tensors.\n",
    "train_features = torch.tensor(train_features, dtype=torch.float32)\n",
    "val_features   = torch.tensor(val_features, dtype=torch.float32)\n",
    "test_features  = torch.tensor(test_features, dtype=torch.float32)\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels   = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_labels  = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets.\n",
    "train_dataset = TensorDataset(train_features, train_labels)\n",
    "val_dataset   = TensorDataset(val_features, val_labels)\n",
    "test_dataset  = TensorDataset(test_features, test_labels)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define an MLP Classifier for Fused Features\n",
    "##########################################\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),  # First hidden layer with 256 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),        # Second hidden layer with 128 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes) # Output layer with num_classes neurons\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "input_dim = train_features.shape[1]\n",
    "model_mlp = MLPClassifier(input_dim, num_classes)\n",
    "model_mlp.to(device)\n",
    "\n",
    "##########################################\n",
    "# 5. Train the MLP Classifier with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model_mlp.parameters(), lr=2e-5)\n",
    "num_epochs = 100\n",
    "patience = 5  # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_mlp.train()\n",
    "    train_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_mlp(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Evaluate on the validation set.\n",
    "    model_mlp.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds_val = []\n",
    "    all_targets_val = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model_mlp(x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds_val.append(preds.cpu().numpy())\n",
    "            all_targets_val.append(y.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds_val = np.concatenate(all_preds_val)\n",
    "    all_targets_val = np.concatenate(all_targets_val)\n",
    "    val_acc = accuracy_score(all_targets_val, all_preds_val)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping check.\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model_mlp.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model state.\n",
    "model_mlp.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model_mlp.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model_mlp(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", test_acc)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))\n",
    "\n",
    "##########################################\n",
    "# 7. Calculate the Margin of Error for Test Accuracy\n",
    "##########################################\n",
    "# Approach 1: Direct Calculation using the Binomial Formula.\n",
    "n_test = len(test_dataset)  # Number of test samples.\n",
    "se = math.sqrt(test_acc * (1 - test_acc) / n_test)\n",
    "moe = 1.96 * se  # For a 95% confidence interval.\n",
    "print(\"\\nDirect Calculation:\")\n",
    "print(\"Standard Error:\", se)\n",
    "print(\"Margin of Error (95% CI):\", moe)\n",
    "print(\"95% Confidence Interval for Accuracy: [{:.4f}, {:.4f}]\".format(test_acc - moe, test_acc + moe))\n",
    "\n",
    "# Approach 2: Bootstrapping.\n",
    "n_bootstrap = 1000\n",
    "boot_acc = []\n",
    "n_samples = len(all_targets)\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_samples), n_samples, replace=True)\n",
    "    boot_accuracy = accuracy_score(np.array(all_targets)[indices], np.array(all_preds)[indices])\n",
    "    boot_acc.append(boot_accuracy)\n",
    "boot_acc = np.array(boot_acc)\n",
    "se_boot = np.std(boot_acc)\n",
    "moe_boot = 1.96 * se_boot  # Using the normal approximation.\n",
    "print(\"\\nBootstrapping:\")\n",
    "print(\"Bootstrapped Mean Accuracy:\", np.mean(boot_acc))\n",
    "print(\"Standard Error (Bootstrap):\", se_boot)\n",
    "print(\"Margin of Error (Bootstrap, 95% CI):\", moe_boot)\n",
    "\n",
    "##########################################\n",
    "# 8. Compute the Margin of Error for Weighted F1 Score\n",
    "##########################################\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Compute the Weighted F1 Score on the test set.\n",
    "weighted_f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "print(\"\\nWeighted F1 Score:\", weighted_f1)\n",
    "\n",
    "# Bootstrapping for Weighted F1 Score.\n",
    "boot_f1s = []\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_samples), n_samples, replace=True)\n",
    "    f1_bs = f1_score(np.array(all_targets)[indices], np.array(all_preds)[indices], average='weighted')\n",
    "    boot_f1s.append(f1_bs)\n",
    "boot_f1s = np.array(boot_f1s)\n",
    "se_boot_f1 = np.std(boot_f1s)\n",
    "moe_boot_f1 = 1.96 * se_boot_f1\n",
    "print(\"\\nBootstrapping for Weighted F1 Score:\")\n",
    "print(\"Bootstrapped Mean Weighted F1:\", np.mean(boot_f1s))\n",
    "print(\"Standard Error (Bootstrap) for Weighted F1:\", se_boot_f1)\n",
    "print(\"Margin of Error (Bootstrap, 95% CI) for Weighted F1:\", moe_boot_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a6f400-cde1-42b4-bd25-ad1cbd8aa3bf",
   "metadata": {},
   "source": [
    "## Self Attention fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a052e9d-40bf-4e62-bf27-c2bc353081df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: Train Loss = 0.9585, Val Loss = 0.7672, Val Acc = 0.7733\n",
      "  -> New best model saved.\n",
      "Epoch 2/100: Train Loss = 0.7079, Val Loss = 0.5522, Val Acc = 0.7820\n",
      "  -> New best model saved.\n",
      "Epoch 3/100: Train Loss = 0.5502, Val Loss = 0.4410, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 4/100: Train Loss = 0.4598, Val Loss = 0.3804, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 5/100: Train Loss = 0.3899, Val Loss = 0.3599, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 6/100: Train Loss = 0.3589, Val Loss = 0.3457, Val Acc = 0.8634\n",
      "  -> New best model saved.\n",
      "Epoch 7/100: Train Loss = 0.3404, Val Loss = 0.3396, Val Acc = 0.8634\n",
      "  -> New best model saved.\n",
      "Epoch 8/100: Train Loss = 0.3262, Val Loss = 0.3362, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 9/100: Train Loss = 0.3204, Val Loss = 0.3328, Val Acc = 0.8663\n",
      "  -> New best model saved.\n",
      "Epoch 10/100: Train Loss = 0.3100, Val Loss = 0.3321, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 11/100: Train Loss = 0.3054, Val Loss = 0.3248, Val Acc = 0.8634\n",
      "  -> New best model saved.\n",
      "Epoch 12/100: Train Loss = 0.2886, Val Loss = 0.3193, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 13/100: Train Loss = 0.2815, Val Loss = 0.3196, Val Acc = 0.8779\n",
      "Epoch 14/100: Train Loss = 0.2791, Val Loss = 0.3252, Val Acc = 0.8692\n",
      "Epoch 15/100: Train Loss = 0.2740, Val Loss = 0.3044, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 16/100: Train Loss = 0.2678, Val Loss = 0.3059, Val Acc = 0.8808\n",
      "Epoch 17/100: Train Loss = 0.2635, Val Loss = 0.3114, Val Acc = 0.8808\n",
      "Epoch 18/100: Train Loss = 0.2512, Val Loss = 0.3093, Val Acc = 0.8808\n",
      "Epoch 19/100: Train Loss = 0.2482, Val Loss = 0.3119, Val Acc = 0.8866\n",
      "Epoch 20/100: Train Loss = 0.2465, Val Loss = 0.3069, Val Acc = 0.8866\n",
      "Early stopping triggered.\n",
      "\n",
      "Test Accuracy: 0.9072463768115943\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.79      0.88      0.83        72\n",
      "          CN       0.98      0.96      0.97       106\n",
      "         MCI       0.92      0.89      0.90       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.90      0.91      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n",
      "\n",
      "Direct Calculation:\n",
      "Standard Error: 0.015617751751747575\n",
      "Margin of Error (95% CI): 0.030610793433425248\n",
      "95% Confidence Interval for Accuracy: [0.8766, 0.9379]\n",
      "\n",
      "Bootstrapping:\n",
      "Bootstrapped Mean Accuracy: 0.907408695652174\n",
      "Standard Error (Bootstrap): 0.015773779886414718\n",
      "Margin of Error (Bootstrap, 95% CI): 0.030916608577372846\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import math\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load Features for Each Modality\n",
    "##########################################\n",
    "# Load DeiT features (expected shape: [num_samples, 768])\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")  \n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")      \n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")     \n",
    "\n",
    "# Load FTTransformer features (expected shape: [num_samples, 192])\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")        \n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")          \n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")         \n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create PyTorch Datasets and DataLoaders\n",
    "##########################################\n",
    "# Convert features to tensors (keep modalities separate for self-attention fusion)\n",
    "train_deit_tensor = torch.tensor(deit_train, dtype=torch.float32)\n",
    "train_ft_tensor   = torch.tensor(ft_train, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "val_deit_tensor = torch.tensor(deit_val, dtype=torch.float32)\n",
    "val_ft_tensor   = torch.tensor(ft_val, dtype=torch.float32)\n",
    "val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
    "\n",
    "test_deit_tensor = torch.tensor(deit_test, dtype=torch.float32)\n",
    "test_ft_tensor   = torch.tensor(ft_test, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets.\n",
    "train_dataset = TensorDataset(train_deit_tensor, train_ft_tensor, train_labels_tensor)\n",
    "val_dataset   = TensorDataset(val_deit_tensor, val_ft_tensor, val_labels_tensor)\n",
    "test_dataset  = TensorDataset(test_deit_tensor, test_ft_tensor, test_labels_tensor)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define a Self-Attention Fusion Classifier\n",
    "##########################################\n",
    "class SelfAttentionFusionClassifier(nn.Module):\n",
    "    def __init__(self, deit_dim, ft_dim, embed_dim, num_heads, num_classes):\n",
    "        super(SelfAttentionFusionClassifier, self).__init__()\n",
    "        # Project each modality into a common embedding space.\n",
    "        self.deit_proj = nn.Linear(deit_dim, embed_dim)\n",
    "        self.ft_proj = nn.Linear(ft_dim, embed_dim)\n",
    "        # Define a Transformer encoder layer for self-attention fusion.\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=0.3)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        # Classifier layer that takes the fused representation and outputs logits.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, deit_input, ft_input):\n",
    "        # deit_input: shape (batch_size, deit_dim)\n",
    "        # ft_input: shape (batch_size, ft_dim)\n",
    "        deit_emb = self.deit_proj(deit_input)  # (batch_size, embed_dim)\n",
    "        ft_emb = self.ft_proj(ft_input)        # (batch_size, embed_dim)\n",
    "        # Stack modality embeddings to create a sequence of length 2.\n",
    "        # Resulting shape: (batch_size, 2, embed_dim)\n",
    "        fusion_seq = torch.stack([deit_emb, ft_emb], dim=1)\n",
    "        # Transformer expects input as (sequence_length, batch_size, embed_dim).\n",
    "        fusion_seq = fusion_seq.transpose(0, 1)  # Shape: (2, batch_size, embed_dim)\n",
    "        fused_seq = self.transformer_encoder(fusion_seq)  # (2, batch_size, embed_dim)\n",
    "        # Aggregate the sequence (e.g., mean pooling) across the tokens.\n",
    "        fused = fused_seq.mean(dim=0)  # (batch_size, embed_dim)\n",
    "        logits = self.classifier(fused)\n",
    "        return logits\n",
    "\n",
    "# Set dimensions. DeiT features are 768, FT features are 192. We'll project both to embed_dim (e.g., 256)\n",
    "deit_dim = deit_train.shape[1]  # 768\n",
    "ft_dim = ft_train.shape[1]      # 192\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "\n",
    "model = SelfAttentionFusionClassifier(deit_dim, ft_dim, embed_dim, num_heads, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "##########################################\n",
    "# 5. Train the Self-Attention Fusion Classifier with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "num_epochs = 100\n",
    "patience = 5  # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for deit_x, ft_x, y in train_loader:\n",
    "        deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(deit_x, ft_x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * y.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Evaluate on the validation set.\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds_val = []\n",
    "    all_targets_val = []\n",
    "    with torch.no_grad():\n",
    "        for deit_x, ft_x, y in val_loader:\n",
    "            deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "            logits = model(deit_x, ft_x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * y.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds_val.append(preds.cpu().numpy())\n",
    "            all_targets_val.append(y.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds_val = np.concatenate(all_preds_val)\n",
    "    all_targets_val = np.concatenate(all_targets_val)\n",
    "    val_acc = accuracy_score(all_targets_val, all_preds_val)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping check.\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model state.\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for deit_x, ft_x, y in test_loader:\n",
    "        deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "        logits = model(deit_x, ft_x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", test_acc)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))\n",
    "\n",
    "##########################################\n",
    "# 7. Calculate the Margin of Error for Test Accuracy\n",
    "##########################################\n",
    "# Approach 1: Direct Calculation using the binomial formula.\n",
    "n_test = len(test_dataset)  # Number of test samples.\n",
    "se = math.sqrt(test_acc * (1 - test_acc) / n_test)\n",
    "moe = 1.96 * se  # For a 95% confidence interval.\n",
    "print(\"\\nDirect Calculation:\")\n",
    "print(\"Standard Error:\", se)\n",
    "print(\"Margin of Error (95% CI):\", moe)\n",
    "print(\"95% Confidence Interval for Accuracy: [{:.4f}, {:.4f}]\".format(test_acc - moe, test_acc + moe))\n",
    "\n",
    "# Approach 2: Bootstrapping.\n",
    "n_bootstrap = 1000\n",
    "boot_acc = []\n",
    "n_samples = len(all_targets)\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_samples), n_samples, replace=True)\n",
    "    boot_accuracy = accuracy_score(all_targets[indices], all_preds[indices])\n",
    "    boot_acc.append(boot_accuracy)\n",
    "boot_acc = np.array(boot_acc)\n",
    "se_boot = np.std(boot_acc)\n",
    "moe_boot = 1.96 * se_boot  # Using the normal approximation.\n",
    "print(\"\\nBootstrapping:\")\n",
    "print(\"Bootstrapped Mean Accuracy:\", np.mean(boot_acc))\n",
    "print(\"Standard Error (Bootstrap):\", se_boot)\n",
    "print(\"Margin of Error (Bootstrap, 95% CI):\", moe_boot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323c3eab-b283-4f4c-9f36-04d6932fc490",
   "metadata": {},
   "source": [
    "## MultiHead attention fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9080707-ee45-4401-97ca-322ab50a8704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Epoch 1/100: Train Loss = 0.8064, Val Loss = 0.4659, Val Acc = 0.8169\n",
      "  -> New best model saved.\n",
      "Epoch 2/100: Train Loss = 0.3690, Val Loss = 0.3571, Val Acc = 0.8401\n",
      "  -> New best model saved.\n",
      "Epoch 3/100: Train Loss = 0.3194, Val Loss = 0.3301, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 4/100: Train Loss = 0.3129, Val Loss = 0.3201, Val Acc = 0.8634\n",
      "  -> New best model saved.\n",
      "Epoch 5/100: Train Loss = 0.3084, Val Loss = 0.3229, Val Acc = 0.8547\n",
      "Epoch 6/100: Train Loss = 0.3009, Val Loss = 0.3142, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 7/100: Train Loss = 0.2897, Val Loss = 0.3280, Val Acc = 0.8488\n",
      "Epoch 8/100: Train Loss = 0.2877, Val Loss = 0.3138, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 9/100: Train Loss = 0.2873, Val Loss = 0.3072, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 10/100: Train Loss = 0.2775, Val Loss = 0.3070, Val Acc = 0.8663\n",
      "  -> New best model saved.\n",
      "Epoch 11/100: Train Loss = 0.2758, Val Loss = 0.3075, Val Acc = 0.8692\n",
      "Epoch 12/100: Train Loss = 0.2658, Val Loss = 0.3093, Val Acc = 0.8663\n",
      "Epoch 13/100: Train Loss = 0.2641, Val Loss = 0.3095, Val Acc = 0.8663\n",
      "Epoch 14/100: Train Loss = 0.2611, Val Loss = 0.3018, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 15/100: Train Loss = 0.2536, Val Loss = 0.3054, Val Acc = 0.8634\n",
      "Epoch 16/100: Train Loss = 0.2488, Val Loss = 0.3019, Val Acc = 0.8779\n",
      "Epoch 17/100: Train Loss = 0.2438, Val Loss = 0.3106, Val Acc = 0.8605\n",
      "Epoch 18/100: Train Loss = 0.2380, Val Loss = 0.3112, Val Acc = 0.8808\n",
      "Epoch 19/100: Train Loss = 0.2266, Val Loss = 0.3035, Val Acc = 0.8750\n",
      "Early stopping triggered.\n",
      "\n",
      "Test Accuracy: 0.9130434782608695\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.86      0.82      0.84        72\n",
      "          CN       0.98      0.95      0.97       106\n",
      "         MCI       0.90      0.93      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.91      0.90      0.91       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n",
      "\n",
      "Direct Calculation:\n",
      "Standard Error: 0.015170058712846038\n",
      "Margin of Error (95% CI): 0.029733315077178232\n",
      "95% Confidence Interval for Accuracy: [0.8833, 0.9428]\n",
      "\n",
      "Bootstrapping:\n",
      "Bootstrapped Mean Accuracy: 0.9125159420289854\n",
      "Standard Error (Bootstrap): 0.015301733417448478\n",
      "Margin of Error (Bootstrap, 95% CI): 0.029991397498199016\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import math\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Prepare Features for Each Modality\n",
    "##########################################\n",
    "# Load DeiT features (e.g., shape: [num_samples, 768])\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")  \n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")      \n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")     \n",
    "\n",
    "# Load FTTransformer features (e.g., shape: [num_samples, 192])\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")        \n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")          \n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")         \n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create PyTorch Datasets and DataLoaders\n",
    "##########################################\n",
    "# Convert features to tensors (keep modalities separate for fusion)\n",
    "train_deit_tensor = torch.tensor(deit_train, dtype=torch.float32)\n",
    "train_ft_tensor   = torch.tensor(ft_train, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "val_deit_tensor = torch.tensor(deit_val, dtype=torch.float32)\n",
    "val_ft_tensor   = torch.tensor(ft_val, dtype=torch.float32)\n",
    "val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
    "\n",
    "test_deit_tensor = torch.tensor(deit_test, dtype=torch.float32)\n",
    "test_ft_tensor   = torch.tensor(ft_test, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets.\n",
    "train_dataset = TensorDataset(train_deit_tensor, train_ft_tensor, train_labels_tensor)\n",
    "val_dataset   = TensorDataset(val_deit_tensor, val_ft_tensor, val_labels_tensor)\n",
    "test_dataset  = TensorDataset(test_deit_tensor, test_ft_tensor, test_labels_tensor)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define a Multi-Head Attention Fusion Classifier\n",
    "##########################################\n",
    "class MultiheadAttentionFusionClassifier(nn.Module):\n",
    "    def __init__(self, deit_dim, ft_dim, embed_dim, num_heads, num_classes):\n",
    "        super(MultiheadAttentionFusionClassifier, self).__init__()\n",
    "        # Project each modality into a common embedding space.\n",
    "        self.deit_proj = nn.Linear(deit_dim, embed_dim)\n",
    "        self.ft_proj = nn.Linear(ft_dim, embed_dim)\n",
    "        # Define a multi-head attention layer.\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=0.3)\n",
    "        # Classifier: fuse features after attention.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, deit_input, ft_input):\n",
    "        # Project the two modalities.\n",
    "        deit_emb = self.deit_proj(deit_input)  # (batch_size, embed_dim)\n",
    "        ft_emb = self.ft_proj(ft_input)          # (batch_size, embed_dim)\n",
    "        # Stack the modality embeddings to form a sequence (2 tokens per sample).\n",
    "        # Shape: (batch_size, 2, embed_dim)\n",
    "        fusion_seq = torch.stack([deit_emb, ft_emb], dim=1)\n",
    "        # nn.MultiheadAttention expects input as (sequence_length, batch_size, embed_dim)\n",
    "        fusion_seq = fusion_seq.transpose(0, 1)  # Now (2, batch_size, embed_dim)\n",
    "        # Apply multi-head attention (self-attention on the fusion sequence).\n",
    "        attn_output, _ = self.mha(fusion_seq, fusion_seq, fusion_seq)\n",
    "        # Aggregate the sequence tokens. Here we use mean pooling.\n",
    "        fused = attn_output.mean(dim=0)  # (batch_size, embed_dim)\n",
    "        logits = self.classifier(fused)\n",
    "        return logits\n",
    "\n",
    "# Set dimensions. For example, DeiT features are 768 and FTTransformer features are 192.\n",
    "deit_dim = deit_train.shape[1]  # 768\n",
    "ft_dim = ft_train.shape[1]      # 192\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "\n",
    "model = MultiheadAttentionFusionClassifier(deit_dim, ft_dim, embed_dim, num_heads, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "##########################################\n",
    "# 5. Train the Multi-Head Attention Fusion Classifier with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "num_epochs = 100\n",
    "patience = 5  # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for deit_x, ft_x, y in train_loader:\n",
    "        deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(deit_x, ft_x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * y.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds_val = []\n",
    "    all_targets_val = []\n",
    "    with torch.no_grad():\n",
    "        for deit_x, ft_x, y in val_loader:\n",
    "            deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "            logits = model(deit_x, ft_x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * y.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds_val.append(preds.cpu().numpy())\n",
    "            all_targets_val.append(y.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds_val = np.concatenate(all_preds_val)\n",
    "    all_targets_val = np.concatenate(all_targets_val)\n",
    "    val_acc = accuracy_score(all_targets_val, all_preds_val)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load best model state.\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for deit_x, ft_x, y in test_loader:\n",
    "        deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "        logits = model(deit_x, ft_x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", test_acc)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))\n",
    "\n",
    "##########################################\n",
    "# 7. Calculate the Margin of Error for Test Accuracy\n",
    "##########################################\n",
    "# Approach 1: Direct Calculation using the binomial formula.\n",
    "n_test = len(test_dataset)\n",
    "se = math.sqrt(test_acc * (1 - test_acc) / n_test)\n",
    "moe = 1.96 * se  # For a 95% confidence interval.\n",
    "print(\"\\nDirect Calculation:\")\n",
    "print(\"Standard Error:\", se)\n",
    "print(\"Margin of Error (95% CI):\", moe)\n",
    "print(\"95% Confidence Interval for Accuracy: [{:.4f}, {:.4f}]\".format(test_acc - moe, test_acc + moe))\n",
    "\n",
    "# Approach 2: Bootstrapping.\n",
    "n_bootstrap = 1000\n",
    "boot_acc = []\n",
    "n_samples = len(all_targets)\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_samples), n_samples, replace=True)\n",
    "    boot_accuracy = accuracy_score(all_targets[indices], all_preds[indices])\n",
    "    boot_acc.append(boot_accuracy)\n",
    "boot_acc = np.array(boot_acc)\n",
    "se_boot = np.std(boot_acc)\n",
    "moe_boot = 1.96 * se_boot\n",
    "print(\"\\nBootstrapping:\")\n",
    "print(\"Bootstrapped Mean Accuracy:\", np.mean(boot_acc))\n",
    "print(\"Standard Error (Bootstrap):\", se_boot)\n",
    "print(\"Margin of Error (Bootstrap, 95% CI):\", moe_boot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "187fea6f-b753-46d8-9cb6-527f17e7c56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Epoch 1/100: Train Loss = 0.9767, Val Loss = 0.7246, Val Acc = 0.7762\n",
      "  -> New best model saved.\n",
      "Epoch 2/100: Train Loss = 0.5400, Val Loss = 0.4258, Val Acc = 0.8459\n",
      "  -> New best model saved.\n",
      "Epoch 3/100: Train Loss = 0.4031, Val Loss = 0.3695, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 4/100: Train Loss = 0.3584, Val Loss = 0.3582, Val Acc = 0.8401\n",
      "  -> New best model saved.\n",
      "Epoch 5/100: Train Loss = 0.3462, Val Loss = 0.3451, Val Acc = 0.8459\n",
      "  -> New best model saved.\n",
      "Epoch 6/100: Train Loss = 0.3340, Val Loss = 0.3339, Val Acc = 0.8517\n",
      "  -> New best model saved.\n",
      "Epoch 7/100: Train Loss = 0.3247, Val Loss = 0.3278, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 8/100: Train Loss = 0.3061, Val Loss = 0.3238, Val Acc = 0.8605\n",
      "  -> New best model saved.\n",
      "Epoch 9/100: Train Loss = 0.3043, Val Loss = 0.3190, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 10/100: Train Loss = 0.3043, Val Loss = 0.3146, Val Acc = 0.8663\n",
      "  -> New best model saved.\n",
      "Epoch 11/100: Train Loss = 0.3111, Val Loss = 0.3240, Val Acc = 0.8721\n",
      "Epoch 12/100: Train Loss = 0.2978, Val Loss = 0.3084, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 13/100: Train Loss = 0.2938, Val Loss = 0.3153, Val Acc = 0.8692\n",
      "Epoch 14/100: Train Loss = 0.2881, Val Loss = 0.3094, Val Acc = 0.8692\n",
      "Epoch 15/100: Train Loss = 0.2730, Val Loss = 0.3092, Val Acc = 0.8634\n",
      "Epoch 16/100: Train Loss = 0.2721, Val Loss = 0.3062, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 17/100: Train Loss = 0.2742, Val Loss = 0.3095, Val Acc = 0.8605\n",
      "Epoch 18/100: Train Loss = 0.2734, Val Loss = 0.3046, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 19/100: Train Loss = 0.2616, Val Loss = 0.2988, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 20/100: Train Loss = 0.2692, Val Loss = 0.3114, Val Acc = 0.8488\n",
      "Epoch 21/100: Train Loss = 0.2553, Val Loss = 0.3043, Val Acc = 0.8721\n",
      "Epoch 22/100: Train Loss = 0.2574, Val Loss = 0.3102, Val Acc = 0.8808\n",
      "Epoch 23/100: Train Loss = 0.2733, Val Loss = 0.3043, Val Acc = 0.8779\n",
      "Epoch 24/100: Train Loss = 0.2486, Val Loss = 0.3114, Val Acc = 0.8808\n",
      "Early stopping triggered.\n",
      "\n",
      "Test Accuracy: 0.9130434782608695\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.84      0.85      0.84        72\n",
      "          CN       0.98      0.95      0.97       106\n",
      "         MCI       0.91      0.92      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.91      0.91      0.91       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n",
      "\n",
      "Direct Calculation:\n",
      "Standard Error: 0.015170058712846038\n",
      "Margin of Error (95% CI): 0.029733315077178232\n",
      "95% Confidence Interval for Accuracy: [0.8833, 0.9428]\n",
      "\n",
      "Bootstrapping:\n",
      "Bootstrapped Mean Accuracy: 0.9133565217391304\n",
      "Standard Error (Bootstrap): 0.014701778444630168\n",
      "Margin of Error (Bootstrap, 95% CI): 0.02881548575147513\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import math\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Prepare Features for Each Modality\n",
    "##########################################\n",
    "# Load DeiT features (e.g., shape: [num_samples, 768])\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")  \n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")      \n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")     \n",
    "\n",
    "# Load FTTransformer features (e.g., shape: [num_samples, 192])\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")        \n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")          \n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")         \n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create PyTorch Datasets and DataLoaders\n",
    "##########################################\n",
    "# Convert features to tensors (keep modalities separate for fusion)\n",
    "train_deit_tensor = torch.tensor(deit_train, dtype=torch.float32)\n",
    "train_ft_tensor   = torch.tensor(ft_train, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "val_deit_tensor = torch.tensor(deit_val, dtype=torch.float32)\n",
    "val_ft_tensor   = torch.tensor(ft_val, dtype=torch.float32)\n",
    "val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
    "\n",
    "test_deit_tensor = torch.tensor(deit_test, dtype=torch.float32)\n",
    "test_ft_tensor   = torch.tensor(ft_test, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets.\n",
    "train_dataset = TensorDataset(train_deit_tensor, train_ft_tensor, train_labels_tensor)\n",
    "val_dataset   = TensorDataset(val_deit_tensor, val_ft_tensor, val_labels_tensor)\n",
    "test_dataset  = TensorDataset(test_deit_tensor, test_ft_tensor, test_labels_tensor)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define a Multi-Head Attention Fusion Classifier\n",
    "##########################################\n",
    "class MultiheadAttentionFusionClassifier(nn.Module):\n",
    "    def __init__(self, deit_dim, ft_dim, embed_dim, num_heads, num_classes):\n",
    "        super(MultiheadAttentionFusionClassifier, self).__init__()\n",
    "        # Project each modality into a common embedding space.\n",
    "        self.deit_proj = nn.Linear(deit_dim, embed_dim)\n",
    "        self.ft_proj = nn.Linear(ft_dim, embed_dim)\n",
    "        # Define a multi-head attention layer.\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=0.3)\n",
    "        # Classifier: fuse features after attention.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, deit_input, ft_input):\n",
    "        # Project the two modalities.\n",
    "        deit_emb = self.deit_proj(deit_input)  # (batch_size, embed_dim)\n",
    "        ft_emb = self.ft_proj(ft_input)          # (batch_size, embed_dim)\n",
    "        # Stack the modality embeddings to form a sequence (2 tokens per sample).\n",
    "        # Shape: (batch_size, 2, embed_dim)\n",
    "        fusion_seq = torch.stack([deit_emb, ft_emb], dim=1)\n",
    "        # nn.MultiheadAttention expects input as (sequence_length, batch_size, embed_dim)\n",
    "        fusion_seq = fusion_seq.transpose(0, 1)  # Now (2, batch_size, embed_dim)\n",
    "        # Apply multi-head attention (self-attention on the fusion sequence).\n",
    "        attn_output, _ = self.mha(fusion_seq, fusion_seq, fusion_seq)\n",
    "        # Aggregate the sequence tokens. Here we use mean pooling.\n",
    "        fused = attn_output.mean(dim=0)  # (batch_size, embed_dim)\n",
    "        logits = self.classifier(fused)\n",
    "        return logits\n",
    "\n",
    "# Set dimensions. For example, DeiT features are 768 and FTTransformer features are 192.\n",
    "deit_dim = deit_train.shape[1]  # 768\n",
    "ft_dim = ft_train.shape[1]      # 192\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "\n",
    "model = MultiheadAttentionFusionClassifier(deit_dim, ft_dim, embed_dim, num_heads, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "##########################################\n",
    "# 5. Train the Multi-Head Attention Fusion Classifier with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "num_epochs = 100\n",
    "patience = 5  # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for deit_x, ft_x, y in train_loader:\n",
    "        deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(deit_x, ft_x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * y.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds_val = []\n",
    "    all_targets_val = []\n",
    "    with torch.no_grad():\n",
    "        for deit_x, ft_x, y in val_loader:\n",
    "            deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "            logits = model(deit_x, ft_x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * y.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds_val.append(preds.cpu().numpy())\n",
    "            all_targets_val.append(y.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds_val = np.concatenate(all_preds_val)\n",
    "    all_targets_val = np.concatenate(all_targets_val)\n",
    "    val_acc = accuracy_score(all_targets_val, all_preds_val)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load best model state.\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for deit_x, ft_x, y in test_loader:\n",
    "        deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "        logits = model(deit_x, ft_x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", test_acc)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))\n",
    "\n",
    "##########################################\n",
    "# 7. Calculate the Margin of Error for Test Accuracy\n",
    "##########################################\n",
    "# Approach 1: Direct Calculation using the binomial formula.\n",
    "n_test = len(test_dataset)\n",
    "se = math.sqrt(test_acc * (1 - test_acc) / n_test)\n",
    "moe = 1.96 * se  # For a 95% confidence interval.\n",
    "print(\"\\nDirect Calculation:\")\n",
    "print(\"Standard Error:\", se)\n",
    "print(\"Margin of Error (95% CI):\", moe)\n",
    "print(\"95% Confidence Interval for Accuracy: [{:.4f}, {:.4f}]\".format(test_acc - moe, test_acc + moe))\n",
    "\n",
    "# Approach 2: Bootstrapping.\n",
    "n_bootstrap = 1000\n",
    "boot_acc = []\n",
    "n_samples = len(all_targets)\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_samples), n_samples, replace=True)\n",
    "    boot_accuracy = accuracy_score(all_targets[indices], all_preds[indices])\n",
    "    boot_acc.append(boot_accuracy)\n",
    "boot_acc = np.array(boot_acc)\n",
    "se_boot = np.std(boot_acc)\n",
    "moe_boot = 1.96 * se_boot\n",
    "print(\"\\nBootstrapping:\")\n",
    "print(\"Bootstrapped Mean Accuracy:\", np.mean(boot_acc))\n",
    "print(\"Standard Error (Bootstrap):\", se_boot)\n",
    "print(\"Margin of Error (Bootstrap, 95% CI):\", moe_boot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1198479-de01-437b-bbc4-652eda506772",
   "metadata": {},
   "source": [
    "## Co-Attention Fusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9b292bc-2ded-4756-97fd-8a920c52b1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Epoch 1/100: Train Loss = 1.0497, Val Loss = 1.0037, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 2/100: Train Loss = 0.9504, Val Loss = 0.8466, Val Acc = 0.7587\n",
      "  -> New best model saved.\n",
      "Epoch 3/100: Train Loss = 0.7530, Val Loss = 0.6219, Val Acc = 0.7674\n",
      "  -> New best model saved.\n",
      "Epoch 4/100: Train Loss = 0.5585, Val Loss = 0.4714, Val Acc = 0.7791\n",
      "  -> New best model saved.\n",
      "Epoch 5/100: Train Loss = 0.4432, Val Loss = 0.3956, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 6/100: Train Loss = 0.3730, Val Loss = 0.3641, Val Acc = 0.8634\n",
      "  -> New best model saved.\n",
      "Epoch 7/100: Train Loss = 0.3454, Val Loss = 0.3519, Val Acc = 0.8605\n",
      "  -> New best model saved.\n",
      "Epoch 8/100: Train Loss = 0.3154, Val Loss = 0.3464, Val Acc = 0.8488\n",
      "  -> New best model saved.\n",
      "Epoch 9/100: Train Loss = 0.3185, Val Loss = 0.3442, Val Acc = 0.8605\n",
      "  -> New best model saved.\n",
      "Epoch 10/100: Train Loss = 0.3074, Val Loss = 0.3384, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 11/100: Train Loss = 0.3066, Val Loss = 0.3337, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 12/100: Train Loss = 0.2877, Val Loss = 0.3298, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 13/100: Train Loss = 0.2905, Val Loss = 0.3230, Val Acc = 0.8634\n",
      "  -> New best model saved.\n",
      "Epoch 14/100: Train Loss = 0.2727, Val Loss = 0.3178, Val Acc = 0.8634\n",
      "  -> New best model saved.\n",
      "Epoch 15/100: Train Loss = 0.2693, Val Loss = 0.3127, Val Acc = 0.8663\n",
      "  -> New best model saved.\n",
      "Epoch 16/100: Train Loss = 0.2677, Val Loss = 0.3125, Val Acc = 0.8663\n",
      "  -> New best model saved.\n",
      "Epoch 17/100: Train Loss = 0.2583, Val Loss = 0.3095, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 18/100: Train Loss = 0.2543, Val Loss = 0.3058, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 19/100: Train Loss = 0.2434, Val Loss = 0.3090, Val Acc = 0.8779\n",
      "Epoch 20/100: Train Loss = 0.2344, Val Loss = 0.3023, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 21/100: Train Loss = 0.2320, Val Loss = 0.3015, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 22/100: Train Loss = 0.2360, Val Loss = 0.3027, Val Acc = 0.8692\n",
      "Epoch 23/100: Train Loss = 0.2266, Val Loss = 0.2972, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 24/100: Train Loss = 0.2208, Val Loss = 0.2979, Val Acc = 0.8721\n",
      "Epoch 25/100: Train Loss = 0.2092, Val Loss = 0.2940, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 26/100: Train Loss = 0.2068, Val Loss = 0.2912, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 27/100: Train Loss = 0.1950, Val Loss = 0.2899, Val Acc = 0.8866\n",
      "  -> New best model saved.\n",
      "Epoch 28/100: Train Loss = 0.1877, Val Loss = 0.2895, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 29/100: Train Loss = 0.1870, Val Loss = 0.2845, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 30/100: Train Loss = 0.1774, Val Loss = 0.2880, Val Acc = 0.8924\n",
      "Epoch 31/100: Train Loss = 0.1701, Val Loss = 0.2797, Val Acc = 0.8924\n",
      "  -> New best model saved.\n",
      "Epoch 32/100: Train Loss = 0.1607, Val Loss = 0.2886, Val Acc = 0.8924\n",
      "Epoch 33/100: Train Loss = 0.1535, Val Loss = 0.2921, Val Acc = 0.8895\n",
      "Epoch 34/100: Train Loss = 0.1540, Val Loss = 0.2877, Val Acc = 0.8895\n",
      "Epoch 35/100: Train Loss = 0.1447, Val Loss = 0.2936, Val Acc = 0.8983\n",
      "Epoch 36/100: Train Loss = 0.1269, Val Loss = 0.2906, Val Acc = 0.8983\n",
      "Early stopping triggered.\n",
      "\n",
      "Test Accuracy: 0.9101449275362319\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.82      0.88      0.85        72\n",
      "          CN       0.97      0.95      0.96       106\n",
      "         MCI       0.91      0.90      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.90      0.91      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n",
      "\n",
      "Direct Calculation:\n",
      "Standard Error: 0.015396323522784855\n",
      "Margin of Error (95% CI): 0.030176794104658317\n",
      "95% Confidence Interval for Accuracy: [0.8800, 0.9403]\n",
      "\n",
      "Bootstrapping:\n",
      "Bootstrapped Mean Accuracy: 0.9111449275362319\n",
      "Standard Error (Bootstrap): 0.015408182835710263\n",
      "Margin of Error (Bootstrap, 95% CI): 0.030200038357992114\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import math\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Prepare Features for Each Modality\n",
    "##########################################\n",
    "# Load DeiT features (expected shape: [num_samples, 768])\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")  \n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")      \n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")     \n",
    "\n",
    "# Load FTTransformer features (expected shape: [num_samples, 192])\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")        \n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")          \n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")         \n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create PyTorch Datasets and DataLoaders\n",
    "##########################################\n",
    "# We keep modalities separate so the fusion module can process them independently.\n",
    "train_deit_tensor = torch.tensor(deit_train, dtype=torch.float32)\n",
    "train_ft_tensor   = torch.tensor(ft_train, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "val_deit_tensor = torch.tensor(deit_val, dtype=torch.float32)\n",
    "val_ft_tensor   = torch.tensor(ft_val, dtype=torch.float32)\n",
    "val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
    "\n",
    "test_deit_tensor = torch.tensor(deit_test, dtype=torch.float32)\n",
    "test_ft_tensor   = torch.tensor(ft_test, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets.\n",
    "train_dataset = TensorDataset(train_deit_tensor, train_ft_tensor, train_labels_tensor)\n",
    "val_dataset   = TensorDataset(val_deit_tensor, val_ft_tensor, val_labels_tensor)\n",
    "test_dataset  = TensorDataset(test_deit_tensor, test_ft_tensor, test_labels_tensor)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define a Co-Attention Fusion Classifier\n",
    "##########################################\n",
    "class CoAttentionFusionClassifier(nn.Module):\n",
    "    def __init__(self, deit_dim, ft_dim, embed_dim, num_classes):\n",
    "        \"\"\"\n",
    "        This module projects each modality into a common embedding space.\n",
    "        Then, it computes a co-attention score based on the dot product between\n",
    "        the projected features (scaled by sqrt(embed_dim)). The sigmoid of this\n",
    "        score is used as a gating weight to fuse the two modalities.\n",
    "        \"\"\"\n",
    "        super(CoAttentionFusionClassifier, self).__init__()\n",
    "        # Project each modality to the common embedding space.\n",
    "        self.deit_proj = nn.Linear(deit_dim, embed_dim)\n",
    "        self.ft_proj = nn.Linear(ft_dim, embed_dim)\n",
    "        \n",
    "        # Optionally, you can add nonlinearity or normalization here.\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        # Final classifier.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Scaling factor to stabilize the dot-product.\n",
    "        self.scale = math.sqrt(embed_dim)\n",
    "    \n",
    "    def forward(self, deit_input, ft_input):\n",
    "        # Project modalities.\n",
    "        deit_emb = self.activation(self.deit_proj(deit_input))  # (batch_size, embed_dim)\n",
    "        ft_emb = self.activation(self.ft_proj(ft_input))          # (batch_size, embed_dim)\n",
    "        \n",
    "        # Compute a co-attention score.\n",
    "        # Dot-product: (batch_size, ) computed per sample.\n",
    "        score = (deit_emb * ft_emb).sum(dim=1) / self.scale\n",
    "        # Compute a gating weight in [0, 1] for each sample.\n",
    "        alpha = torch.sigmoid(score).unsqueeze(1)  # (batch_size, 1)\n",
    "        \n",
    "        # Fuse modalities using the co-attention weight.\n",
    "        # When alpha is high, deit_emb will have greater contribution; otherwise ft_emb dominates.\n",
    "        fused = alpha * deit_emb + (1 - alpha) * ft_emb  # (batch_size, embed_dim)\n",
    "        \n",
    "        # Classify.\n",
    "        logits = self.classifier(fused)\n",
    "        return logits\n",
    "\n",
    "# Set dimensions.\n",
    "deit_dim = deit_train.shape[1]  # e.g., 768\n",
    "ft_dim = ft_train.shape[1]      # e.g., 192\n",
    "embed_dim = 512\n",
    "\n",
    "model = CoAttentionFusionClassifier(deit_dim, ft_dim, embed_dim, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "##########################################\n",
    "# 5. Train the Co-Attention Fusion Classifier with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "num_epochs = 100\n",
    "patience = 5  # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for deit_x, ft_x, y in train_loader:\n",
    "        deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(deit_x, ft_x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * y.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Evaluate on the validation set.\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds_val = []\n",
    "    all_targets_val = []\n",
    "    with torch.no_grad():\n",
    "        for deit_x, ft_x, y in val_loader:\n",
    "            deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "            logits = model(deit_x, ft_x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * y.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds_val.append(preds.cpu().numpy())\n",
    "            all_targets_val.append(y.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds_val = np.concatenate(all_preds_val)\n",
    "    all_targets_val = np.concatenate(all_targets_val)\n",
    "    val_acc = accuracy_score(all_targets_val, all_preds_val)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model state.\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for deit_x, ft_x, y in test_loader:\n",
    "        deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "        logits = model(deit_x, ft_x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", test_acc)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))\n",
    "\n",
    "##########################################\n",
    "# 7. Calculate the Margin of Error for Test Accuracy\n",
    "##########################################\n",
    "# Approach 1: Direct Calculation using the binomial formula.\n",
    "n_test = len(test_dataset)\n",
    "se = math.sqrt(test_acc * (1 - test_acc) / n_test)\n",
    "moe = 1.96 * se  # For a 95% confidence interval.\n",
    "print(\"\\nDirect Calculation:\")\n",
    "print(\"Standard Error:\", se)\n",
    "print(\"Margin of Error (95% CI):\", moe)\n",
    "print(\"95% Confidence Interval for Accuracy: [{:.4f}, {:.4f}]\".format(test_acc - moe, test_acc + moe))\n",
    "\n",
    "# Approach 2: Bootstrapping.\n",
    "n_bootstrap = 1000\n",
    "boot_acc = []\n",
    "n_samples = len(all_targets)\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_samples), n_samples, replace=True)\n",
    "    boot_accuracy = accuracy_score(all_targets[indices], all_preds[indices])\n",
    "    boot_acc.append(boot_accuracy)\n",
    "boot_acc = np.array(boot_acc)\n",
    "se_boot = np.std(boot_acc)\n",
    "moe_boot = 1.96 * se_boot\n",
    "print(\"\\nBootstrapping:\")\n",
    "print(\"Bootstrapped Mean Accuracy:\", np.mean(boot_acc))\n",
    "print(\"Standard Error (Bootstrap):\", se_boot)\n",
    "print(\"Margin of Error (Bootstrap, 95% CI):\", moe_boot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37347413-4fc0-48fc-ad62-b5d566e622d5",
   "metadata": {},
   "source": [
    "## Cross Attention Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "395d2e71-c77c-48d9-b377-212073ceb5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Epoch 1/100: Train Loss = 1.0351, Val Loss = 0.9144, Val Acc = 0.5407\n",
      "  -> New best model saved.\n",
      "Epoch 2/100: Train Loss = 0.7710, Val Loss = 0.5522, Val Acc = 0.7733\n",
      "  -> New best model saved.\n",
      "Epoch 3/100: Train Loss = 0.4832, Val Loss = 0.3759, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 4/100: Train Loss = 0.3676, Val Loss = 0.3391, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 5/100: Train Loss = 0.3265, Val Loss = 0.3337, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 6/100: Train Loss = 0.3208, Val Loss = 0.3396, Val Acc = 0.8488\n",
      "Epoch 7/100: Train Loss = 0.3295, Val Loss = 0.3373, Val Acc = 0.8547\n",
      "Epoch 8/100: Train Loss = 0.3274, Val Loss = 0.3355, Val Acc = 0.8547\n",
      "Epoch 9/100: Train Loss = 0.3166, Val Loss = 0.3340, Val Acc = 0.8488\n",
      "Epoch 10/100: Train Loss = 0.3145, Val Loss = 0.3302, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 11/100: Train Loss = 0.3006, Val Loss = 0.3265, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 12/100: Train Loss = 0.3163, Val Loss = 0.3399, Val Acc = 0.8401\n",
      "Epoch 13/100: Train Loss = 0.2994, Val Loss = 0.3310, Val Acc = 0.8517\n",
      "Epoch 14/100: Train Loss = 0.3065, Val Loss = 0.3229, Val Acc = 0.8663\n",
      "  -> New best model saved.\n",
      "Epoch 15/100: Train Loss = 0.3061, Val Loss = 0.3206, Val Acc = 0.8634\n",
      "  -> New best model saved.\n",
      "Epoch 16/100: Train Loss = 0.3004, Val Loss = 0.3292, Val Acc = 0.8459\n",
      "Epoch 17/100: Train Loss = 0.3012, Val Loss = 0.3175, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 18/100: Train Loss = 0.3030, Val Loss = 0.3155, Val Acc = 0.8605\n",
      "  -> New best model saved.\n",
      "Epoch 19/100: Train Loss = 0.2881, Val Loss = 0.3227, Val Acc = 0.8547\n",
      "Epoch 20/100: Train Loss = 0.2918, Val Loss = 0.3171, Val Acc = 0.8692\n",
      "Epoch 21/100: Train Loss = 0.2899, Val Loss = 0.3149, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 22/100: Train Loss = 0.2868, Val Loss = 0.3149, Val Acc = 0.8634\n",
      "Epoch 23/100: Train Loss = 0.2884, Val Loss = 0.3109, Val Acc = 0.8634\n",
      "  -> New best model saved.\n",
      "Epoch 24/100: Train Loss = 0.2926, Val Loss = 0.3076, Val Acc = 0.8634\n",
      "  -> New best model saved.\n",
      "Epoch 25/100: Train Loss = 0.2862, Val Loss = 0.3095, Val Acc = 0.8750\n",
      "Epoch 26/100: Train Loss = 0.2881, Val Loss = 0.3113, Val Acc = 0.8634\n",
      "Epoch 27/100: Train Loss = 0.2752, Val Loss = 0.3128, Val Acc = 0.8663\n",
      "Epoch 28/100: Train Loss = 0.2724, Val Loss = 0.3080, Val Acc = 0.8634\n",
      "Epoch 29/100: Train Loss = 0.2734, Val Loss = 0.3055, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 30/100: Train Loss = 0.2709, Val Loss = 0.3048, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 31/100: Train Loss = 0.2705, Val Loss = 0.3087, Val Acc = 0.8692\n",
      "Epoch 32/100: Train Loss = 0.2768, Val Loss = 0.3005, Val Acc = 0.8663\n",
      "  -> New best model saved.\n",
      "Epoch 33/100: Train Loss = 0.2682, Val Loss = 0.2996, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 34/100: Train Loss = 0.2685, Val Loss = 0.3019, Val Acc = 0.8663\n",
      "Epoch 35/100: Train Loss = 0.2616, Val Loss = 0.2994, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 36/100: Train Loss = 0.2654, Val Loss = 0.2986, Val Acc = 0.8634\n",
      "  -> New best model saved.\n",
      "Epoch 37/100: Train Loss = 0.2565, Val Loss = 0.2977, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 38/100: Train Loss = 0.2695, Val Loss = 0.2966, Val Acc = 0.8663\n",
      "  -> New best model saved.\n",
      "Epoch 39/100: Train Loss = 0.2481, Val Loss = 0.2995, Val Acc = 0.8721\n",
      "Epoch 40/100: Train Loss = 0.2469, Val Loss = 0.3025, Val Acc = 0.8634\n",
      "Epoch 41/100: Train Loss = 0.2508, Val Loss = 0.2958, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 42/100: Train Loss = 0.2426, Val Loss = 0.2998, Val Acc = 0.8721\n",
      "Epoch 43/100: Train Loss = 0.2393, Val Loss = 0.3017, Val Acc = 0.8750\n",
      "Epoch 44/100: Train Loss = 0.2408, Val Loss = 0.2943, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 45/100: Train Loss = 0.2464, Val Loss = 0.2977, Val Acc = 0.8808\n",
      "Epoch 46/100: Train Loss = 0.2362, Val Loss = 0.2939, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 47/100: Train Loss = 0.2424, Val Loss = 0.2938, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 48/100: Train Loss = 0.2394, Val Loss = 0.3043, Val Acc = 0.8721\n",
      "Epoch 49/100: Train Loss = 0.2412, Val Loss = 0.2922, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 50/100: Train Loss = 0.2328, Val Loss = 0.2930, Val Acc = 0.8866\n",
      "Epoch 51/100: Train Loss = 0.2284, Val Loss = 0.3004, Val Acc = 0.8837\n",
      "Epoch 52/100: Train Loss = 0.2256, Val Loss = 0.2995, Val Acc = 0.8837\n",
      "Epoch 53/100: Train Loss = 0.2248, Val Loss = 0.2927, Val Acc = 0.8895\n",
      "Epoch 54/100: Train Loss = 0.2213, Val Loss = 0.2969, Val Acc = 0.8866\n",
      "Early stopping triggered.\n",
      "\n",
      "Test Accuracy: 0.9072463768115943\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.81      0.86      0.83        72\n",
      "          CN       0.98      0.95      0.97       106\n",
      "         MCI       0.91      0.90      0.90       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.90      0.90      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n",
      "\n",
      "Direct Calculation:\n",
      "Standard Error: 0.015617751751747575\n",
      "Margin of Error (95% CI): 0.030610793433425248\n",
      "95% Confidence Interval for Accuracy: [0.8766, 0.9379]\n",
      "\n",
      "Bootstrapping:\n",
      "Bootstrapped Mean Accuracy: 0.9059797101449275\n",
      "Standard Error (Bootstrap): 0.015315770605410447\n",
      "Margin of Error (Bootstrap, 95% CI): 0.030018910386604477\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import math\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Prepare Features for Each Modality\n",
    "##########################################\n",
    "# Load DeiT features (e.g., shape: [num_samples, 768])\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")\n",
    "\n",
    "# Load FTTransformer features (e.g., shape: [num_samples, 192])\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")\n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create PyTorch Datasets and DataLoaders\n",
    "##########################################\n",
    "# For cross-attention fusion, we keep modalities separate.\n",
    "train_deit_tensor = torch.tensor(deit_train, dtype=torch.float32)\n",
    "train_ft_tensor   = torch.tensor(ft_train, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "val_deit_tensor = torch.tensor(deit_val, dtype=torch.float32)\n",
    "val_ft_tensor   = torch.tensor(ft_val, dtype=torch.float32)\n",
    "val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
    "\n",
    "test_deit_tensor = torch.tensor(deit_test, dtype=torch.float32)\n",
    "test_ft_tensor   = torch.tensor(ft_test, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets.\n",
    "train_dataset = TensorDataset(train_deit_tensor, train_ft_tensor, train_labels_tensor)\n",
    "val_dataset   = TensorDataset(val_deit_tensor, val_ft_tensor, val_labels_tensor)\n",
    "test_dataset  = TensorDataset(test_deit_tensor, test_ft_tensor, test_labels_tensor)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define a Cross-Attention Fusion Classifier\n",
    "##########################################\n",
    "class CrossAttentionFusionClassifier(nn.Module):\n",
    "    def __init__(self, deit_dim, ft_dim, embed_dim, num_classes):\n",
    "        \"\"\"\n",
    "        In this model, we use the DeiT features (image modality)\n",
    "        as the query, and the FTTransformer features (other modality)\n",
    "        provide the keys and values for cross-attention.\n",
    "        \"\"\"\n",
    "        super(CrossAttentionFusionClassifier, self).__init__()\n",
    "        # Project modalities to a common embedding space.\n",
    "        self.deit_proj = nn.Linear(deit_dim, embed_dim)\n",
    "        # For cross-attention, use separate projections for keys and values.\n",
    "        self.ft_k_proj = nn.Linear(ft_dim, embed_dim)\n",
    "        self.ft_v_proj = nn.Linear(ft_dim, embed_dim)\n",
    "        \n",
    "        # MultiheadAttention layer for cross-attention.\n",
    "        # We'll treat the query as having a sequence length of 1.\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=embed_dim,\n",
    "                                                num_heads=8,\n",
    "                                                dropout=0.3)\n",
    "        # Optional: A linear layer to fuse the cross-attended output with the original query.\n",
    "        self.fusion_linear = nn.Linear(embed_dim * 2, embed_dim)\n",
    "        \n",
    "        # Final classifier.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(embed_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, deit_input, ft_input):\n",
    "        # Project the input features.\n",
    "        query = self.deit_proj(deit_input)         # (batch_size, embed_dim)\n",
    "        key = self.ft_k_proj(ft_input)               # (batch_size, embed_dim)\n",
    "        value = self.ft_v_proj(ft_input)             # (batch_size, embed_dim)\n",
    "        \n",
    "        # Unsqueeze to create a sequence dimension (sequence length = 1).\n",
    "        # New shape: (1, batch_size, embed_dim)\n",
    "        query_seq = query.unsqueeze(0)\n",
    "        key_seq = key.unsqueeze(0)\n",
    "        value_seq = value.unsqueeze(0)\n",
    "        \n",
    "        # Apply multi-head cross-attention:\n",
    "        # Query attends over Key and Value (from FTTransformer features).\n",
    "        # attn_output shape: (1, batch_size, embed_dim)\n",
    "        attn_output, _ = self.cross_attn(query_seq, key_seq, value_seq)\n",
    "        attn_output = attn_output.squeeze(0)  # (batch_size, embed_dim)\n",
    "        \n",
    "        # (Optional) Concatenate the original query representation\n",
    "        # with the cross-attended output and fuse them.\n",
    "        fused = torch.cat([query, attn_output], dim=1)  # (batch_size, embed_dim*2)\n",
    "        fused = self.fusion_linear(fused)              # (batch_size, embed_dim)\n",
    "        \n",
    "        # Classification.\n",
    "        logits = self.classifier(fused)\n",
    "        return logits\n",
    "\n",
    "# Set dimensions. For example, DeiT features are 768 and FTTransformer features are 192.\n",
    "deit_dim = deit_train.shape[1]  # e.g., 768\n",
    "ft_dim = ft_train.shape[1]      # e.g., 192\n",
    "embed_dim = 256\n",
    "\n",
    "model = CrossAttentionFusionClassifier(deit_dim, ft_dim, embed_dim, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "##########################################\n",
    "# 5. Train the Cross-Attention Fusion Classifier with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "num_epochs = 100\n",
    "patience = 5  # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for deit_x, ft_x, y in train_loader:\n",
    "        deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(deit_x, ft_x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * y.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Evaluate on the validation set.\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds_val = []\n",
    "    all_targets_val = []\n",
    "    with torch.no_grad():\n",
    "        for deit_x, ft_x, y in val_loader:\n",
    "            deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "            logits = model(deit_x, ft_x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * y.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds_val.append(preds.cpu().numpy())\n",
    "            all_targets_val.append(y.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds_val = np.concatenate(all_preds_val)\n",
    "    all_targets_val = np.concatenate(all_targets_val)\n",
    "    val_acc = accuracy_score(all_targets_val, all_preds_val)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping check.\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model state.\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for deit_x, ft_x, y in test_loader:\n",
    "        deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "        logits = model(deit_x, ft_x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", test_acc)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))\n",
    "\n",
    "##########################################\n",
    "# 7. Calculate the Margin of Error for Test Accuracy\n",
    "##########################################\n",
    "# Approach 1: Direct Calculation using the binomial formula.\n",
    "n_test = len(test_dataset)\n",
    "se = math.sqrt(test_acc * (1 - test_acc) / n_test)\n",
    "moe = 1.96 * se  # For a 95% confidence interval.\n",
    "print(\"\\nDirect Calculation:\")\n",
    "print(\"Standard Error:\", se)\n",
    "print(\"Margin of Error (95% CI):\", moe)\n",
    "print(\"95% Confidence Interval for Accuracy: [{:.4f}, {:.4f}]\".format(test_acc - moe, test_acc + moe))\n",
    "\n",
    "# Approach 2: Bootstrapping.\n",
    "n_bootstrap = 1000\n",
    "boot_acc = []\n",
    "n_samples = len(all_targets)\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_samples), n_samples, replace=True)\n",
    "    boot_accuracy = accuracy_score(all_targets[indices], all_preds[indices])\n",
    "    boot_acc.append(boot_accuracy)\n",
    "boot_acc = np.array(boot_acc)\n",
    "se_boot = np.std(boot_acc)\n",
    "moe_boot = 1.96 * se_boot\n",
    "print(\"\\nBootstrapping:\")\n",
    "print(\"Bootstrapped Mean Accuracy:\", np.mean(boot_acc))\n",
    "print(\"Standard Error (Bootstrap):\", se_boot)\n",
    "print(\"Margin of Error (Bootstrap, 95% CI):\", moe_boot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb580f11-7eb3-489a-9d07-3f2494ae09b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Epoch 1/100: Train Loss = 0.8928, Val Loss = 0.6121, Val Acc = 0.7733\n",
      "  -> New best model saved.\n",
      "Epoch 2/100: Train Loss = 0.5064, Val Loss = 0.3794, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 3/100: Train Loss = 0.3752, Val Loss = 0.3322, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 4/100: Train Loss = 0.3283, Val Loss = 0.3361, Val Acc = 0.8488\n",
      "Epoch 5/100: Train Loss = 0.3202, Val Loss = 0.3347, Val Acc = 0.8517\n",
      "Epoch 6/100: Train Loss = 0.3181, Val Loss = 0.3376, Val Acc = 0.8547\n",
      "Epoch 7/100: Train Loss = 0.3126, Val Loss = 0.3342, Val Acc = 0.8517\n",
      "Epoch 8/100: Train Loss = 0.3124, Val Loss = 0.3255, Val Acc = 0.8605\n",
      "  -> New best model saved.\n",
      "Epoch 9/100: Train Loss = 0.3026, Val Loss = 0.3236, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 10/100: Train Loss = 0.2993, Val Loss = 0.3193, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 11/100: Train Loss = 0.3000, Val Loss = 0.3124, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 12/100: Train Loss = 0.2935, Val Loss = 0.3094, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 13/100: Train Loss = 0.2820, Val Loss = 0.3104, Val Acc = 0.8663\n",
      "Epoch 14/100: Train Loss = 0.2750, Val Loss = 0.3010, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 15/100: Train Loss = 0.2726, Val Loss = 0.2996, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 16/100: Train Loss = 0.2682, Val Loss = 0.3217, Val Acc = 0.8576\n",
      "Epoch 17/100: Train Loss = 0.2679, Val Loss = 0.3085, Val Acc = 0.8634\n",
      "Epoch 18/100: Train Loss = 0.2508, Val Loss = 0.2992, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 19/100: Train Loss = 0.2465, Val Loss = 0.2993, Val Acc = 0.8779\n",
      "Epoch 20/100: Train Loss = 0.2448, Val Loss = 0.3189, Val Acc = 0.8750\n",
      "Epoch 21/100: Train Loss = 0.2469, Val Loss = 0.3082, Val Acc = 0.8837\n",
      "Epoch 22/100: Train Loss = 0.2338, Val Loss = 0.3047, Val Acc = 0.8808\n",
      "Epoch 23/100: Train Loss = 0.2347, Val Loss = 0.3060, Val Acc = 0.8866\n",
      "Early stopping triggered.\n",
      "\n",
      "Test Accuracy: 0.9014492753623189\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.88      0.72      0.79        72\n",
      "          CN       0.98      0.95      0.97       106\n",
      "         MCI       0.86      0.95      0.90       167\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.91      0.87      0.89       345\n",
      "weighted avg       0.90      0.90      0.90       345\n",
      "\n",
      "\n",
      "Direct Calculation:\n",
      "Standard Error: 0.016046894982606724\n",
      "Margin of Error (95% CI): 0.03145191416590918\n",
      "95% Confidence Interval for Accuracy: [0.8700, 0.9329]\n",
      "\n",
      "Bootstrapping:\n",
      "Bootstrapped Mean Accuracy: 0.9016376811594203\n",
      "Standard Error (Bootstrap): 0.015867751865162184\n",
      "Margin of Error (Bootstrap, 95% CI): 0.03110079365571788\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import math\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Prepare Features for Each Modality\n",
    "##########################################\n",
    "# Load DeiT features (e.g., shape: [num_samples, 768])\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")\n",
    "\n",
    "# Load FTTransformer features (e.g., shape: [num_samples, 192])\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")\n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create PyTorch Datasets and DataLoaders\n",
    "##########################################\n",
    "# For cross-attention fusion, we keep modalities separate.\n",
    "train_deit_tensor = torch.tensor(deit_train, dtype=torch.float32)\n",
    "train_ft_tensor   = torch.tensor(ft_train, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "val_deit_tensor = torch.tensor(deit_val, dtype=torch.float32)\n",
    "val_ft_tensor   = torch.tensor(ft_val, dtype=torch.float32)\n",
    "val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
    "\n",
    "test_deit_tensor = torch.tensor(deit_test, dtype=torch.float32)\n",
    "test_ft_tensor   = torch.tensor(ft_test, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets.\n",
    "train_dataset = TensorDataset(train_deit_tensor, train_ft_tensor, train_labels_tensor)\n",
    "val_dataset   = TensorDataset(val_deit_tensor, val_ft_tensor, val_labels_tensor)\n",
    "test_dataset  = TensorDataset(test_deit_tensor, test_ft_tensor, test_labels_tensor)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define a Cross-Attention Fusion Classifier with Swapped Modalities\n",
    "##########################################\n",
    "class CrossAttentionSwappedFusionClassifier(nn.Module):\n",
    "    def __init__(self, deit_dim, ft_dim, embed_dim, num_classes):\n",
    "        \"\"\"\n",
    "        Here, the stronger modality (FTTransformer) is used as the query,\n",
    "        while the weaker modality (DeiT) is used to generate keys and values.\n",
    "        \"\"\"\n",
    "        super(CrossAttentionSwappedFusionClassifier, self).__init__()\n",
    "        # Project FTTransformer features to the common embedding space (as Query).\n",
    "        self.ft_proj = nn.Linear(ft_dim, embed_dim)\n",
    "        # Project DeiT features to produce Keys and Values.\n",
    "        self.deit_k_proj = nn.Linear(deit_dim, embed_dim)\n",
    "        self.deit_v_proj = nn.Linear(deit_dim, embed_dim)\n",
    "        \n",
    "        # MultiheadAttention layer for cross-attention.\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=8, dropout=0.3)\n",
    "        \n",
    "        # A fusion layer to combine the original query with the attended output.\n",
    "        self.fusion_linear = nn.Linear(embed_dim * 2, embed_dim)\n",
    "        \n",
    "        # Final classifier network.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, deit_input, ft_input):\n",
    "        # Use FTTransformer features (ft_input) as the query.\n",
    "        query = self.ft_proj(ft_input)              # (batch_size, embed_dim)\n",
    "        # Use DeiT features (deit_input) to produce keys and values.\n",
    "        key = self.deit_k_proj(deit_input)            # (batch_size, embed_dim)\n",
    "        value = self.deit_v_proj(deit_input)          # (batch_size, embed_dim)\n",
    "        \n",
    "        # Unsqueeze to add a sequence dimension (expected shape: (sequence_length, batch_size, embed_dim)).\n",
    "        query_seq = query.unsqueeze(0)  # (1, batch_size, embed_dim)\n",
    "        key_seq = key.unsqueeze(0)      # (1, batch_size, embed_dim)\n",
    "        value_seq = value.unsqueeze(0)  # (1, batch_size, embed_dim)\n",
    "        \n",
    "        # Cross-attention: query attends over key/value.\n",
    "        attn_output, _ = self.cross_attn(query_seq, key_seq, value_seq)\n",
    "        attn_output = attn_output.squeeze(0)  # (batch_size, embed_dim)\n",
    "        \n",
    "        # Concatenate the original query with its cross-attended output and fuse them.\n",
    "        fused = torch.cat([query, attn_output], dim=1)  # (batch_size, embed_dim * 2)\n",
    "        fused = self.fusion_linear(fused)               # (batch_size, embed_dim)\n",
    "        \n",
    "        # Pass the fused representation through the classifier.\n",
    "        logits = self.classifier(fused)\n",
    "        return logits\n",
    "\n",
    "# Define dimensions. For example: DeiT features: 768, FTTransformer features: 192.\n",
    "deit_dim = deit_train.shape[1]  # e.g., 768\n",
    "ft_dim = ft_train.shape[1]      # e.g., 192\n",
    "embed_dim = 512\n",
    "\n",
    "model = CrossAttentionSwappedFusionClassifier(deit_dim, ft_dim, embed_dim, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "##########################################\n",
    "# 5. Train the Cross-Attention Fusion Classifier with Swapped Modalities (Early Stopping)\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "num_epochs = 100\n",
    "patience = 5  # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for deit_x, ft_x, y in train_loader:\n",
    "        deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(deit_x, ft_x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * y.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Validate.\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds_val = []\n",
    "    all_targets_val = []\n",
    "    with torch.no_grad():\n",
    "        for deit_x, ft_x, y in val_loader:\n",
    "            deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "            logits = model(deit_x, ft_x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * y.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds_val.append(preds.cpu().numpy())\n",
    "            all_targets_val.append(y.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds_val = np.concatenate(all_preds_val)\n",
    "    all_targets_val = np.concatenate(all_targets_val)\n",
    "    val_acc = accuracy_score(all_targets_val, all_preds_val)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model state.\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for deit_x, ft_x, y in test_loader:\n",
    "        deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "        logits = model(deit_x, ft_x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", test_acc)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))\n",
    "\n",
    "##########################################\n",
    "# 7. Calculate the Margin of Error for Test Accuracy\n",
    "##########################################\n",
    "# Approach 1: Direct Calculation using the binomial formula.\n",
    "n_test = len(test_dataset)\n",
    "se = math.sqrt(test_acc * (1 - test_acc) / n_test)\n",
    "moe = 1.96 * se  # 95% CI\n",
    "print(\"\\nDirect Calculation:\")\n",
    "print(\"Standard Error:\", se)\n",
    "print(\"Margin of Error (95% CI):\", moe)\n",
    "print(\"95% Confidence Interval for Accuracy: [{:.4f}, {:.4f}]\".format(test_acc - moe, test_acc + moe))\n",
    "\n",
    "# Approach 2: Bootstrapping.\n",
    "n_bootstrap = 1000\n",
    "boot_acc = []\n",
    "n_samples = len(all_targets)\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_samples), n_samples, replace=True)\n",
    "    boot_accuracy = accuracy_score(all_targets[indices], all_preds[indices])\n",
    "    boot_acc.append(boot_accuracy)\n",
    "boot_acc = np.array(boot_acc)\n",
    "se_boot = np.std(boot_acc)\n",
    "moe_boot = 1.96 * se_boot\n",
    "print(\"\\nBootstrapping:\")\n",
    "print(\"Bootstrapped Mean Accuracy:\", np.mean(boot_acc))\n",
    "print(\"Standard Error (Bootstrap):\", se_boot)\n",
    "print(\"Margin of Error (Bootstrap, 95% CI):\", moe_boot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3ac381-f01a-438e-a08f-b0fd3789a13c",
   "metadata": {},
   "source": [
    "## Hierarchicial attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c1d47db-3244-4b7c-827a-363b5a63c410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Epoch 1/100: Train Loss = 1.0565, Val Loss = 0.8786, Val Acc = 0.7674\n",
      "  -> New best model saved.\n",
      "Epoch 2/100: Train Loss = 0.5792, Val Loss = 0.4051, Val Acc = 0.7762\n",
      "  -> New best model saved.\n",
      "Epoch 3/100: Train Loss = 0.3896, Val Loss = 0.3576, Val Acc = 0.8634\n",
      "  -> New best model saved.\n",
      "Epoch 4/100: Train Loss = 0.3497, Val Loss = 0.3602, Val Acc = 0.8430\n",
      "Epoch 5/100: Train Loss = 0.3431, Val Loss = 0.3449, Val Acc = 0.8517\n",
      "  -> New best model saved.\n",
      "Epoch 6/100: Train Loss = 0.3367, Val Loss = 0.3491, Val Acc = 0.8488\n",
      "Epoch 7/100: Train Loss = 0.3381, Val Loss = 0.3417, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 8/100: Train Loss = 0.3258, Val Loss = 0.3389, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 9/100: Train Loss = 0.3356, Val Loss = 0.3381, Val Acc = 0.8634\n",
      "  -> New best model saved.\n",
      "Epoch 10/100: Train Loss = 0.3363, Val Loss = 0.3351, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 11/100: Train Loss = 0.3317, Val Loss = 0.3314, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 12/100: Train Loss = 0.3210, Val Loss = 0.3297, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 13/100: Train Loss = 0.3186, Val Loss = 0.3330, Val Acc = 0.8576\n",
      "Epoch 14/100: Train Loss = 0.3272, Val Loss = 0.3378, Val Acc = 0.8459\n",
      "Epoch 15/100: Train Loss = 0.3280, Val Loss = 0.3360, Val Acc = 0.8488\n",
      "Epoch 16/100: Train Loss = 0.3232, Val Loss = 0.3281, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 17/100: Train Loss = 0.3113, Val Loss = 0.3284, Val Acc = 0.8547\n",
      "Epoch 18/100: Train Loss = 0.3134, Val Loss = 0.3261, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 19/100: Train Loss = 0.3246, Val Loss = 0.3286, Val Acc = 0.8576\n",
      "Epoch 20/100: Train Loss = 0.3035, Val Loss = 0.3356, Val Acc = 0.8488\n",
      "Epoch 21/100: Train Loss = 0.3130, Val Loss = 0.3295, Val Acc = 0.8576\n",
      "Epoch 22/100: Train Loss = 0.3145, Val Loss = 0.3295, Val Acc = 0.8547\n",
      "Epoch 23/100: Train Loss = 0.3202, Val Loss = 0.3236, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 24/100: Train Loss = 0.3150, Val Loss = 0.3243, Val Acc = 0.8547\n",
      "Epoch 25/100: Train Loss = 0.3200, Val Loss = 0.3251, Val Acc = 0.8576\n",
      "Epoch 26/100: Train Loss = 0.3132, Val Loss = 0.3294, Val Acc = 0.8547\n",
      "Epoch 27/100: Train Loss = 0.3131, Val Loss = 0.3414, Val Acc = 0.8430\n",
      "Epoch 28/100: Train Loss = 0.3123, Val Loss = 0.3234, Val Acc = 0.8663\n",
      "  -> New best model saved.\n",
      "Epoch 29/100: Train Loss = 0.3070, Val Loss = 0.3255, Val Acc = 0.8547\n",
      "Epoch 30/100: Train Loss = 0.3128, Val Loss = 0.3265, Val Acc = 0.8547\n",
      "Epoch 31/100: Train Loss = 0.3084, Val Loss = 0.3262, Val Acc = 0.8547\n",
      "Epoch 32/100: Train Loss = 0.3033, Val Loss = 0.3244, Val Acc = 0.8547\n",
      "Epoch 33/100: Train Loss = 0.3064, Val Loss = 0.3190, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 34/100: Train Loss = 0.3098, Val Loss = 0.3232, Val Acc = 0.8576\n",
      "Epoch 35/100: Train Loss = 0.3079, Val Loss = 0.3197, Val Acc = 0.8576\n",
      "Epoch 36/100: Train Loss = 0.3107, Val Loss = 0.3184, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 37/100: Train Loss = 0.3144, Val Loss = 0.3259, Val Acc = 0.8663\n",
      "Epoch 38/100: Train Loss = 0.3179, Val Loss = 0.3264, Val Acc = 0.8576\n",
      "Epoch 39/100: Train Loss = 0.3032, Val Loss = 0.3373, Val Acc = 0.8430\n",
      "Epoch 40/100: Train Loss = 0.3086, Val Loss = 0.3180, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 41/100: Train Loss = 0.3010, Val Loss = 0.3210, Val Acc = 0.8547\n",
      "Epoch 42/100: Train Loss = 0.2970, Val Loss = 0.3161, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 43/100: Train Loss = 0.2842, Val Loss = 0.3304, Val Acc = 0.8547\n",
      "Epoch 44/100: Train Loss = 0.2883, Val Loss = 0.3228, Val Acc = 0.8634\n",
      "Epoch 45/100: Train Loss = 0.2873, Val Loss = 0.3295, Val Acc = 0.8547\n",
      "Epoch 46/100: Train Loss = 0.2905, Val Loss = 0.3192, Val Acc = 0.8779\n",
      "Epoch 47/100: Train Loss = 0.2807, Val Loss = 0.3384, Val Acc = 0.8547\n",
      "Early stopping triggered.\n",
      "\n",
      "Test Accuracy: 0.9130434782608695\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.81      0.89      0.85        72\n",
      "          CN       0.98      0.95      0.97       106\n",
      "         MCI       0.92      0.90      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.90      0.91      0.91       345\n",
      "weighted avg       0.92      0.91      0.91       345\n",
      "\n",
      "\n",
      "Direct Calculation:\n",
      "Standard Error: 0.015170058712846038\n",
      "Margin of Error (95% CI): 0.029733315077178232\n",
      "95% Confidence Interval for Accuracy: [0.8833, 0.9428]\n",
      "\n",
      "Bootstrapping:\n",
      "Bootstrapped Mean Accuracy: 0.9132608695652173\n",
      "Standard Error (Bootstrap): 0.014977228391807233\n",
      "Margin of Error (Bootstrap, 95% CI): 0.029355367647942176\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import math\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Prepare Features for Each Modality\n",
    "##########################################\n",
    "# Load DeiT features (expected shape: [num_samples, 768])\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")\n",
    "\n",
    "# Load FTTransformer features (expected shape: [num_samples, 192])\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")\n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create PyTorch Datasets and DataLoaders\n",
    "##########################################\n",
    "# We keep modalities separate, as the model will process them with hierarchical attention.\n",
    "train_deit_tensor = torch.tensor(deit_train, dtype=torch.float32)\n",
    "train_ft_tensor   = torch.tensor(ft_train, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "val_deit_tensor = torch.tensor(deit_val, dtype=torch.float32)\n",
    "val_ft_tensor   = torch.tensor(ft_val, dtype=torch.float32)\n",
    "val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
    "\n",
    "test_deit_tensor = torch.tensor(deit_test, dtype=torch.float32)\n",
    "test_ft_tensor   = torch.tensor(ft_test, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(train_deit_tensor, train_ft_tensor, train_labels_tensor)\n",
    "val_dataset   = TensorDataset(val_deit_tensor, val_ft_tensor, val_labels_tensor)\n",
    "test_dataset  = TensorDataset(test_deit_tensor, test_ft_tensor, test_labels_tensor)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define a Hierarchical Attention Fusion Classifier\n",
    "##########################################\n",
    "class HierarchicalAttentionFusionClassifier(nn.Module):\n",
    "    def __init__(self, deit_dim, ft_dim, tokens_deit, tokens_ft, embed_dim, num_heads, num_classes):\n",
    "        \"\"\"\n",
    "        Hierarchical Attention Fusion:\n",
    "          - Stage 1: Within-modality attention.\n",
    "            Split each modality's feature vector into tokens and apply self-attention.\n",
    "          - Stage 2: Across-modality fusion.\n",
    "            Fuse the modality representations via self-attention.\n",
    "        Args:\n",
    "          deit_dim: dimension of DeiT features (e.g., 768)\n",
    "          ft_dim: dimension of FTTransformer features (e.g., 192)\n",
    "          tokens_deit: number of tokens to split DeiT features into (e.g., 8)\n",
    "          tokens_ft: number of tokens to split FTTransformer features into (e.g., 8)\n",
    "          embed_dim: common embedding dimension for tokens (e.g., 256)\n",
    "          num_heads: number of attention heads\n",
    "          num_classes: number of output classes\n",
    "        \"\"\"\n",
    "        super(HierarchicalAttentionFusionClassifier, self).__init__()\n",
    "        self.tokens_deit = tokens_deit\n",
    "        self.tokens_ft = tokens_ft\n",
    "        self.deit_token_dim = deit_dim // tokens_deit  # e.g., 768//8 = 96\n",
    "        self.ft_token_dim = ft_dim // tokens_ft          # e.g., 192//8 = 24\n",
    "        \n",
    "        # Stage 1: Project tokens to a common embedding space.\n",
    "        self.deit_token_proj = nn.Linear(self.deit_token_dim, embed_dim)\n",
    "        self.ft_token_proj = nn.Linear(self.ft_token_dim, embed_dim)\n",
    "        \n",
    "        # Self-attention for within-modality fusion.\n",
    "        # We'll use nn.MultiheadAttention. It expects input shape (seq_len, batch, embed_dim).\n",
    "        self.deit_self_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=0.3)\n",
    "        self.ft_self_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=0.3)\n",
    "        \n",
    "        # Stage 1 aggregation: We will simply use mean pooling over tokens.\n",
    "        \n",
    "        # Stage 2: Fuse the two modality representations.\n",
    "        # Concatenate the two modality-level vectors to form a sequence of length 2.\n",
    "        # Use self-attention on this short sequence.\n",
    "        self.fusion_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=0.3)\n",
    "        \n",
    "        # Final classifier.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "             nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, deit_input, ft_input):\n",
    "        batch_size = deit_input.size(0)\n",
    "        # Stage 1: Within-modality attention.\n",
    "        # ---- For DeiT modality ----\n",
    "        # Reshape from (batch, deit_dim) to (batch, tokens_deit, deit_token_dim)\n",
    "        deit_tokens = deit_input.view(batch_size, self.tokens_deit, self.deit_token_dim)\n",
    "        # Project tokens: resulting shape (batch, tokens_deit, embed_dim)\n",
    "        deit_tokens_proj = self.deit_token_proj(deit_tokens)\n",
    "        # Transpose for multihead attention: (seq_len, batch, embed_dim)\n",
    "        deit_tokens_proj = deit_tokens_proj.transpose(0, 1)\n",
    "        # Apply self-attention.\n",
    "        deit_attn_output, _ = self.deit_self_attn(deit_tokens_proj, deit_tokens_proj, deit_tokens_proj)\n",
    "        # Aggregate tokens by mean pooling over sequence dimension.\n",
    "        deit_rep = deit_attn_output.mean(dim=0)  # (batch, embed_dim)\n",
    "        \n",
    "        # ---- For FT modality ----\n",
    "        ft_tokens = ft_input.view(batch_size, self.tokens_ft, self.ft_token_dim)\n",
    "        ft_tokens_proj = self.ft_token_proj(ft_tokens)  # (batch, tokens_ft, embed_dim)\n",
    "        ft_tokens_proj = ft_tokens_proj.transpose(0, 1)  # (seq_len, batch, embed_dim)\n",
    "        ft_attn_output, _ = self.ft_self_attn(ft_tokens_proj, ft_tokens_proj, ft_tokens_proj)\n",
    "        ft_rep = ft_attn_output.mean(dim=0)  # (batch, embed_dim)\n",
    "        \n",
    "        # Stage 2: Across-modality fusion.\n",
    "        # Stack the two modality representations to form a sequence of length 2.\n",
    "        # Shape: (batch, 2, embed_dim)\n",
    "        fused_seq = torch.stack([deit_rep, ft_rep], dim=1)\n",
    "        # Transpose to (seq_len, batch, embed_dim) for attention.\n",
    "        fused_seq = fused_seq.transpose(0, 1)\n",
    "        # Apply self-attention on the 2-token sequence.\n",
    "        fused_attn_output, _ = self.fusion_attn(fused_seq, fused_seq, fused_seq)\n",
    "        # Aggregate the fused tokens by mean pooling.\n",
    "        fused_rep = fused_attn_output.mean(dim=0)  # (batch, embed_dim)\n",
    "        \n",
    "        # Classify.\n",
    "        logits = self.classifier(fused_rep)\n",
    "        return logits\n",
    "\n",
    "# Set model parameters.\n",
    "deit_dim = deit_train.shape[1]  # e.g., 768\n",
    "ft_dim = ft_train.shape[1]      # e.g., 192\n",
    "tokens_deit = 8                # Ensure 768 % 8 == 0\n",
    "tokens_ft = 8                  # Ensure 192 % 8 == 0\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "\n",
    "model = HierarchicalAttentionFusionClassifier(deit_dim, ft_dim, tokens_deit, tokens_ft, embed_dim, num_heads, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "##########################################\n",
    "# 5. Train the Hierarchical Attention Fusion Classifier with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "num_epochs = 100\n",
    "patience = 5  # early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    for deit_x, ft_x, y in train_loader:\n",
    "        deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(deit_x, ft_x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item() * y.size(0)\n",
    "    total_train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Validate.\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    all_preds_val = []\n",
    "    all_targets_val = []\n",
    "    with torch.no_grad():\n",
    "        for deit_x, ft_x, y in val_loader:\n",
    "            deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "            logits = model(deit_x, ft_x)\n",
    "            loss = criterion(logits, y)\n",
    "            total_val_loss += loss.item() * y.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds_val.append(preds.cpu().numpy())\n",
    "            all_targets_val.append(y.cpu().numpy())\n",
    "    total_val_loss /= len(val_dataset)\n",
    "    all_preds_val = np.concatenate(all_preds_val)\n",
    "    all_targets_val = np.concatenate(all_targets_val)\n",
    "    val_acc = accuracy_score(all_targets_val, all_preds_val)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {total_train_loss:.4f}, Val Loss = {total_val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    if total_val_loss < best_val_loss:\n",
    "        best_val_loss = total_val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model state.\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for deit_x, ft_x, y in test_loader:\n",
    "        deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "        logits = model(deit_x, ft_x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", test_acc)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))\n",
    "\n",
    "##########################################\n",
    "# 7. Calculate the Margin of Error for Test Accuracy\n",
    "##########################################\n",
    "# Approach 1: Direct Calculation using the binomial formula.\n",
    "n_test = len(test_dataset)\n",
    "se = math.sqrt(test_acc * (1 - test_acc) / n_test)\n",
    "moe = 1.96 * se  # 95% confidence interval.\n",
    "print(\"\\nDirect Calculation:\")\n",
    "print(\"Standard Error:\", se)\n",
    "print(\"Margin of Error (95% CI):\", moe)\n",
    "print(\"95% Confidence Interval for Accuracy: [{:.4f}, {:.4f}]\".format(test_acc - moe, test_acc + moe))\n",
    "\n",
    "# Approach 2: Bootstrapping.\n",
    "n_bootstrap = 1000\n",
    "boot_acc = []\n",
    "n_samples = len(all_targets)\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_samples), n_samples, replace=True)\n",
    "    boot_accuracy = accuracy_score(all_targets[indices], all_preds[indices])\n",
    "    boot_acc.append(boot_accuracy)\n",
    "boot_acc = np.array(boot_acc)\n",
    "se_boot = np.std(boot_acc)\n",
    "moe_boot = 1.96 * se_boot\n",
    "print(\"\\nBootstrapping:\")\n",
    "print(\"Bootstrapped Mean Accuracy:\", np.mean(boot_acc))\n",
    "print(\"Standard Error (Bootstrap):\", se_boot)\n",
    "print(\"Margin of Error (Bootstrap, 95% CI):\", moe_boot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229ef30d-5abb-49e5-80d9-bd5151a1e465",
   "metadata": {},
   "source": [
    "## adaptive fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54096d45-c3a6-4afc-9bca-d4939a52a996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Epoch 1/100: Train Loss = 1.0397, Val Loss = 0.9580, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 2/100: Train Loss = 0.8931, Val Loss = 0.7839, Val Acc = 0.7645\n",
      "  -> New best model saved.\n",
      "Epoch 3/100: Train Loss = 0.7059, Val Loss = 0.5959, Val Acc = 0.8227\n",
      "  -> New best model saved.\n",
      "Epoch 4/100: Train Loss = 0.5374, Val Loss = 0.4595, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 5/100: Train Loss = 0.4237, Val Loss = 0.3949, Val Acc = 0.8663\n",
      "  -> New best model saved.\n",
      "Epoch 6/100: Train Loss = 0.3610, Val Loss = 0.3673, Val Acc = 0.8605\n",
      "  -> New best model saved.\n",
      "Epoch 7/100: Train Loss = 0.3480, Val Loss = 0.3554, Val Acc = 0.8605\n",
      "  -> New best model saved.\n",
      "Epoch 8/100: Train Loss = 0.3297, Val Loss = 0.3474, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 9/100: Train Loss = 0.3216, Val Loss = 0.3444, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 10/100: Train Loss = 0.3142, Val Loss = 0.3416, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 11/100: Train Loss = 0.2985, Val Loss = 0.3377, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 12/100: Train Loss = 0.3112, Val Loss = 0.3338, Val Acc = 0.8605\n",
      "  -> New best model saved.\n",
      "Epoch 13/100: Train Loss = 0.3019, Val Loss = 0.3314, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 14/100: Train Loss = 0.2969, Val Loss = 0.3345, Val Acc = 0.8517\n",
      "Epoch 15/100: Train Loss = 0.2971, Val Loss = 0.3284, Val Acc = 0.8663\n",
      "  -> New best model saved.\n",
      "Epoch 16/100: Train Loss = 0.2938, Val Loss = 0.3254, Val Acc = 0.8663\n",
      "  -> New best model saved.\n",
      "Epoch 17/100: Train Loss = 0.2892, Val Loss = 0.3218, Val Acc = 0.8663\n",
      "  -> New best model saved.\n",
      "Epoch 18/100: Train Loss = 0.2868, Val Loss = 0.3233, Val Acc = 0.8576\n",
      "Epoch 19/100: Train Loss = 0.2769, Val Loss = 0.3196, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 20/100: Train Loss = 0.2843, Val Loss = 0.3178, Val Acc = 0.8663\n",
      "  -> New best model saved.\n",
      "Epoch 21/100: Train Loss = 0.2784, Val Loss = 0.3172, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 22/100: Train Loss = 0.2765, Val Loss = 0.3212, Val Acc = 0.8663\n",
      "Epoch 23/100: Train Loss = 0.2732, Val Loss = 0.3134, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 24/100: Train Loss = 0.2663, Val Loss = 0.3106, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 25/100: Train Loss = 0.2651, Val Loss = 0.3088, Val Acc = 0.8663\n",
      "  -> New best model saved.\n",
      "Epoch 26/100: Train Loss = 0.2604, Val Loss = 0.3110, Val Acc = 0.8779\n",
      "Epoch 27/100: Train Loss = 0.2602, Val Loss = 0.3062, Val Acc = 0.8663\n",
      "  -> New best model saved.\n",
      "Epoch 28/100: Train Loss = 0.2675, Val Loss = 0.3043, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 29/100: Train Loss = 0.2575, Val Loss = 0.3069, Val Acc = 0.8663\n",
      "Epoch 30/100: Train Loss = 0.2451, Val Loss = 0.3001, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 31/100: Train Loss = 0.2492, Val Loss = 0.3036, Val Acc = 0.8663\n",
      "Epoch 32/100: Train Loss = 0.2409, Val Loss = 0.3042, Val Acc = 0.8721\n",
      "Epoch 33/100: Train Loss = 0.2525, Val Loss = 0.3007, Val Acc = 0.8779\n",
      "Epoch 34/100: Train Loss = 0.2344, Val Loss = 0.3114, Val Acc = 0.8837\n",
      "Epoch 35/100: Train Loss = 0.2295, Val Loss = 0.2970, Val Acc = 0.8895\n",
      "  -> New best model saved.\n",
      "Epoch 36/100: Train Loss = 0.2302, Val Loss = 0.2951, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 37/100: Train Loss = 0.2274, Val Loss = 0.2917, Val Acc = 0.8895\n",
      "  -> New best model saved.\n",
      "Epoch 38/100: Train Loss = 0.2216, Val Loss = 0.2891, Val Acc = 0.8895\n",
      "  -> New best model saved.\n",
      "Epoch 39/100: Train Loss = 0.2266, Val Loss = 0.2907, Val Acc = 0.8924\n",
      "Epoch 40/100: Train Loss = 0.2128, Val Loss = 0.2928, Val Acc = 0.8953\n",
      "Epoch 41/100: Train Loss = 0.2109, Val Loss = 0.2924, Val Acc = 0.8866\n",
      "Epoch 42/100: Train Loss = 0.2061, Val Loss = 0.2949, Val Acc = 0.8924\n",
      "Epoch 43/100: Train Loss = 0.2025, Val Loss = 0.2883, Val Acc = 0.8983\n",
      "  -> New best model saved.\n",
      "Epoch 44/100: Train Loss = 0.2029, Val Loss = 0.2975, Val Acc = 0.8808\n",
      "Epoch 45/100: Train Loss = 0.1992, Val Loss = 0.2911, Val Acc = 0.8953\n",
      "Epoch 46/100: Train Loss = 0.1908, Val Loss = 0.2859, Val Acc = 0.8983\n",
      "  -> New best model saved.\n",
      "Epoch 47/100: Train Loss = 0.1778, Val Loss = 0.2855, Val Acc = 0.8924\n",
      "  -> New best model saved.\n",
      "Epoch 48/100: Train Loss = 0.1902, Val Loss = 0.2948, Val Acc = 0.9012\n",
      "Epoch 49/100: Train Loss = 0.1837, Val Loss = 0.2913, Val Acc = 0.8866\n",
      "Epoch 50/100: Train Loss = 0.1714, Val Loss = 0.2850, Val Acc = 0.8983\n",
      "  -> New best model saved.\n",
      "Epoch 51/100: Train Loss = 0.1719, Val Loss = 0.2912, Val Acc = 0.8983\n",
      "Epoch 52/100: Train Loss = 0.1685, Val Loss = 0.2837, Val Acc = 0.8924\n",
      "  -> New best model saved.\n",
      "Epoch 53/100: Train Loss = 0.1586, Val Loss = 0.2864, Val Acc = 0.9041\n",
      "Epoch 54/100: Train Loss = 0.1528, Val Loss = 0.2905, Val Acc = 0.9012\n",
      "Epoch 55/100: Train Loss = 0.1501, Val Loss = 0.2795, Val Acc = 0.8983\n",
      "  -> New best model saved.\n",
      "Epoch 56/100: Train Loss = 0.1411, Val Loss = 0.2837, Val Acc = 0.9070\n",
      "Epoch 57/100: Train Loss = 0.1409, Val Loss = 0.3037, Val Acc = 0.9041\n",
      "Epoch 58/100: Train Loss = 0.1298, Val Loss = 0.2862, Val Acc = 0.8837\n",
      "Epoch 59/100: Train Loss = 0.1302, Val Loss = 0.2916, Val Acc = 0.8837\n",
      "Epoch 60/100: Train Loss = 0.1251, Val Loss = 0.2871, Val Acc = 0.8953\n",
      "Early stopping triggered.\n",
      "\n",
      "Test Accuracy: 0.8956521739130435\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.78      0.86      0.82        72\n",
      "          CN       0.97      0.95      0.96       106\n",
      "         MCI       0.91      0.87      0.89       167\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.88      0.90      0.89       345\n",
      "weighted avg       0.90      0.90      0.90       345\n",
      "\n",
      "\n",
      "Direct Calculation:\n",
      "Standard Error: 0.01645893944827488\n",
      "Margin of Error (95% CI): 0.03225952131861877\n",
      "95% Confidence Interval for Accuracy: [0.8634, 0.9279]\n",
      "\n",
      "Bootstrapping:\n",
      "Bootstrapped Mean Accuracy: 0.8961478260869564\n",
      "Standard Error (Bootstrap): 0.016415053209010693\n",
      "Margin of Error (Bootstrap, 95% CI): 0.03217350428966096\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import math\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Concatenate (but keep separate) Features\n",
    "##########################################\n",
    "# Load DeiT features (e.g., shape: [num_samples, 768])\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")\n",
    "\n",
    "# Load FTTransformer features (e.g., shape: [num_samples, 192])\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")\n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create PyTorch Datasets and DataLoaders\n",
    "##########################################\n",
    "# Each modality is kept separate so the network can process them independently.\n",
    "train_deit_tensor = torch.tensor(deit_train, dtype=torch.float32)\n",
    "train_ft_tensor   = torch.tensor(ft_train, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "val_deit_tensor = torch.tensor(deit_val, dtype=torch.float32)\n",
    "val_ft_tensor   = torch.tensor(ft_val, dtype=torch.float32)\n",
    "val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
    "\n",
    "test_deit_tensor = torch.tensor(deit_test, dtype=torch.float32)\n",
    "test_ft_tensor   = torch.tensor(ft_test, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets.\n",
    "train_dataset = TensorDataset(train_deit_tensor, train_ft_tensor, train_labels_tensor)\n",
    "val_dataset   = TensorDataset(val_deit_tensor, val_ft_tensor, val_labels_tensor)\n",
    "test_dataset  = TensorDataset(test_deit_tensor, test_ft_tensor, test_labels_tensor)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define the Dynamic/Adaptive Fusion Classifier with Attention Gating\n",
    "##########################################\n",
    "class DynamicFusionClassifier(nn.Module):\n",
    "    def __init__(self, deit_dim, ft_dim, embed_dim, num_classes):\n",
    "        \"\"\"\n",
    "        This network processes two modalities (DeiT and FTTransformer)\n",
    "        using two separate branches. Their outputs are projected to a common\n",
    "        embedding space. A small gating network computes a dynamic weight (alpha)\n",
    "        for each sample based on the concatenation of the two representations.\n",
    "        The final fused representation is computed as:\n",
    "        \n",
    "            fused = alpha * rep_deit + (1 - alpha) * rep_ft\n",
    "        \n",
    "        and then classified.\n",
    "        \"\"\"\n",
    "        super(DynamicFusionClassifier, self).__init__()\n",
    "        # Branch for DeiT modality.\n",
    "        self.deit_branch = nn.Sequential(\n",
    "            nn.Linear(deit_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        # Branch for FTTransformer modality.\n",
    "        self.ft_branch = nn.Sequential(\n",
    "            nn.Linear(ft_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        # Adaptive gating network that outputs a weight (alpha) in [0, 1].\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 2, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, 1),\n",
    "            nn.Sigmoid()  # ensures output is between 0 and 1\n",
    "        )\n",
    "        # Classifier that takes the fused representation.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, deit_input, ft_input):\n",
    "        # Process each modality through its branch.\n",
    "        rep_deit = self.deit_branch(deit_input)  # (batch_size, embed_dim)\n",
    "        rep_ft = self.ft_branch(ft_input)          # (batch_size, embed_dim)\n",
    "        \n",
    "        # Concatenate representations.\n",
    "        combined = torch.cat([rep_deit, rep_ft], dim=1)  # (batch_size, 2*embed_dim)\n",
    "        # Compute gating weight alpha for each sample.\n",
    "        alpha = self.gate(combined)  # (batch_size, 1)\n",
    "        \n",
    "        # Fuse modalities dynamically.\n",
    "        fused_rep = alpha * rep_deit + (1 - alpha) * rep_ft  # (batch_size, embed_dim)\n",
    "        \n",
    "        # Classify.\n",
    "        logits = self.classifier(fused_rep)\n",
    "        return logits\n",
    "\n",
    "# Define dimensions.\n",
    "# Assume DeiT features are 768 and FTTransformer features are 192.\n",
    "deit_dim = deit_train.shape[1]  # 768\n",
    "ft_dim = ft_train.shape[1]      # 192\n",
    "embed_dim = 256\n",
    "\n",
    "model = DynamicFusionClassifier(deit_dim, ft_dim, embed_dim, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "##########################################\n",
    "# 5. Train the Dynamic Fusion Classifier with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "num_epochs = 100\n",
    "patience = 5  # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for deit_x, ft_x, y in train_loader:\n",
    "        deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(deit_x, ft_x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * y.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds_val = []\n",
    "    all_targets_val = []\n",
    "    with torch.no_grad():\n",
    "        for deit_x, ft_x, y in val_loader:\n",
    "            deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "            logits = model(deit_x, ft_x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * y.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds_val.append(preds.cpu().numpy())\n",
    "            all_targets_val.append(y.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds_val = np.concatenate(all_preds_val)\n",
    "    all_targets_val = np.concatenate(all_targets_val)\n",
    "    val_acc = accuracy_score(all_targets_val, all_preds_val)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model state.\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for deit_x, ft_x, y in test_loader:\n",
    "        deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "        logits = model(deit_x, ft_x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", test_acc)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))\n",
    "\n",
    "##########################################\n",
    "# 7. Calculate the Margin of Error for Test Accuracy\n",
    "##########################################\n",
    "# Approach 1: Direct Calculation using the binomial formula.\n",
    "n_test = len(test_dataset)\n",
    "se = math.sqrt(test_acc * (1 - test_acc) / n_test)\n",
    "moe = 1.96 * se  # For a 95% confidence interval.\n",
    "print(\"\\nDirect Calculation:\")\n",
    "print(\"Standard Error:\", se)\n",
    "print(\"Margin of Error (95% CI):\", moe)\n",
    "print(\"95% Confidence Interval for Accuracy: [{:.4f}, {:.4f}]\".format(test_acc - moe, test_acc + moe))\n",
    "\n",
    "# Approach 2: Bootstrapping.\n",
    "n_bootstrap = 1000\n",
    "boot_acc = []\n",
    "n_samples = len(all_targets)\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_samples), n_samples, replace=True)\n",
    "    boot_accuracy = accuracy_score(all_targets[indices], all_preds[indices])\n",
    "    boot_acc.append(boot_accuracy)\n",
    "boot_acc = np.array(boot_acc)\n",
    "se_boot = np.std(boot_acc)\n",
    "moe_boot = 1.96 * se_boot\n",
    "print(\"\\nBootstrapping:\")\n",
    "print(\"Bootstrapped Mean Accuracy:\", np.mean(boot_acc))\n",
    "print(\"Standard Error (Bootstrap):\", se_boot)\n",
    "print(\"Margin of Error (Bootstrap, 95% CI):\", moe_boot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2f044-6504-4411-b07a-8b86a8cf3b1e",
   "metadata": {},
   "source": [
    "## graph neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d03b5c4-44c6-416b-b1e6-8a29c6908442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-geometric\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "Requirement already satisfied: aiohttp in ./multimodal_env/lib/python3.8/site-packages (from torch-geometric) (3.10.11)\n",
      "Requirement already satisfied: fsspec in ./multimodal_env/lib/python3.8/site-packages (from torch-geometric) (2024.12.0)\n",
      "Requirement already satisfied: jinja2 in ./multimodal_env/lib/python3.8/site-packages (from torch-geometric) (3.1.5)\n",
      "Requirement already satisfied: numpy in ./multimodal_env/lib/python3.8/site-packages (from torch-geometric) (1.24.3)\n",
      "Requirement already satisfied: psutil>=5.8.0 in ./multimodal_env/lib/python3.8/site-packages (from torch-geometric) (6.1.1)\n",
      "Requirement already satisfied: pyparsing in ./multimodal_env/lib/python3.8/site-packages (from torch-geometric) (3.1.4)\n",
      "Requirement already satisfied: requests in ./multimodal_env/lib/python3.8/site-packages (from torch-geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in ./multimodal_env/lib/python3.8/site-packages (from torch-geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./multimodal_env/lib/python3.8/site-packages (from aiohttp->torch-geometric) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./multimodal_env/lib/python3.8/site-packages (from aiohttp->torch-geometric) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./multimodal_env/lib/python3.8/site-packages (from aiohttp->torch-geometric) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./multimodal_env/lib/python3.8/site-packages (from aiohttp->torch-geometric) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./multimodal_env/lib/python3.8/site-packages (from aiohttp->torch-geometric) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./multimodal_env/lib/python3.8/site-packages (from aiohttp->torch-geometric) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./multimodal_env/lib/python3.8/site-packages (from aiohttp->torch-geometric) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./multimodal_env/lib/python3.8/site-packages (from jinja2->torch-geometric) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./multimodal_env/lib/python3.8/site-packages (from requests->torch-geometric) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./multimodal_env/lib/python3.8/site-packages (from requests->torch-geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./multimodal_env/lib/python3.8/site-packages (from requests->torch-geometric) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./multimodal_env/lib/python3.8/site-packages (from requests->torch-geometric) (2024.12.14)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in ./multimodal_env/lib/python3.8/site-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./multimodal_env/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp->torch-geometric) (0.2.0)\n",
      "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: torch-geometric\n",
      "Successfully installed torch-geometric-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26b48dc6-c245-4be8-924b-9319868f285b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Epoch 1/100: Train Loss = 1.0571, Val Loss = 1.0447, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 2/100: Train Loss = 1.0544, Val Loss = 1.0385, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 3/100: Train Loss = 1.0474, Val Loss = 1.0315, Val Acc = 0.4884\n",
      "  -> New best model saved.\n",
      "Epoch 4/100: Train Loss = 1.0454, Val Loss = 1.0286, Val Acc = 0.4884\n",
      "  -> New best model saved.\n",
      "Epoch 5/100: Train Loss = 1.0445, Val Loss = 1.0368, Val Acc = 0.4913\n",
      "Epoch 6/100: Train Loss = 1.0422, Val Loss = 1.0331, Val Acc = 0.4855\n",
      "Epoch 7/100: Train Loss = 1.0469, Val Loss = 1.0331, Val Acc = 0.4826\n",
      "Epoch 8/100: Train Loss = 1.0397, Val Loss = 1.0257, Val Acc = 0.4913\n",
      "  -> New best model saved.\n",
      "Epoch 9/100: Train Loss = 1.0481, Val Loss = 1.0300, Val Acc = 0.4855\n",
      "Epoch 10/100: Train Loss = 1.0430, Val Loss = 1.0288, Val Acc = 0.4855\n",
      "Epoch 11/100: Train Loss = 1.0406, Val Loss = 1.0255, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 12/100: Train Loss = 1.0501, Val Loss = 1.0297, Val Acc = 0.4855\n",
      "Epoch 13/100: Train Loss = 1.0461, Val Loss = 1.0312, Val Acc = 0.4855\n",
      "Epoch 14/100: Train Loss = 1.0496, Val Loss = 1.0297, Val Acc = 0.4855\n",
      "Epoch 15/100: Train Loss = 1.0521, Val Loss = 1.0311, Val Acc = 0.4855\n",
      "Epoch 16/100: Train Loss = 1.0487, Val Loss = 1.0351, Val Acc = 0.4855\n",
      "Early stopping triggered.\n",
      "\n",
      "Test Accuracy: 0.48405797101449277\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.00      0.00      0.00        72\n",
      "          CN       0.00      0.00      0.00       106\n",
      "         MCI       0.48      1.00      0.65       167\n",
      "\n",
      "    accuracy                           0.48       345\n",
      "   macro avg       0.16      0.33      0.22       345\n",
      "weighted avg       0.23      0.48      0.32       345\n",
      "\n",
      "\n",
      "Direct Calculation:\n",
      "Standard Error: 0.02690540873977881\n",
      "Margin of Error (95% CI): 0.05273460112996647\n",
      "95% Confidence Interval for Accuracy: [0.4313, 0.5368]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bootstrapping:\n",
      "Bootstrapped Mean Accuracy: 0.48295072463768124\n",
      "Standard Error (Bootstrap): 0.027270121629934207\n",
      "Margin of Error (Bootstrap, 95% CI): 0.05344943839467105\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import math\n",
    "\n",
    "# Import modules from PyTorch Geometric.\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Prepare Features for Each Modality\n",
    "##########################################\n",
    "# Load DeiT features (e.g., shape: [num_samples, 768])\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")\n",
    "\n",
    "# Load FTTransformer features (e.g., shape: [num_samples, 192])\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")\n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create PyTorch Datasets and DataLoaders\n",
    "##########################################\n",
    "# We keep the two modalities separate so that the GAT module can fuse them.\n",
    "train_deit_tensor = torch.tensor(deit_train, dtype=torch.float32)\n",
    "train_ft_tensor   = torch.tensor(ft_train, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "val_deit_tensor = torch.tensor(deit_val, dtype=torch.float32)\n",
    "val_ft_tensor   = torch.tensor(ft_val, dtype=torch.float32)\n",
    "val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
    "\n",
    "test_deit_tensor = torch.tensor(deit_test, dtype=torch.float32)\n",
    "test_ft_tensor   = torch.tensor(ft_test, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(train_deit_tensor, train_ft_tensor, train_labels_tensor)\n",
    "val_dataset   = TensorDataset(val_deit_tensor, val_ft_tensor, val_labels_tensor)\n",
    "test_dataset  = TensorDataset(test_deit_tensor, test_ft_tensor, test_labels_tensor)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define the Graph Attention Fusion Classifier\n",
    "##########################################\n",
    "class GraphAttentionFusionClassifier(nn.Module):\n",
    "    def __init__(self, deit_dim, ft_dim, embed_dim, num_classes):\n",
    "        \"\"\"\n",
    "        For each sample, we build a small graph with two nodes:\n",
    "          - Node 0: projected DeiT features.\n",
    "          - Node 1: projected FTTransformer features.\n",
    "        Edges (bidirectional) connect the two nodes.\n",
    "        A GAT layer computes new node representations by attending over their neighbors.\n",
    "        The node features are then pooled (mean pooling) to produce a fused representation.\n",
    "        \"\"\"\n",
    "        super(GraphAttentionFusionClassifier, self).__init__()\n",
    "        # Projection layers to a common embedding space.\n",
    "        self.deit_proj = nn.Linear(deit_dim, embed_dim)\n",
    "        self.ft_proj = nn.Linear(ft_dim, embed_dim)\n",
    "        \n",
    "        # Graph Attention layer.\n",
    "        # Here, we use a single-head attention layer for simplicity.\n",
    "        self.gat = GATConv(embed_dim, embed_dim, heads=1, concat=False, dropout=0.3)\n",
    "        \n",
    "        # Final classifier.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, deit_input, ft_input):\n",
    "        batch_size = deit_input.size(0)\n",
    "        # Project each modality.\n",
    "        rep_deit = self.deit_proj(deit_input)  # (B, embed_dim)\n",
    "        rep_ft = self.ft_proj(ft_input)          # (B, embed_dim)\n",
    "        \n",
    "        # For each sample, create a graph with 2 nodes.\n",
    "        # Stack representations for each sample.\n",
    "        # x: (B*2, embed_dim) where for sample i:\n",
    "        #   Node 0 index = 2*i => rep_deit[i]\n",
    "        #   Node 1 index = 2*i + 1 => rep_ft[i]\n",
    "        x = torch.cat([rep_deit, rep_ft], dim=0)\n",
    "        \n",
    "        # Build edge_index for each sample.\n",
    "        # Each sample's graph will have 2 edges: from node0 to node1 and vice versa.\n",
    "        edge_index_list = []\n",
    "        for i in range(batch_size):\n",
    "            node0 = 2 * i\n",
    "            node1 = 2 * i + 1\n",
    "            # Add both directions.\n",
    "            edge_index_list.append([node0, node1])\n",
    "            edge_index_list.append([node1, node0])\n",
    "        edge_index = torch.tensor(edge_index_list, dtype=torch.long, device=x.device).t().contiguous()\n",
    "        # edge_index shape: (2, batch_size*2)\n",
    "        \n",
    "        # Create a batch vector indicating graph membership.\n",
    "        # For each sample, we assign the same batch index to its two nodes.\n",
    "        batch_vector = torch.arange(batch_size, device=x.device).repeat_interleave(2)\n",
    "        \n",
    "        # Apply the GAT layer.\n",
    "        gat_out = self.gat(x, edge_index)  # (B*2, embed_dim)\n",
    "        \n",
    "        # Pool node features for each sample (mean pooling).\n",
    "        # Use global mean pooling from torch_geometric.\n",
    "        fused_rep = global_mean_pool(gat_out, batch_vector)  # (B, embed_dim)\n",
    "        \n",
    "        # Classify.\n",
    "        logits = self.classifier(fused_rep)\n",
    "        return logits\n",
    "\n",
    "# Define dimensions.\n",
    "deit_dim = deit_train.shape[1]  # e.g., 768\n",
    "ft_dim = ft_train.shape[1]      # e.g., 192\n",
    "embed_dim = 512\n",
    "\n",
    "model = GraphAttentionFusionClassifier(deit_dim, ft_dim, embed_dim, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "##########################################\n",
    "# 5. Train the Graph Attention Fusion Classifier with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "num_epochs = 100\n",
    "patience = 5  # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    for deit_x, ft_x, y in train_loader:\n",
    "        deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(deit_x, ft_x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item() * y.size(0)\n",
    "    total_train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Validation phase.\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    all_preds_val = []\n",
    "    all_targets_val = []\n",
    "    with torch.no_grad():\n",
    "        for deit_x, ft_x, y in val_loader:\n",
    "            deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "            logits = model(deit_x, ft_x)\n",
    "            loss = criterion(logits, y)\n",
    "            total_val_loss += loss.item() * y.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds_val.append(preds.cpu().numpy())\n",
    "            all_targets_val.append(y.cpu().numpy())\n",
    "    total_val_loss /= len(val_dataset)\n",
    "    all_preds_val = np.concatenate(all_preds_val)\n",
    "    all_targets_val = np.concatenate(all_targets_val)\n",
    "    val_acc = accuracy_score(all_targets_val, all_preds_val)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {total_train_loss:.4f}, Val Loss = {total_val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    if total_val_loss < best_val_loss:\n",
    "        best_val_loss = total_val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best saved model.\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for deit_x, ft_x, y in test_loader:\n",
    "        deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "        logits = model(deit_x, ft_x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", test_acc)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))\n",
    "\n",
    "##########################################\n",
    "# 7. Calculate the Margin of Error for Test Accuracy\n",
    "##########################################\n",
    "# Approach 1: Direct Calculation using the binomial formula.\n",
    "n_test = len(test_dataset)\n",
    "se = math.sqrt(test_acc * (1 - test_acc) / n_test)\n",
    "moe = 1.96 * se  # For a 95% confidence interval.\n",
    "print(\"\\nDirect Calculation:\")\n",
    "print(\"Standard Error:\", se)\n",
    "print(\"Margin of Error (95% CI):\", moe)\n",
    "print(\"95% Confidence Interval for Accuracy: [{:.4f}, {:.4f}]\".format(test_acc - moe, test_acc + moe))\n",
    "\n",
    "# Approach 2: Bootstrapping.\n",
    "n_bootstrap = 1000\n",
    "boot_acc = []\n",
    "n_samples = len(all_targets)\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_samples), n_samples, replace=True)\n",
    "    boot_accuracy = accuracy_score(all_targets[indices], all_preds[indices])\n",
    "    boot_acc.append(boot_accuracy)\n",
    "boot_acc = np.array(boot_acc)\n",
    "se_boot = np.std(boot_acc)\n",
    "moe_boot = 1.96 * se_boot\n",
    "print(\"\\nBootstrapping:\")\n",
    "print(\"Bootstrapped Mean Accuracy:\", np.mean(boot_acc))\n",
    "print(\"Standard Error (Bootstrap):\", se_boot)\n",
    "print(\"Margin of Error (Bootstrap, 95% CI):\", moe_boot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "189557fe-7990-40f6-831a-90a877c38274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Epoch 1/100: Train Loss = 0.9835, Val Loss = 0.6793, Val Acc = 0.7733\n",
      "  -> New best model saved.\n",
      "Epoch 2/100: Train Loss = 0.5262, Val Loss = 0.3796, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 3/100: Train Loss = 0.3744, Val Loss = 0.3390, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 4/100: Train Loss = 0.3410, Val Loss = 0.3388, Val Acc = 0.8459\n",
      "  -> New best model saved.\n",
      "Epoch 5/100: Train Loss = 0.3362, Val Loss = 0.3445, Val Acc = 0.8430\n",
      "Epoch 6/100: Train Loss = 0.3207, Val Loss = 0.3386, Val Acc = 0.8605\n",
      "  -> New best model saved.\n",
      "Epoch 7/100: Train Loss = 0.3239, Val Loss = 0.3454, Val Acc = 0.8430\n",
      "Epoch 8/100: Train Loss = 0.3178, Val Loss = 0.3301, Val Acc = 0.8663\n",
      "  -> New best model saved.\n",
      "Epoch 9/100: Train Loss = 0.3208, Val Loss = 0.3339, Val Acc = 0.8517\n",
      "Epoch 10/100: Train Loss = 0.3211, Val Loss = 0.3288, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 11/100: Train Loss = 0.3061, Val Loss = 0.3283, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 12/100: Train Loss = 0.3098, Val Loss = 0.3261, Val Acc = 0.8634\n",
      "  -> New best model saved.\n",
      "Epoch 13/100: Train Loss = 0.3060, Val Loss = 0.3224, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 14/100: Train Loss = 0.3027, Val Loss = 0.3178, Val Acc = 0.8634\n",
      "  -> New best model saved.\n",
      "Epoch 15/100: Train Loss = 0.3017, Val Loss = 0.3175, Val Acc = 0.8605\n",
      "  -> New best model saved.\n",
      "Epoch 16/100: Train Loss = 0.2940, Val Loss = 0.3101, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 17/100: Train Loss = 0.2905, Val Loss = 0.3088, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 18/100: Train Loss = 0.2851, Val Loss = 0.3108, Val Acc = 0.8663\n",
      "Epoch 19/100: Train Loss = 0.2770, Val Loss = 0.3081, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 20/100: Train Loss = 0.2770, Val Loss = 0.2962, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 21/100: Train Loss = 0.2683, Val Loss = 0.2948, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 22/100: Train Loss = 0.2683, Val Loss = 0.2875, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 23/100: Train Loss = 0.2577, Val Loss = 0.2929, Val Acc = 0.8779\n",
      "Epoch 24/100: Train Loss = 0.2456, Val Loss = 0.2940, Val Acc = 0.8808\n",
      "Epoch 25/100: Train Loss = 0.2570, Val Loss = 0.2902, Val Acc = 0.8808\n",
      "Epoch 26/100: Train Loss = 0.2424, Val Loss = 0.3004, Val Acc = 0.8924\n",
      "Epoch 27/100: Train Loss = 0.2316, Val Loss = 0.2929, Val Acc = 0.8895\n",
      "Epoch 28/100: Train Loss = 0.2326, Val Loss = 0.2951, Val Acc = 0.8808\n",
      "Epoch 29/100: Train Loss = 0.2199, Val Loss = 0.3085, Val Acc = 0.8837\n",
      "Epoch 30/100: Train Loss = 0.2266, Val Loss = 0.2957, Val Acc = 0.8895\n",
      "Epoch 31/100: Train Loss = 0.2161, Val Loss = 0.2960, Val Acc = 0.8895\n",
      "Epoch 32/100: Train Loss = 0.2169, Val Loss = 0.2929, Val Acc = 0.8866\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 0.9043478260869565\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.81      0.83      0.82        72\n",
      "          CN       0.97      0.96      0.97       106\n",
      "         MCI       0.90      0.90      0.90       167\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.90      0.90      0.90       345\n",
      "weighted avg       0.91      0.90      0.90       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "##########################################\n",
    "# Set device.\n",
    "##########################################\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load Features Separately (No Concatenation)\n",
    "##########################################\n",
    "# Load DeiT features.\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")\n",
    "\n",
    "# Load FTTransformer features.\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")\n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "# Read the CSV files \n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "\n",
    "# Encode labels.\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create PyTorch Datasets and DataLoaders\n",
    "##########################################\n",
    "# Convert features and labels to tensors.\n",
    "train_deit = torch.tensor(deit_train, dtype=torch.float32)\n",
    "train_ft   = torch.tensor(ft_train, dtype=torch.float32)\n",
    "val_deit   = torch.tensor(deit_val, dtype=torch.float32)\n",
    "val_ft     = torch.tensor(ft_val, dtype=torch.float32)\n",
    "test_deit  = torch.tensor(deit_test, dtype=torch.float32)\n",
    "test_ft    = torch.tensor(ft_test, dtype=torch.float32)\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels   = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_labels  = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create datasets that yield both modalities along with labels.\n",
    "train_dataset = TensorDataset(train_deit, train_ft, train_labels)\n",
    "val_dataset   = TensorDataset(val_deit, val_ft, val_labels)\n",
    "test_dataset  = TensorDataset(test_deit, test_ft, test_labels)\n",
    "\n",
    "# DataLoaders.\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define a Cross-Modal Attention Fusion Model\n",
    "##########################################\n",
    "class CrossModalAttentionFusion(nn.Module):\n",
    "    def __init__(self, deit_dim, ft_dim, d_model, num_classes, num_heads=4):\n",
    "        \"\"\"\n",
    "        deit_dim: Dimension of DeiT features.\n",
    "        ft_dim: Dimension of FTTransformer features.\n",
    "        d_model: Common projection dimension.\n",
    "        num_classes: Number of output classes.\n",
    "        num_heads: Number of attention heads.\n",
    "        \"\"\"\n",
    "        super(CrossModalAttentionFusion, self).__init__()\n",
    "        # Project each modality to the common embedding space.\n",
    "        self.deit_proj = nn.Linear(deit_dim, d_model)\n",
    "        self.ft_proj   = nn.Linear(ft_dim, d_model)\n",
    "        \n",
    "        # Cross-attention layers.\n",
    "        # 1. Updated DeiT representation when attending to FT features.\n",
    "        self.cross_attn_deit = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)\n",
    "        # 2. Updated FT representation when attending to DeiT features.\n",
    "        self.cross_attn_ft = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)\n",
    "        \n",
    "        # Final classifier MLP. Since we fuse both modalities via concatenation,\n",
    "        # the classifier input dimension is 2*d_model.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2 * d_model, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, deit_feat, ft_feat):\n",
    "        \"\"\"\n",
    "        deit_feat: Tensor of shape (batch_size, deit_dim)\n",
    "        ft_feat: Tensor of shape (batch_size, ft_dim)\n",
    "        \"\"\"\n",
    "        # Project features into the common space.\n",
    "        proj_deit = self.deit_proj(deit_feat)  # (batch_size, d_model)\n",
    "        proj_ft   = self.ft_proj(ft_feat)        # (batch_size, d_model)\n",
    "        \n",
    "        # Introduce a sequence dimension (each modality becomes a token).\n",
    "        proj_deit = proj_deit.unsqueeze(1)  # (batch_size, 1, d_model)\n",
    "        proj_ft   = proj_ft.unsqueeze(1)    # (batch_size, 1, d_model)\n",
    "        \n",
    "        # Each modality attends to the other.\n",
    "        # Updated DeiT features: query from DeiT with key/value from FT.\n",
    "        updated_deit, _ = self.cross_attn_deit(query=proj_deit, key=proj_ft, value=proj_ft)\n",
    "        # Updated FT features: query from FT with key/value from DeiT.\n",
    "        updated_ft, _   = self.cross_attn_ft(query=proj_ft, key=proj_deit, value=proj_deit)\n",
    "        \n",
    "        # Remove the sequence dimension.\n",
    "        updated_deit = updated_deit.squeeze(1)  # (batch_size, d_model)\n",
    "        updated_ft   = updated_ft.squeeze(1)      # (batch_size, d_model)\n",
    "        \n",
    "        # Fuse the updated representations (e.g., via concatenation).\n",
    "        fused_rep = torch.cat([updated_deit, updated_ft], dim=-1)  # (batch_size, 2*d_model)\n",
    "        \n",
    "        # Classify the fused representation.\n",
    "        logits = self.classifier(fused_rep)\n",
    "        return logits\n",
    "\n",
    "##########################################\n",
    "# 5. Create the Model Instance\n",
    "##########################################\n",
    "# Set projection dimension (d_model). You may adjust this as needed.\n",
    "d_model = 512\n",
    "deit_dim = deit_train.shape[1]  # Input dimension for DeiT features.\n",
    "ft_dim   = ft_train.shape[1]    # Input dimension for FTTransformer features.\n",
    "\n",
    "model = CrossModalAttentionFusion(deit_dim, ft_dim, d_model, num_classes, num_heads=4)\n",
    "model.to(device)\n",
    "\n",
    "##########################################\n",
    "# 6. Train the Model with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "num_epochs = 100\n",
    "patience = 10  # Early stopping patience.\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for deit_x, ft_x, y in train_loader:\n",
    "        deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(deit_x, ft_x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * y.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Validation.\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for deit_x, ft_x, y in val_loader:\n",
    "            deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "            logits = model(deit_x, ft_x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * y.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    val_acc = accuracy_score(all_targets, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping.\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best saved model state.\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 7. Evaluate on the Test Set\n",
    "##########################################\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for deit_x, ft_x, y in test_loader:\n",
    "        deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "        logits = model(deit_x, ft_x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "        \n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30b5f554-0561-4583-9747-6faac08ba2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Epoch 1/100: Train Loss = 0.9828, Val Loss = 0.8065, Val Acc = 0.7733\n",
      "  -> New best model saved.\n",
      "Epoch 2/100: Train Loss = 0.6277, Val Loss = 0.4537, Val Acc = 0.7791\n",
      "  -> New best model saved.\n",
      "Epoch 3/100: Train Loss = 0.4035, Val Loss = 0.3475, Val Acc = 0.8663\n",
      "  -> New best model saved.\n",
      "Epoch 4/100: Train Loss = 0.3417, Val Loss = 0.3348, Val Acc = 0.8634\n",
      "  -> New best model saved.\n",
      "Epoch 5/100: Train Loss = 0.3216, Val Loss = 0.3324, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 6/100: Train Loss = 0.3155, Val Loss = 0.3341, Val Acc = 0.8634\n",
      "Epoch 7/100: Train Loss = 0.3132, Val Loss = 0.3343, Val Acc = 0.8547\n",
      "Epoch 8/100: Train Loss = 0.3028, Val Loss = 0.3301, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 9/100: Train Loss = 0.3068, Val Loss = 0.3415, Val Acc = 0.8430\n",
      "Epoch 10/100: Train Loss = 0.3110, Val Loss = 0.3301, Val Acc = 0.8605\n",
      "Epoch 11/100: Train Loss = 0.2935, Val Loss = 0.3253, Val Acc = 0.8634\n",
      "  -> New best model saved.\n",
      "Epoch 12/100: Train Loss = 0.3007, Val Loss = 0.3243, Val Acc = 0.8605\n",
      "  -> New best model saved.\n",
      "Epoch 13/100: Train Loss = 0.2962, Val Loss = 0.3258, Val Acc = 0.8605\n",
      "Epoch 14/100: Train Loss = 0.2903, Val Loss = 0.3218, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 15/100: Train Loss = 0.3041, Val Loss = 0.3230, Val Acc = 0.8663\n",
      "Epoch 16/100: Train Loss = 0.2933, Val Loss = 0.3198, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 17/100: Train Loss = 0.3006, Val Loss = 0.3177, Val Acc = 0.8634\n",
      "  -> New best model saved.\n",
      "Epoch 18/100: Train Loss = 0.2937, Val Loss = 0.3162, Val Acc = 0.8663\n",
      "  -> New best model saved.\n",
      "Epoch 19/100: Train Loss = 0.2978, Val Loss = 0.3139, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 20/100: Train Loss = 0.2952, Val Loss = 0.3122, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 21/100: Train Loss = 0.2882, Val Loss = 0.3123, Val Acc = 0.8721\n",
      "Epoch 22/100: Train Loss = 0.2822, Val Loss = 0.3117, Val Acc = 0.8663\n",
      "  -> New best model saved.\n",
      "Epoch 23/100: Train Loss = 0.2820, Val Loss = 0.3066, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 24/100: Train Loss = 0.2817, Val Loss = 0.3042, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 25/100: Train Loss = 0.2745, Val Loss = 0.3047, Val Acc = 0.8808\n",
      "Epoch 26/100: Train Loss = 0.2717, Val Loss = 0.3010, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 27/100: Train Loss = 0.2673, Val Loss = 0.3027, Val Acc = 0.8721\n",
      "Epoch 28/100: Train Loss = 0.2698, Val Loss = 0.3017, Val Acc = 0.8750\n",
      "Epoch 29/100: Train Loss = 0.2684, Val Loss = 0.2981, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 30/100: Train Loss = 0.2565, Val Loss = 0.2994, Val Acc = 0.8779\n",
      "Epoch 31/100: Train Loss = 0.2603, Val Loss = 0.2979, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 32/100: Train Loss = 0.2597, Val Loss = 0.3055, Val Acc = 0.8750\n",
      "Epoch 33/100: Train Loss = 0.2459, Val Loss = 0.2957, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 34/100: Train Loss = 0.2560, Val Loss = 0.2910, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 35/100: Train Loss = 0.2524, Val Loss = 0.2916, Val Acc = 0.8721\n",
      "Epoch 36/100: Train Loss = 0.2472, Val Loss = 0.2952, Val Acc = 0.8837\n",
      "Epoch 37/100: Train Loss = 0.2434, Val Loss = 0.2921, Val Acc = 0.8779\n",
      "Epoch 38/100: Train Loss = 0.2377, Val Loss = 0.2922, Val Acc = 0.8837\n",
      "Epoch 39/100: Train Loss = 0.2351, Val Loss = 0.3024, Val Acc = 0.8750\n",
      "Epoch 40/100: Train Loss = 0.2285, Val Loss = 0.2928, Val Acc = 0.8837\n",
      "Epoch 41/100: Train Loss = 0.2338, Val Loss = 0.2969, Val Acc = 0.8866\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 0.9101449275362319\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.88      0.78      0.82        72\n",
      "          CN       0.98      0.95      0.97       106\n",
      "         MCI       0.88      0.94      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.91      0.89      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load Features Separately (for Cross-Modal Fusion)\n",
    "##########################################\n",
    "# Load DeiT features.\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")  \n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")      \n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")     \n",
    "\n",
    "# Load FTTransformer features.\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")        \n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")          \n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")         \n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "# Read the CSV files.\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "\n",
    "# Encode labels using LabelEncoder.\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create PyTorch Datasets and DataLoaders\n",
    "##########################################\n",
    "# Convert features and labels to tensors (keeping modalities separate).\n",
    "train_deit = torch.tensor(deit_train, dtype=torch.float32)\n",
    "train_ft   = torch.tensor(ft_train, dtype=torch.float32)\n",
    "val_deit   = torch.tensor(deit_val, dtype=torch.float32)\n",
    "val_ft     = torch.tensor(ft_val, dtype=torch.float32)\n",
    "test_deit  = torch.tensor(deit_test, dtype=torch.float32)\n",
    "test_ft    = torch.tensor(ft_test, dtype=torch.float32)\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels   = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_labels  = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets that return (DeiT_features, FT_features, label).\n",
    "train_dataset = TensorDataset(train_deit, train_ft, train_labels)\n",
    "val_dataset   = TensorDataset(val_deit, val_ft, val_labels)\n",
    "test_dataset  = TensorDataset(test_deit, test_ft, test_labels)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define a Cross-Modal Attention Fusion Model with 2 Hidden Layers in the Classifier\n",
    "##########################################\n",
    "class CrossModalFusion(nn.Module):\n",
    "    def __init__(self, deit_dim, ft_dim, d_model, num_classes, num_heads=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          deit_dim: Dimension of DeiT features.\n",
    "          ft_dim: Dimension of FTTransformer features.\n",
    "          d_model: Common projection dimension.\n",
    "          num_classes: Number of target classes.\n",
    "          num_heads: Number of attention heads.\n",
    "        \"\"\"\n",
    "        super(CrossModalFusion, self).__init__()\n",
    "        # Project each modality to the same dimension.\n",
    "        self.deit_proj = nn.Linear(deit_dim, d_model)\n",
    "        self.ft_proj   = nn.Linear(ft_dim, d_model)\n",
    "        \n",
    "        # Bidirectional Cross-Attention Modules.\n",
    "        # 1. Updated DeiT representation by attending to FT features.\n",
    "        self.cross_attn_deit = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)\n",
    "        # 2. Updated FT representation by attending to DeiT features.\n",
    "        self.cross_attn_ft   = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)\n",
    "        \n",
    "        # MLP Classifier using 2 hidden layers.\n",
    "        # The fused representation will be of size (2 * d_model) after concatenation.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2 * d_model, 512),   # First hidden layer.\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128),           # Second hidden layer.\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)    # Output layer.\n",
    "        )\n",
    "        \n",
    "    def forward(self, deit_feat, ft_feat):\n",
    "        # Project the features.\n",
    "        proj_deit = self.deit_proj(deit_feat)  # Shape: (batch_size, d_model)\n",
    "        proj_ft   = self.ft_proj(ft_feat)        # Shape: (batch_size, d_model)\n",
    "        \n",
    "        # Add a sequence dimension (each modality becomes a token).\n",
    "        proj_deit = proj_deit.unsqueeze(1)  # (batch_size, 1, d_model)\n",
    "        proj_ft   = proj_ft.unsqueeze(1)    # (batch_size, 1, d_model)\n",
    "        \n",
    "        # Cross-Attention:\n",
    "        # (a) DeiT attends to FT.\n",
    "        updated_deit, _ = self.cross_attn_deit(query=proj_deit, key=proj_ft, value=proj_ft)\n",
    "        # (b) FT attends to DeiT.\n",
    "        updated_ft, _   = self.cross_attn_ft(query=proj_ft, key=proj_deit, value=proj_deit)\n",
    "        \n",
    "        # Remove the sequence dimension.\n",
    "        updated_deit = updated_deit.squeeze(1)  # (batch_size, d_model)\n",
    "        updated_ft   = updated_ft.squeeze(1)      # (batch_size, d_model)\n",
    "        \n",
    "        # Fuse the modalities by concatenation.\n",
    "        fused_rep = torch.cat([updated_deit, updated_ft], dim=-1)  # (batch_size, 2 * d_model)\n",
    "        \n",
    "        # Pass the fused representation through the classifier.\n",
    "        logits = self.classifier(fused_rep)\n",
    "        return logits\n",
    "\n",
    "# Determine dimensions.\n",
    "deit_dim = deit_train.shape[1]  # Dimension of DeiT features.\n",
    "ft_dim   = ft_train.shape[1]    # Dimension of FTTransformer features.\n",
    "d_model  = 512                  # Common projection dimension.\n",
    "\n",
    "# Create the model.\n",
    "model = CrossModalFusion(deit_dim, ft_dim, d_model, num_classes, num_heads=4)\n",
    "model.to(device)\n",
    "\n",
    "##########################################\n",
    "# 5. Train the Model with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "num_epochs = 100\n",
    "patience = 7\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for deit_x, ft_x, y in train_loader:\n",
    "        deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(deit_x, ft_x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * y.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Evaluate on the validation set.\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for deit_x, ft_x, y in val_loader:\n",
    "            deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "            logits = model(deit_x, ft_x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * y.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    val_acc = accuracy_score(all_targets, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping check.\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model state.\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model.eval()\n",
    "all_preds, all_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for deit_x, ft_x, y in test_loader:\n",
    "        deit_x, ft_x, y = deit_x.to(device), ft_x.to(device), y.to(device)\n",
    "        logits = model(deit_x, ft_x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6496b2-d8c3-4bbb-bbe5-5497281cdfb5",
   "metadata": {},
   "source": [
    "## Mid Fusion (with deit = 384 and ft.t = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca3287ba-0729-4cdf-8504-3afd7d58ea18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Classes: ['AD' 'CN' 'MCI']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Train Loss: 1.1336, Train Acc: 0.3713 Val Loss: 0.9490, Val Acc: 0.7209\n",
      "Best model saved.\n",
      "Epoch [2/100] Train Loss: 0.9498, Train Acc: 0.5558 Val Loss: 0.7965, Val Acc: 0.8372\n",
      "Best model saved.\n",
      "Epoch [3/100] Train Loss: 0.8152, Train Acc: 0.6916 Val Loss: 0.6886, Val Acc: 0.8692\n",
      "Best model saved.\n",
      "Epoch [4/100] Train Loss: 0.7051, Train Acc: 0.7857 Val Loss: 0.6013, Val Acc: 0.8692\n",
      "Best model saved.\n",
      "Epoch [5/100] Train Loss: 0.6262, Train Acc: 0.8343 Val Loss: 0.5509, Val Acc: 0.8634\n",
      "Best model saved.\n",
      "Epoch [6/100] Train Loss: 0.5701, Train Acc: 0.8555 Val Loss: 0.4742, Val Acc: 0.8663\n",
      "Best model saved.\n",
      "Epoch [7/100] Train Loss: 0.5249, Train Acc: 0.8611 Val Loss: 0.4457, Val Acc: 0.8721\n",
      "Best model saved.\n",
      "Epoch [8/100] Train Loss: 0.4894, Train Acc: 0.8704 Val Loss: 0.4336, Val Acc: 0.8692\n",
      "Best model saved.\n",
      "Epoch [9/100] Train Loss: 0.4573, Train Acc: 0.8791 Val Loss: 0.4159, Val Acc: 0.8721\n",
      "Best model saved.\n",
      "Epoch [10/100] Train Loss: 0.4410, Train Acc: 0.8754 Val Loss: 0.4168, Val Acc: 0.8721\n",
      "Epoch [11/100] Train Loss: 0.4141, Train Acc: 0.8785 Val Loss: 0.3851, Val Acc: 0.8779\n",
      "Best model saved.\n",
      "Epoch [12/100] Train Loss: 0.3974, Train Acc: 0.8891 Val Loss: 0.3692, Val Acc: 0.8779\n",
      "Best model saved.\n",
      "Epoch [13/100] Train Loss: 0.3804, Train Acc: 0.8978 Val Loss: 0.3707, Val Acc: 0.8779\n",
      "Epoch [14/100] Train Loss: 0.3840, Train Acc: 0.8910 Val Loss: 0.3525, Val Acc: 0.8779\n",
      "Best model saved.\n",
      "Epoch [15/100] Train Loss: 0.3480, Train Acc: 0.8984 Val Loss: 0.3556, Val Acc: 0.8750\n",
      "Epoch [16/100] Train Loss: 0.3574, Train Acc: 0.8984 Val Loss: 0.3384, Val Acc: 0.8895\n",
      "Best model saved.\n",
      "Epoch [17/100] Train Loss: 0.3453, Train Acc: 0.8916 Val Loss: 0.3449, Val Acc: 0.8808\n",
      "Epoch [18/100] Train Loss: 0.3309, Train Acc: 0.8991 Val Loss: 0.3358, Val Acc: 0.8808\n",
      "Best model saved.\n",
      "Epoch [19/100] Train Loss: 0.3237, Train Acc: 0.8960 Val Loss: 0.3344, Val Acc: 0.8924\n",
      "Best model saved.\n",
      "Epoch [20/100] Train Loss: 0.3309, Train Acc: 0.8947 Val Loss: 0.3307, Val Acc: 0.8808\n",
      "Best model saved.\n",
      "Epoch [21/100] Train Loss: 0.3112, Train Acc: 0.9084 Val Loss: 0.3277, Val Acc: 0.8866\n",
      "Best model saved.\n",
      "Epoch [22/100] Train Loss: 0.3130, Train Acc: 0.9065 Val Loss: 0.3145, Val Acc: 0.8866\n",
      "Best model saved.\n",
      "Epoch [23/100] Train Loss: 0.3161, Train Acc: 0.8978 Val Loss: 0.3191, Val Acc: 0.8924\n",
      "Epoch [24/100] Train Loss: 0.3141, Train Acc: 0.9028 Val Loss: 0.3150, Val Acc: 0.8924\n",
      "Epoch [25/100] Train Loss: 0.2969, Train Acc: 0.9084 Val Loss: 0.3101, Val Acc: 0.8895\n",
      "Best model saved.\n",
      "Epoch [26/100] Train Loss: 0.2811, Train Acc: 0.9115 Val Loss: 0.3075, Val Acc: 0.8953\n",
      "Best model saved.\n",
      "Epoch [27/100] Train Loss: 0.2732, Train Acc: 0.9190 Val Loss: 0.3053, Val Acc: 0.8924\n",
      "Best model saved.\n",
      "Epoch [28/100] Train Loss: 0.2788, Train Acc: 0.9128 Val Loss: 0.3037, Val Acc: 0.8866\n",
      "Best model saved.\n",
      "Epoch [29/100] Train Loss: 0.2639, Train Acc: 0.9178 Val Loss: 0.2977, Val Acc: 0.8924\n",
      "Best model saved.\n",
      "Epoch [30/100] Train Loss: 0.2549, Train Acc: 0.9227 Val Loss: 0.2994, Val Acc: 0.8924\n",
      "Epoch [31/100] Train Loss: 0.2561, Train Acc: 0.9221 Val Loss: 0.2918, Val Acc: 0.8983\n",
      "Best model saved.\n",
      "Epoch [32/100] Train Loss: 0.2506, Train Acc: 0.9184 Val Loss: 0.2878, Val Acc: 0.8983\n",
      "Best model saved.\n",
      "Epoch [33/100] Train Loss: 0.2444, Train Acc: 0.9221 Val Loss: 0.2887, Val Acc: 0.8924\n",
      "Epoch [34/100] Train Loss: 0.2360, Train Acc: 0.9246 Val Loss: 0.2819, Val Acc: 0.9041\n",
      "Best model saved.\n",
      "Epoch [35/100] Train Loss: 0.2265, Train Acc: 0.9227 Val Loss: 0.2838, Val Acc: 0.8983\n",
      "Epoch [36/100] Train Loss: 0.2130, Train Acc: 0.9414 Val Loss: 0.2770, Val Acc: 0.9012\n",
      "Best model saved.\n",
      "Epoch [37/100] Train Loss: 0.2225, Train Acc: 0.9364 Val Loss: 0.2755, Val Acc: 0.9041\n",
      "Best model saved.\n",
      "Epoch [38/100] Train Loss: 0.2115, Train Acc: 0.9439 Val Loss: 0.2736, Val Acc: 0.9041\n",
      "Best model saved.\n",
      "Epoch [39/100] Train Loss: 0.2029, Train Acc: 0.9371 Val Loss: 0.2711, Val Acc: 0.9012\n",
      "Best model saved.\n",
      "Epoch [40/100] Train Loss: 0.2148, Train Acc: 0.9346 Val Loss: 0.2665, Val Acc: 0.9070\n",
      "Best model saved.\n",
      "Epoch [41/100] Train Loss: 0.2023, Train Acc: 0.9371 Val Loss: 0.2684, Val Acc: 0.9070\n",
      "Epoch [42/100] Train Loss: 0.2120, Train Acc: 0.9352 Val Loss: 0.2699, Val Acc: 0.9012\n",
      "Epoch [43/100] Train Loss: 0.1928, Train Acc: 0.9414 Val Loss: 0.2626, Val Acc: 0.9128\n",
      "Best model saved.\n",
      "Epoch [44/100] Train Loss: 0.1759, Train Acc: 0.9545 Val Loss: 0.2692, Val Acc: 0.9012\n",
      "Epoch [45/100] Train Loss: 0.1525, Train Acc: 0.9626 Val Loss: 0.2625, Val Acc: 0.9070\n",
      "Best model saved.\n",
      "Epoch [46/100] Train Loss: 0.1644, Train Acc: 0.9576 Val Loss: 0.2635, Val Acc: 0.9041\n",
      "Epoch [47/100] Train Loss: 0.1539, Train Acc: 0.9570 Val Loss: 0.2633, Val Acc: 0.8953\n",
      "Epoch [48/100] Train Loss: 0.1656, Train Acc: 0.9564 Val Loss: 0.2585, Val Acc: 0.9099\n",
      "Best model saved.\n",
      "Epoch [49/100] Train Loss: 0.1698, Train Acc: 0.9502 Val Loss: 0.2539, Val Acc: 0.9157\n",
      "Best model saved.\n",
      "Epoch [50/100] Train Loss: 0.1480, Train Acc: 0.9595 Val Loss: 0.2540, Val Acc: 0.9128\n",
      "Epoch [51/100] Train Loss: 0.1544, Train Acc: 0.9564 Val Loss: 0.2530, Val Acc: 0.9070\n",
      "Best model saved.\n",
      "Epoch [52/100] Train Loss: 0.1457, Train Acc: 0.9620 Val Loss: 0.2539, Val Acc: 0.9128\n",
      "Epoch [53/100] Train Loss: 0.1309, Train Acc: 0.9651 Val Loss: 0.2544, Val Acc: 0.9099\n",
      "Epoch [54/100] Train Loss: 0.1348, Train Acc: 0.9626 Val Loss: 0.2538, Val Acc: 0.9099\n",
      "Epoch [55/100] Train Loss: 0.1263, Train Acc: 0.9670 Val Loss: 0.2532, Val Acc: 0.9099\n",
      "Epoch [56/100] Train Loss: 0.1186, Train Acc: 0.9713 Val Loss: 0.2508, Val Acc: 0.9099\n",
      "Best model saved.\n",
      "Epoch [57/100] Train Loss: 0.1283, Train Acc: 0.9676 Val Loss: 0.2492, Val Acc: 0.9041\n",
      "Best model saved.\n",
      "Epoch [58/100] Train Loss: 0.1083, Train Acc: 0.9713 Val Loss: 0.2499, Val Acc: 0.9128\n",
      "Epoch [59/100] Train Loss: 0.1121, Train Acc: 0.9726 Val Loss: 0.2480, Val Acc: 0.9128\n",
      "Best model saved.\n",
      "Epoch [60/100] Train Loss: 0.1235, Train Acc: 0.9651 Val Loss: 0.2482, Val Acc: 0.9070\n",
      "Epoch [61/100] Train Loss: 0.1229, Train Acc: 0.9682 Val Loss: 0.2521, Val Acc: 0.9070\n",
      "Epoch [62/100] Train Loss: 0.1162, Train Acc: 0.9670 Val Loss: 0.2462, Val Acc: 0.9128\n",
      "Best model saved.\n",
      "Epoch [63/100] Train Loss: 0.1057, Train Acc: 0.9757 Val Loss: 0.2435, Val Acc: 0.9157\n",
      "Best model saved.\n",
      "Epoch [64/100] Train Loss: 0.1201, Train Acc: 0.9682 Val Loss: 0.2487, Val Acc: 0.9099\n",
      "Epoch [65/100] Train Loss: 0.1122, Train Acc: 0.9745 Val Loss: 0.2526, Val Acc: 0.9099\n",
      "Epoch [66/100] Train Loss: 0.1124, Train Acc: 0.9720 Val Loss: 0.2532, Val Acc: 0.9099\n",
      "Epoch [67/100] Train Loss: 0.1047, Train Acc: 0.9745 Val Loss: 0.2500, Val Acc: 0.9099\n",
      "Epoch [68/100] Train Loss: 0.1142, Train Acc: 0.9713 Val Loss: 0.2479, Val Acc: 0.9099\n",
      "Epoch [69/100] Train Loss: 0.0974, Train Acc: 0.9763 Val Loss: 0.2510, Val Acc: 0.9099\n",
      "Epoch [70/100] Train Loss: 0.0950, Train Acc: 0.9769 Val Loss: 0.2510, Val Acc: 0.9012\n",
      "Epoch [71/100] Train Loss: 0.0994, Train Acc: 0.9769 Val Loss: 0.2483, Val Acc: 0.9157\n",
      "Epoch [72/100] Train Loss: 0.1071, Train Acc: 0.9769 Val Loss: 0.2531, Val Acc: 0.9041\n",
      "Epoch [73/100] Train Loss: 0.1055, Train Acc: 0.9707 Val Loss: 0.2546, Val Acc: 0.8983\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 0.9014\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.79      0.85      0.82        72\n",
      "          CN       0.98      0.95      0.97       106\n",
      "         MCI       0.90      0.89      0.90       167\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.89      0.90      0.89       345\n",
      "weighted avg       0.90      0.90      0.90       345\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 61   0  11]\n",
      " [  0 101   5]\n",
      " [ 16   2 149]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1015441/2146405688.py:221: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load DeiT features \n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")\n",
    "\n",
    "# Load FTTransformer features \n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")\n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "# Load CSV labels\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "\n",
    "# Initialize scalers for each modality\n",
    "scaler_deit = StandardScaler()\n",
    "scaler_ft = StandardScaler()\n",
    "\n",
    "# Fit scalers on training data and transform\n",
    "train_features_deit = scaler_deit.fit_transform(deit_train)\n",
    "val_features_deit = scaler_deit.transform(deit_val)\n",
    "test_features_deit = scaler_deit.transform(deit_test)\n",
    "\n",
    "train_features_ft = scaler_ft.fit_transform(ft_train)\n",
    "val_features_ft = scaler_ft.transform(ft_val)\n",
    "test_features_ft = scaler_ft.transform(ft_test)\n",
    "\n",
    "# 4. Define the Concatenation Fusion Model\n",
    "class ConcatenationFusionModel(nn.Module):\n",
    "    def __init__(self, deit_input_size, ft_input_size, num_classes):\n",
    "        super(ConcatenationFusionModel, self).__init__()\n",
    "        # DeiT branch (image modality)\n",
    "        self.deit_fc = nn.Sequential(\n",
    "            nn.Linear(deit_input_size, 384),   \n",
    "            nn.BatchNorm1d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        # FTTransformer branch (tabular modality)\n",
    "        self.ft_fc = nn.Sequential(\n",
    "            nn.Linear(ft_input_size, 128),   \n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        # Classifier: concatenated features from both branches\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, deit_input, ft_input):\n",
    "        # Process each modality separately\n",
    "        deit_output = self.deit_fc(deit_input)    \n",
    "        ft_output = self.ft_fc(ft_input)           \n",
    "        # Concatenate features from both modalities\n",
    "        concatenated_features = torch.cat([deit_output, ft_output], dim=1)  \n",
    "        # Classify\n",
    "        logits = self.classifier(concatenated_features)  \n",
    "        return logits\n",
    "\n",
    "# Initialize the model\n",
    "image_input_size = train_features_deit.shape[1]  \n",
    "tab_input_size = train_features_ft.shape[1]        \n",
    "\n",
    "model = ConcatenationFusionModel(image_input_size, tab_input_size, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# 5. Prepare the data loaders\n",
    "# Convert to tensors\n",
    "train_deit_tensor = torch.tensor(train_features_deit, dtype=torch.float32)\n",
    "val_deit_tensor = torch.tensor(val_features_deit, dtype=torch.float32)\n",
    "test_deit_tensor = torch.tensor(test_features_deit, dtype=torch.float32)\n",
    "\n",
    "train_ft_tensor = torch.tensor(train_features_ft, dtype=torch.float32)\n",
    "val_ft_tensor = torch.tensor(val_features_ft, dtype=torch.float32)\n",
    "test_ft_tensor = torch.tensor(test_features_ft, dtype=torch.float32)\n",
    "\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(train_deit_tensor, train_ft_tensor, train_labels_tensor)\n",
    "val_dataset = TensorDataset(val_deit_tensor, val_ft_tensor, val_labels_tensor)\n",
    "test_dataset = TensorDataset(test_deit_tensor, test_ft_tensor, test_labels_tensor)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 6. Training Setup\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_patience = 10\n",
    "epochs_no_improve = 0\n",
    "best_val_loss = np.Inf\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# 7. Training Loop with Validation and Early Stopping mechanism\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_train_correct = 0\n",
    "    for deit_inputs, ft_inputs, labels in train_loader:\n",
    "        deit_inputs = deit_inputs.to(device)\n",
    "        ft_inputs = ft_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(deit_inputs, ft_inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item() * labels.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total_train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    train_loss = total_train_loss / len(train_dataset)\n",
    "    train_accuracy = total_train_correct.double() / len(train_dataset)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    total_val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for deit_inputs, ft_inputs, labels in val_loader:\n",
    "            deit_inputs = deit_inputs.to(device)\n",
    "            ft_inputs = ft_inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(deit_inputs, ft_inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_val_loss += loss.item() * labels.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    val_loss = total_val_loss / len(val_dataset)\n",
    "    val_accuracy = total_val_correct.double() / len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f} \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# 8. Evaluate the Model on the Test Set\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "total_test_correct = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for deit_inputs, ft_inputs, labels in test_loader:\n",
    "        deit_inputs = deit_inputs.to(device)\n",
    "        ft_inputs = ft_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(deit_inputs, ft_inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        total_test_correct += torch.sum(preds == labels.data)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_accuracy = total_test_correct.double() / len(test_dataset)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8908f5-c511-4d1c-8564-29868c10a7a2",
   "metadata": {},
   "source": [
    "## Mid Fusion (with deit = 308 and ft.t = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54ed03fd-aa9f-4ad3-9ab7-bfe1fe4ab3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Classes: ['AD' 'CN' 'MCI']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Train Loss: 1.0254, Train Acc: 0.4729 Val Loss: 0.9001, Val Acc: 0.7587\n",
      "Best model saved.\n",
      "Epoch [2/100] Train Loss: 0.8606, Train Acc: 0.6617 Val Loss: 0.7841, Val Acc: 0.8430\n",
      "Best model saved.\n",
      "Epoch [3/100] Train Loss: 0.7354, Train Acc: 0.7751 Val Loss: 0.6766, Val Acc: 0.8517\n",
      "Best model saved.\n",
      "Epoch [4/100] Train Loss: 0.6467, Train Acc: 0.8224 Val Loss: 0.6163, Val Acc: 0.8488\n",
      "Best model saved.\n",
      "Epoch [5/100] Train Loss: 0.5909, Train Acc: 0.8293 Val Loss: 0.5412, Val Acc: 0.8576\n",
      "Best model saved.\n",
      "Epoch [6/100] Train Loss: 0.5464, Train Acc: 0.8505 Val Loss: 0.4938, Val Acc: 0.8547\n",
      "Best model saved.\n",
      "Epoch [7/100] Train Loss: 0.4950, Train Acc: 0.8679 Val Loss: 0.4852, Val Acc: 0.8547\n",
      "Best model saved.\n",
      "Epoch [8/100] Train Loss: 0.4632, Train Acc: 0.8760 Val Loss: 0.4881, Val Acc: 0.8547\n",
      "Epoch [9/100] Train Loss: 0.4428, Train Acc: 0.8766 Val Loss: 0.4319, Val Acc: 0.8517\n",
      "Best model saved.\n",
      "Epoch [10/100] Train Loss: 0.4066, Train Acc: 0.8810 Val Loss: 0.4142, Val Acc: 0.8517\n",
      "Best model saved.\n",
      "Epoch [11/100] Train Loss: 0.3917, Train Acc: 0.8804 Val Loss: 0.3843, Val Acc: 0.8576\n",
      "Best model saved.\n",
      "Epoch [12/100] Train Loss: 0.3854, Train Acc: 0.8860 Val Loss: 0.3832, Val Acc: 0.8605\n",
      "Best model saved.\n",
      "Epoch [13/100] Train Loss: 0.3692, Train Acc: 0.8941 Val Loss: 0.3807, Val Acc: 0.8576\n",
      "Best model saved.\n",
      "Epoch [14/100] Train Loss: 0.3730, Train Acc: 0.8872 Val Loss: 0.3622, Val Acc: 0.8576\n",
      "Best model saved.\n",
      "Epoch [15/100] Train Loss: 0.3574, Train Acc: 0.8879 Val Loss: 0.3684, Val Acc: 0.8576\n",
      "Epoch [16/100] Train Loss: 0.3654, Train Acc: 0.8910 Val Loss: 0.3540, Val Acc: 0.8605\n",
      "Best model saved.\n",
      "Epoch [17/100] Train Loss: 0.3433, Train Acc: 0.8947 Val Loss: 0.3456, Val Acc: 0.8721\n",
      "Best model saved.\n",
      "Epoch [18/100] Train Loss: 0.3281, Train Acc: 0.8953 Val Loss: 0.3488, Val Acc: 0.8692\n",
      "Epoch [19/100] Train Loss: 0.3483, Train Acc: 0.8866 Val Loss: 0.3375, Val Acc: 0.8634\n",
      "Best model saved.\n",
      "Epoch [20/100] Train Loss: 0.3158, Train Acc: 0.9034 Val Loss: 0.3333, Val Acc: 0.8634\n",
      "Best model saved.\n",
      "Epoch [21/100] Train Loss: 0.3143, Train Acc: 0.9028 Val Loss: 0.3240, Val Acc: 0.8663\n",
      "Best model saved.\n",
      "Epoch [22/100] Train Loss: 0.2997, Train Acc: 0.9078 Val Loss: 0.3134, Val Acc: 0.8721\n",
      "Best model saved.\n",
      "Epoch [23/100] Train Loss: 0.2983, Train Acc: 0.9097 Val Loss: 0.3228, Val Acc: 0.8692\n",
      "Epoch [24/100] Train Loss: 0.2962, Train Acc: 0.9034 Val Loss: 0.3108, Val Acc: 0.8750\n",
      "Best model saved.\n",
      "Epoch [25/100] Train Loss: 0.2965, Train Acc: 0.9016 Val Loss: 0.3105, Val Acc: 0.8634\n",
      "Best model saved.\n",
      "Epoch [26/100] Train Loss: 0.2929, Train Acc: 0.9078 Val Loss: 0.3040, Val Acc: 0.8808\n",
      "Best model saved.\n",
      "Epoch [27/100] Train Loss: 0.2903, Train Acc: 0.9047 Val Loss: 0.3007, Val Acc: 0.8779\n",
      "Best model saved.\n",
      "Epoch [28/100] Train Loss: 0.2719, Train Acc: 0.9146 Val Loss: 0.2971, Val Acc: 0.8837\n",
      "Best model saved.\n",
      "Epoch [29/100] Train Loss: 0.2653, Train Acc: 0.9128 Val Loss: 0.2993, Val Acc: 0.8837\n",
      "Epoch [30/100] Train Loss: 0.2637, Train Acc: 0.9190 Val Loss: 0.2916, Val Acc: 0.8924\n",
      "Best model saved.\n",
      "Epoch [31/100] Train Loss: 0.2658, Train Acc: 0.9121 Val Loss: 0.2840, Val Acc: 0.8837\n",
      "Best model saved.\n",
      "Epoch [32/100] Train Loss: 0.2517, Train Acc: 0.9246 Val Loss: 0.2879, Val Acc: 0.8895\n",
      "Epoch [33/100] Train Loss: 0.2636, Train Acc: 0.9146 Val Loss: 0.2759, Val Acc: 0.8924\n",
      "Best model saved.\n",
      "Epoch [34/100] Train Loss: 0.2465, Train Acc: 0.9190 Val Loss: 0.2820, Val Acc: 0.8924\n",
      "Epoch [35/100] Train Loss: 0.2292, Train Acc: 0.9240 Val Loss: 0.2777, Val Acc: 0.8866\n",
      "Epoch [36/100] Train Loss: 0.2357, Train Acc: 0.9277 Val Loss: 0.2855, Val Acc: 0.8924\n",
      "Epoch [37/100] Train Loss: 0.2326, Train Acc: 0.9308 Val Loss: 0.2749, Val Acc: 0.8953\n",
      "Best model saved.\n",
      "Epoch [38/100] Train Loss: 0.2042, Train Acc: 0.9383 Val Loss: 0.2741, Val Acc: 0.9012\n",
      "Best model saved.\n",
      "Epoch [39/100] Train Loss: 0.2153, Train Acc: 0.9389 Val Loss: 0.2676, Val Acc: 0.8953\n",
      "Best model saved.\n",
      "Epoch [40/100] Train Loss: 0.2147, Train Acc: 0.9252 Val Loss: 0.2693, Val Acc: 0.8924\n",
      "Epoch [41/100] Train Loss: 0.2299, Train Acc: 0.9290 Val Loss: 0.2666, Val Acc: 0.8983\n",
      "Best model saved.\n",
      "Epoch [42/100] Train Loss: 0.2043, Train Acc: 0.9427 Val Loss: 0.2687, Val Acc: 0.8953\n",
      "Epoch [43/100] Train Loss: 0.2074, Train Acc: 0.9377 Val Loss: 0.2633, Val Acc: 0.9012\n",
      "Best model saved.\n",
      "Epoch [44/100] Train Loss: 0.2130, Train Acc: 0.9371 Val Loss: 0.2602, Val Acc: 0.9070\n",
      "Best model saved.\n",
      "Epoch [45/100] Train Loss: 0.2213, Train Acc: 0.9290 Val Loss: 0.2705, Val Acc: 0.9012\n",
      "Epoch [46/100] Train Loss: 0.1940, Train Acc: 0.9396 Val Loss: 0.2661, Val Acc: 0.9012\n",
      "Epoch [47/100] Train Loss: 0.1944, Train Acc: 0.9364 Val Loss: 0.2657, Val Acc: 0.9070\n",
      "Epoch [48/100] Train Loss: 0.1980, Train Acc: 0.9427 Val Loss: 0.2638, Val Acc: 0.9041\n",
      "Epoch [49/100] Train Loss: 0.1946, Train Acc: 0.9389 Val Loss: 0.2621, Val Acc: 0.9012\n",
      "Epoch [50/100] Train Loss: 0.1835, Train Acc: 0.9483 Val Loss: 0.2615, Val Acc: 0.9041\n",
      "Epoch [51/100] Train Loss: 0.1933, Train Acc: 0.9458 Val Loss: 0.2593, Val Acc: 0.9070\n",
      "Best model saved.\n",
      "Epoch [52/100] Train Loss: 0.1858, Train Acc: 0.9396 Val Loss: 0.2600, Val Acc: 0.9041\n",
      "Epoch [53/100] Train Loss: 0.1997, Train Acc: 0.9421 Val Loss: 0.2616, Val Acc: 0.9070\n",
      "Epoch [54/100] Train Loss: 0.1893, Train Acc: 0.9421 Val Loss: 0.2633, Val Acc: 0.9041\n",
      "Epoch [55/100] Train Loss: 0.1842, Train Acc: 0.9421 Val Loss: 0.2619, Val Acc: 0.9012\n",
      "Epoch [56/100] Train Loss: 0.1878, Train Acc: 0.9464 Val Loss: 0.2624, Val Acc: 0.9041\n",
      "Epoch [57/100] Train Loss: 0.1865, Train Acc: 0.9483 Val Loss: 0.2636, Val Acc: 0.9041\n",
      "Epoch [58/100] Train Loss: 0.1774, Train Acc: 0.9445 Val Loss: 0.2589, Val Acc: 0.9070\n",
      "Best model saved.\n",
      "Epoch [59/100] Train Loss: 0.1819, Train Acc: 0.9427 Val Loss: 0.2622, Val Acc: 0.9041\n",
      "Epoch [60/100] Train Loss: 0.1823, Train Acc: 0.9508 Val Loss: 0.2596, Val Acc: 0.9099\n",
      "Epoch [61/100] Train Loss: 0.1900, Train Acc: 0.9408 Val Loss: 0.2582, Val Acc: 0.9041\n",
      "Best model saved.\n",
      "Epoch [62/100] Train Loss: 0.1965, Train Acc: 0.9371 Val Loss: 0.2630, Val Acc: 0.9041\n",
      "Epoch [63/100] Train Loss: 0.1710, Train Acc: 0.9539 Val Loss: 0.2644, Val Acc: 0.8924\n",
      "Epoch [64/100] Train Loss: 0.1884, Train Acc: 0.9470 Val Loss: 0.2632, Val Acc: 0.9012\n",
      "Epoch [65/100] Train Loss: 0.1811, Train Acc: 0.9489 Val Loss: 0.2606, Val Acc: 0.9041\n",
      "Epoch [66/100] Train Loss: 0.1833, Train Acc: 0.9433 Val Loss: 0.2617, Val Acc: 0.9070\n",
      "Epoch [67/100] Train Loss: 0.1918, Train Acc: 0.9445 Val Loss: 0.2713, Val Acc: 0.8895\n",
      "Epoch [68/100] Train Loss: 0.1908, Train Acc: 0.9402 Val Loss: 0.2594, Val Acc: 0.9070\n",
      "Epoch [69/100] Train Loss: 0.2085, Train Acc: 0.9458 Val Loss: 0.2682, Val Acc: 0.8924\n",
      "Epoch [70/100] Train Loss: 0.1887, Train Acc: 0.9396 Val Loss: 0.2646, Val Acc: 0.9012\n",
      "Epoch [71/100] Train Loss: 0.1950, Train Acc: 0.9458 Val Loss: 0.2593, Val Acc: 0.9041\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 0.9246\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.87      0.85      0.86        72\n",
      "          CN       0.99      0.95      0.97       106\n",
      "         MCI       0.91      0.94      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.92      0.91      0.92       345\n",
      "weighted avg       0.93      0.92      0.92       345\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 61   0  11]\n",
      " [  0 101   5]\n",
      " [  9   1 157]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2616349/265524843.py:221: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load DeiT features \n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")\n",
    "\n",
    "# Load FTTransformer features \n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")\n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "# Load CSV labels\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "\n",
    "# Initialize scalers for each modality\n",
    "scaler_deit = StandardScaler()\n",
    "scaler_ft = StandardScaler()\n",
    "\n",
    "# Fit scalers on training data and transform\n",
    "train_features_deit = scaler_deit.fit_transform(deit_train)\n",
    "val_features_deit = scaler_deit.transform(deit_val)\n",
    "test_features_deit = scaler_deit.transform(deit_test)\n",
    "\n",
    "train_features_ft = scaler_ft.fit_transform(ft_train)\n",
    "val_features_ft = scaler_ft.transform(ft_val)\n",
    "test_features_ft = scaler_ft.transform(ft_test)\n",
    "\n",
    "# 4. Define the Concatenation Fusion Model\n",
    "class ConcatenationFusionModel(nn.Module):\n",
    "    def __init__(self, deit_input_size, ft_input_size, num_classes):\n",
    "        super(ConcatenationFusionModel, self).__init__()\n",
    "        # DeiT branch (image modality)\n",
    "        self.deit_fc = nn.Sequential(\n",
    "            nn.Linear(deit_input_size, 308),   \n",
    "            nn.BatchNorm1d(308),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        # FTTransformer branch (tabular modality)\n",
    "        self.ft_fc = nn.Sequential(\n",
    "            nn.Linear(ft_input_size, 128),   \n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        # Classifier: concatenated features from both branches\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(436, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, deit_input, ft_input):\n",
    "        # Process each modality separately\n",
    "        deit_output = self.deit_fc(deit_input)    \n",
    "        ft_output = self.ft_fc(ft_input)           \n",
    "        # Concatenate features from both modalities\n",
    "        concatenated_features = torch.cat([deit_output, ft_output], dim=1)  \n",
    "        # Classify\n",
    "        logits = self.classifier(concatenated_features)  \n",
    "        return logits\n",
    "\n",
    "# Initialize the model\n",
    "image_input_size = train_features_deit.shape[1]  \n",
    "tab_input_size = train_features_ft.shape[1]        \n",
    "\n",
    "model = ConcatenationFusionModel(image_input_size, tab_input_size, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# 5. Prepare the data loaders\n",
    "# Convert to tensors\n",
    "train_deit_tensor = torch.tensor(train_features_deit, dtype=torch.float32)\n",
    "val_deit_tensor = torch.tensor(val_features_deit, dtype=torch.float32)\n",
    "test_deit_tensor = torch.tensor(test_features_deit, dtype=torch.float32)\n",
    "\n",
    "train_ft_tensor = torch.tensor(train_features_ft, dtype=torch.float32)\n",
    "val_ft_tensor = torch.tensor(val_features_ft, dtype=torch.float32)\n",
    "test_ft_tensor = torch.tensor(test_features_ft, dtype=torch.float32)\n",
    "\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(train_deit_tensor, train_ft_tensor, train_labels_tensor)\n",
    "val_dataset = TensorDataset(val_deit_tensor, val_ft_tensor, val_labels_tensor)\n",
    "test_dataset = TensorDataset(test_deit_tensor, test_ft_tensor, test_labels_tensor)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 6. Training Setup\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_patience = 10\n",
    "epochs_no_improve = 0\n",
    "best_val_loss = np.Inf\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# 7. Training Loop with Validation and Early Stopping mechanism\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_train_correct = 0\n",
    "    for deit_inputs, ft_inputs, labels in train_loader:\n",
    "        deit_inputs = deit_inputs.to(device)\n",
    "        ft_inputs = ft_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(deit_inputs, ft_inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item() * labels.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total_train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    train_loss = total_train_loss / len(train_dataset)\n",
    "    train_accuracy = total_train_correct.double() / len(train_dataset)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    total_val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for deit_inputs, ft_inputs, labels in val_loader:\n",
    "            deit_inputs = deit_inputs.to(device)\n",
    "            ft_inputs = ft_inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(deit_inputs, ft_inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_val_loss += loss.item() * labels.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    val_loss = total_val_loss / len(val_dataset)\n",
    "    val_accuracy = total_val_correct.double() / len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f} \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# 8. Evaluate the Model on the Test Set\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "total_test_correct = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for deit_inputs, ft_inputs, labels in test_loader:\n",
    "        deit_inputs = deit_inputs.to(device)\n",
    "        ft_inputs = ft_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(deit_inputs, ft_inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        total_test_correct += torch.sum(preds == labels.data)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_accuracy = total_test_correct.double() / len(test_dataset)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6de7ff0-7130-489c-8168-999c20dfb63d",
   "metadata": {},
   "source": [
    "## Margin of Error for Mid fusion approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e57abcbc-0b38-472d-82af-22041b4cb79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Classes: ['AD' 'CN' 'MCI']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Train Loss: 1.0254, Train Acc: 0.4729 Val Loss: 0.9001, Val Acc: 0.7587\n",
      "Best model saved.\n",
      "Epoch [2/100] Train Loss: 0.8606, Train Acc: 0.6617 Val Loss: 0.7841, Val Acc: 0.8430\n",
      "Best model saved.\n",
      "Epoch [3/100] Train Loss: 0.7354, Train Acc: 0.7751 Val Loss: 0.6766, Val Acc: 0.8517\n",
      "Best model saved.\n",
      "Epoch [4/100] Train Loss: 0.6467, Train Acc: 0.8224 Val Loss: 0.6163, Val Acc: 0.8488\n",
      "Best model saved.\n",
      "Epoch [5/100] Train Loss: 0.5909, Train Acc: 0.8293 Val Loss: 0.5412, Val Acc: 0.8576\n",
      "Best model saved.\n",
      "Epoch [6/100] Train Loss: 0.5464, Train Acc: 0.8505 Val Loss: 0.4938, Val Acc: 0.8547\n",
      "Best model saved.\n",
      "Epoch [7/100] Train Loss: 0.4950, Train Acc: 0.8679 Val Loss: 0.4852, Val Acc: 0.8547\n",
      "Best model saved.\n",
      "Epoch [8/100] Train Loss: 0.4632, Train Acc: 0.8760 Val Loss: 0.4881, Val Acc: 0.8547\n",
      "Epoch [9/100] Train Loss: 0.4428, Train Acc: 0.8766 Val Loss: 0.4319, Val Acc: 0.8517\n",
      "Best model saved.\n",
      "Epoch [10/100] Train Loss: 0.4066, Train Acc: 0.8810 Val Loss: 0.4142, Val Acc: 0.8517\n",
      "Best model saved.\n",
      "Epoch [11/100] Train Loss: 0.3917, Train Acc: 0.8804 Val Loss: 0.3843, Val Acc: 0.8576\n",
      "Best model saved.\n",
      "Epoch [12/100] Train Loss: 0.3854, Train Acc: 0.8860 Val Loss: 0.3832, Val Acc: 0.8605\n",
      "Best model saved.\n",
      "Epoch [13/100] Train Loss: 0.3692, Train Acc: 0.8941 Val Loss: 0.3807, Val Acc: 0.8576\n",
      "Best model saved.\n",
      "Epoch [14/100] Train Loss: 0.3730, Train Acc: 0.8872 Val Loss: 0.3622, Val Acc: 0.8576\n",
      "Best model saved.\n",
      "Epoch [15/100] Train Loss: 0.3574, Train Acc: 0.8879 Val Loss: 0.3684, Val Acc: 0.8576\n",
      "Epoch [16/100] Train Loss: 0.3654, Train Acc: 0.8910 Val Loss: 0.3540, Val Acc: 0.8605\n",
      "Best model saved.\n",
      "Epoch [17/100] Train Loss: 0.3433, Train Acc: 0.8947 Val Loss: 0.3456, Val Acc: 0.8721\n",
      "Best model saved.\n",
      "Epoch [18/100] Train Loss: 0.3281, Train Acc: 0.8953 Val Loss: 0.3488, Val Acc: 0.8692\n",
      "Epoch [19/100] Train Loss: 0.3483, Train Acc: 0.8866 Val Loss: 0.3375, Val Acc: 0.8634\n",
      "Best model saved.\n",
      "Epoch [20/100] Train Loss: 0.3158, Train Acc: 0.9034 Val Loss: 0.3333, Val Acc: 0.8634\n",
      "Best model saved.\n",
      "Epoch [21/100] Train Loss: 0.3143, Train Acc: 0.9028 Val Loss: 0.3240, Val Acc: 0.8663\n",
      "Best model saved.\n",
      "Epoch [22/100] Train Loss: 0.2997, Train Acc: 0.9078 Val Loss: 0.3134, Val Acc: 0.8721\n",
      "Best model saved.\n",
      "Epoch [23/100] Train Loss: 0.2983, Train Acc: 0.9097 Val Loss: 0.3228, Val Acc: 0.8692\n",
      "Epoch [24/100] Train Loss: 0.2962, Train Acc: 0.9034 Val Loss: 0.3108, Val Acc: 0.8750\n",
      "Best model saved.\n",
      "Epoch [25/100] Train Loss: 0.2965, Train Acc: 0.9016 Val Loss: 0.3105, Val Acc: 0.8634\n",
      "Best model saved.\n",
      "Epoch [26/100] Train Loss: 0.2929, Train Acc: 0.9078 Val Loss: 0.3040, Val Acc: 0.8808\n",
      "Best model saved.\n",
      "Epoch [27/100] Train Loss: 0.2903, Train Acc: 0.9047 Val Loss: 0.3007, Val Acc: 0.8779\n",
      "Best model saved.\n",
      "Epoch [28/100] Train Loss: 0.2719, Train Acc: 0.9146 Val Loss: 0.2971, Val Acc: 0.8837\n",
      "Best model saved.\n",
      "Epoch [29/100] Train Loss: 0.2653, Train Acc: 0.9128 Val Loss: 0.2993, Val Acc: 0.8837\n",
      "Epoch [30/100] Train Loss: 0.2637, Train Acc: 0.9190 Val Loss: 0.2916, Val Acc: 0.8924\n",
      "Best model saved.\n",
      "Epoch [31/100] Train Loss: 0.2658, Train Acc: 0.9121 Val Loss: 0.2840, Val Acc: 0.8837\n",
      "Best model saved.\n",
      "Epoch [32/100] Train Loss: 0.2517, Train Acc: 0.9246 Val Loss: 0.2879, Val Acc: 0.8895\n",
      "Epoch [33/100] Train Loss: 0.2636, Train Acc: 0.9146 Val Loss: 0.2759, Val Acc: 0.8924\n",
      "Best model saved.\n",
      "Epoch [34/100] Train Loss: 0.2465, Train Acc: 0.9190 Val Loss: 0.2820, Val Acc: 0.8924\n",
      "Epoch [35/100] Train Loss: 0.2292, Train Acc: 0.9240 Val Loss: 0.2777, Val Acc: 0.8866\n",
      "Epoch [36/100] Train Loss: 0.2357, Train Acc: 0.9277 Val Loss: 0.2855, Val Acc: 0.8924\n",
      "Epoch [37/100] Train Loss: 0.2326, Train Acc: 0.9308 Val Loss: 0.2749, Val Acc: 0.8953\n",
      "Best model saved.\n",
      "Epoch [38/100] Train Loss: 0.2042, Train Acc: 0.9383 Val Loss: 0.2741, Val Acc: 0.9012\n",
      "Best model saved.\n",
      "Epoch [39/100] Train Loss: 0.2153, Train Acc: 0.9389 Val Loss: 0.2676, Val Acc: 0.8953\n",
      "Best model saved.\n",
      "Epoch [40/100] Train Loss: 0.2147, Train Acc: 0.9252 Val Loss: 0.2693, Val Acc: 0.8924\n",
      "Epoch [41/100] Train Loss: 0.2299, Train Acc: 0.9290 Val Loss: 0.2666, Val Acc: 0.8983\n",
      "Best model saved.\n",
      "Epoch [42/100] Train Loss: 0.2043, Train Acc: 0.9427 Val Loss: 0.2687, Val Acc: 0.8953\n",
      "Epoch [43/100] Train Loss: 0.2074, Train Acc: 0.9377 Val Loss: 0.2633, Val Acc: 0.9012\n",
      "Best model saved.\n",
      "Epoch [44/100] Train Loss: 0.2130, Train Acc: 0.9371 Val Loss: 0.2602, Val Acc: 0.9070\n",
      "Best model saved.\n",
      "Epoch [45/100] Train Loss: 0.2213, Train Acc: 0.9290 Val Loss: 0.2705, Val Acc: 0.9012\n",
      "Epoch [46/100] Train Loss: 0.1940, Train Acc: 0.9396 Val Loss: 0.2661, Val Acc: 0.9012\n",
      "Epoch [47/100] Train Loss: 0.1944, Train Acc: 0.9364 Val Loss: 0.2657, Val Acc: 0.9070\n",
      "Epoch [48/100] Train Loss: 0.1980, Train Acc: 0.9427 Val Loss: 0.2638, Val Acc: 0.9041\n",
      "Epoch [49/100] Train Loss: 0.1946, Train Acc: 0.9389 Val Loss: 0.2621, Val Acc: 0.9012\n",
      "Epoch [50/100] Train Loss: 0.1835, Train Acc: 0.9483 Val Loss: 0.2615, Val Acc: 0.9041\n",
      "Epoch [51/100] Train Loss: 0.1933, Train Acc: 0.9458 Val Loss: 0.2593, Val Acc: 0.9070\n",
      "Best model saved.\n",
      "Epoch [52/100] Train Loss: 0.1858, Train Acc: 0.9396 Val Loss: 0.2600, Val Acc: 0.9041\n",
      "Epoch [53/100] Train Loss: 0.1997, Train Acc: 0.9421 Val Loss: 0.2616, Val Acc: 0.9070\n",
      "Epoch [54/100] Train Loss: 0.1893, Train Acc: 0.9421 Val Loss: 0.2633, Val Acc: 0.9041\n",
      "Epoch [55/100] Train Loss: 0.1842, Train Acc: 0.9421 Val Loss: 0.2619, Val Acc: 0.9012\n",
      "Epoch [56/100] Train Loss: 0.1878, Train Acc: 0.9464 Val Loss: 0.2624, Val Acc: 0.9041\n",
      "Epoch [57/100] Train Loss: 0.1865, Train Acc: 0.9483 Val Loss: 0.2636, Val Acc: 0.9041\n",
      "Epoch [58/100] Train Loss: 0.1774, Train Acc: 0.9445 Val Loss: 0.2589, Val Acc: 0.9070\n",
      "Best model saved.\n",
      "Epoch [59/100] Train Loss: 0.1819, Train Acc: 0.9427 Val Loss: 0.2622, Val Acc: 0.9041\n",
      "Epoch [60/100] Train Loss: 0.1823, Train Acc: 0.9508 Val Loss: 0.2596, Val Acc: 0.9099\n",
      "Epoch [61/100] Train Loss: 0.1900, Train Acc: 0.9408 Val Loss: 0.2582, Val Acc: 0.9041\n",
      "Best model saved.\n",
      "Epoch [62/100] Train Loss: 0.1965, Train Acc: 0.9371 Val Loss: 0.2630, Val Acc: 0.9041\n",
      "Epoch [63/100] Train Loss: 0.1710, Train Acc: 0.9539 Val Loss: 0.2644, Val Acc: 0.8924\n",
      "Epoch [64/100] Train Loss: 0.1884, Train Acc: 0.9470 Val Loss: 0.2632, Val Acc: 0.9012\n",
      "Epoch [65/100] Train Loss: 0.1811, Train Acc: 0.9489 Val Loss: 0.2606, Val Acc: 0.9041\n",
      "Epoch [66/100] Train Loss: 0.1833, Train Acc: 0.9433 Val Loss: 0.2617, Val Acc: 0.9070\n",
      "Epoch [67/100] Train Loss: 0.1918, Train Acc: 0.9445 Val Loss: 0.2713, Val Acc: 0.8895\n",
      "Epoch [68/100] Train Loss: 0.1908, Train Acc: 0.9402 Val Loss: 0.2594, Val Acc: 0.9070\n",
      "Epoch [69/100] Train Loss: 0.2085, Train Acc: 0.9458 Val Loss: 0.2682, Val Acc: 0.8924\n",
      "Epoch [70/100] Train Loss: 0.1887, Train Acc: 0.9396 Val Loss: 0.2646, Val Acc: 0.9012\n",
      "Epoch [71/100] Train Loss: 0.1950, Train Acc: 0.9458 Val Loss: 0.2593, Val Acc: 0.9041\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 0.9246\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.87      0.85      0.86        72\n",
      "          CN       0.99      0.95      0.97       106\n",
      "         MCI       0.91      0.94      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.92      0.91      0.92       345\n",
      "weighted avg       0.93      0.92      0.92       345\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 61   0  11]\n",
      " [  0 101   5]\n",
      " [  9   1 157]]\n",
      "\n",
      "--- Margin of Error Calculation: Direct Method ---\n",
      "Standard Error: 0.0142\n",
      "Margin of Error (95% CI): 0.0279\n",
      "95% Confidence Interval for Accuracy: [0.8968, 0.9525]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_209839/3339595432.py:226: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Margin of Error Calculation: Bootstrapping ---\n",
      "Bootstrapped Mean Accuracy: 0.9243\n",
      "Standard Error (Bootstrap): 0.0146\n",
      "Margin of Error (Bootstrap, 95% CI): 0.0287\n",
      "\n",
      "Weighted F1 Score: 0.9247271721350467\n",
      "\n",
      "Bootstrapping for Weighted F1 Score:\n",
      "Bootstrapped Mean Weighted F1: 0.9245520757195598\n",
      "Standard Error (Bootstrap) for Weighted F1: 0.01326339528043738\n",
      "Margin of Error (Bootstrap, 95% CI) for Weighted F1: 0.025996254749657267\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import math\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Check device (using cuda:1 if available)\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "##############################################\n",
    "# 1. Load Features and Labels\n",
    "##############################################\n",
    "# Load DeiT features (expected shape: [num_samples, 768])\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")\n",
    "\n",
    "# Load FTTransformer features (expected shape: [num_samples, 192])\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")\n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "# Load CSV labels\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "\n",
    "##############################################\n",
    "# 2. Scale the Features Separately (for each modality)\n",
    "##############################################\n",
    "# Initialize scalers for each modality\n",
    "scaler_deit = StandardScaler()\n",
    "scaler_ft = StandardScaler()\n",
    "\n",
    "# Fit scalers on training data and transform\n",
    "train_features_deit = scaler_deit.fit_transform(deit_train)\n",
    "val_features_deit   = scaler_deit.transform(deit_val)\n",
    "test_features_deit  = scaler_deit.transform(deit_test)\n",
    "\n",
    "train_features_ft = scaler_ft.fit_transform(ft_train)\n",
    "val_features_ft   = scaler_ft.transform(ft_val)\n",
    "test_features_ft  = scaler_ft.transform(ft_test)\n",
    "\n",
    "##############################################\n",
    "# 3. Define the Concatenation Fusion (Mid Fusion) Model\n",
    "##############################################\n",
    "class ConcatenationFusionModel(nn.Module):\n",
    "    def __init__(self, deit_input_size, ft_input_size, num_classes):\n",
    "        super(ConcatenationFusionModel, self).__init__()\n",
    "        # DeiT branch (image modality)\n",
    "        self.deit_fc = nn.Sequential(\n",
    "            nn.Linear(deit_input_size, 308),   # Reduce dimension (example: 768 -> 308)\n",
    "            nn.BatchNorm1d(308),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        # FTTransformer branch (tabular modality)\n",
    "        self.ft_fc = nn.Sequential(\n",
    "            nn.Linear(ft_input_size, 128),     # Reduce dimension (example: 192 -> 128)\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        # Classifier: concatenate features from both branches\n",
    "        # Note: Although the branches output 308 and 128 features respectively,\n",
    "        # you can change the concatenated dimension if further processing is desired.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(308 + 128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, deit_input, ft_input):\n",
    "        # Process each modality separately\n",
    "        deit_output = self.deit_fc(deit_input)\n",
    "        ft_output = self.ft_fc(ft_input)\n",
    "        # Concatenate features from both modalities\n",
    "        concatenated_features = torch.cat([deit_output, ft_output], dim=1)\n",
    "        # Classification\n",
    "        logits = self.classifier(concatenated_features)\n",
    "        return logits\n",
    "\n",
    "# Initialize the model\n",
    "image_input_size = train_features_deit.shape[1]  # e.g., 768\n",
    "tab_input_size = train_features_ft.shape[1]        # e.g., 192\n",
    "model = ConcatenationFusionModel(image_input_size, tab_input_size, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "##############################################\n",
    "# 4. Prepare the Data Loaders\n",
    "##############################################\n",
    "# Convert features to tensors\n",
    "train_deit_tensor = torch.tensor(train_features_deit, dtype=torch.float32)\n",
    "val_deit_tensor   = torch.tensor(val_features_deit, dtype=torch.float32)\n",
    "test_deit_tensor  = torch.tensor(test_features_deit, dtype=torch.float32)\n",
    "\n",
    "train_ft_tensor = torch.tensor(train_features_ft, dtype=torch.float32)\n",
    "val_ft_tensor   = torch.tensor(val_features_ft, dtype=torch.float32)\n",
    "test_ft_tensor  = torch.tensor(test_features_ft, dtype=torch.float32)\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels_tensor   = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_labels_tensor  = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(train_deit_tensor, train_ft_tensor, train_labels_tensor)\n",
    "val_dataset   = TensorDataset(val_deit_tensor, val_ft_tensor, val_labels_tensor)\n",
    "test_dataset  = TensorDataset(test_deit_tensor, test_ft_tensor, test_labels_tensor)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##############################################\n",
    "# 5. Training Setup\n",
    "##############################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "early_stopping_patience = 10\n",
    "epochs_no_improve = 0\n",
    "best_val_loss = np.Inf\n",
    "num_epochs = 100\n",
    "\n",
    "##############################################\n",
    "# 6. Training Loop with Validation and Early Stopping\n",
    "##############################################\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_train_correct = 0\n",
    "    for deit_inputs, ft_inputs, labels in train_loader:\n",
    "        deit_inputs = deit_inputs.to(device)\n",
    "        ft_inputs = ft_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(deit_inputs, ft_inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item() * labels.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total_train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    train_loss = total_train_loss / len(train_dataset)\n",
    "    train_accuracy = total_train_correct.double() / len(train_dataset)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    total_val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for deit_inputs, ft_inputs, labels in val_loader:\n",
    "            deit_inputs = deit_inputs.to(device)\n",
    "            ft_inputs = ft_inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(deit_inputs, ft_inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item() * labels.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    val_loss = total_val_loss / len(val_dataset)\n",
    "    val_accuracy = total_val_correct.double() / len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f} \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "##############################################\n",
    "# 7. Evaluation on the Test Set\n",
    "##############################################\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "total_test_correct = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for deit_inputs, ft_inputs, labels in test_loader:\n",
    "        deit_inputs = deit_inputs.to(device)\n",
    "        ft_inputs = ft_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(deit_inputs, ft_inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total_test_correct += torch.sum(preds == labels.data)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_accuracy = total_test_correct.double() / len(test_dataset)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "##############################################\n",
    "# 8. Margin of Error Calculation for Test Accuracy\n",
    "##############################################\n",
    "# Convert test accuracy to float\n",
    "acc = float(test_accuracy)\n",
    "\n",
    "# Approach 1: Direct Calculation using the binomial formula\n",
    "n_test = len(test_dataset)\n",
    "se = math.sqrt(acc * (1 - acc) / n_test)\n",
    "moe = 1.96 * se  # 95% CI using z-value ~1.96\n",
    "\n",
    "print(\"\\n--- Margin of Error Calculation: Direct Method ---\")\n",
    "print(f\"Standard Error: {se:.4f}\")\n",
    "print(f\"Margin of Error (95% CI): {moe:.4f}\")\n",
    "print(f\"95% Confidence Interval for Accuracy: [{acc - moe:.4f}, {acc + moe:.4f}]\")\n",
    "\n",
    "# Approach 2: Bootstrapping\n",
    "n_bootstrap = 1000\n",
    "boot_acc = []\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "n_samples = len(all_labels)\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_samples), n_samples, replace=True)\n",
    "    boot_accuracy = accuracy_score(all_labels[indices], all_preds[indices])\n",
    "    boot_acc.append(boot_accuracy)\n",
    "\n",
    "boot_acc = np.array(boot_acc)\n",
    "se_boot = np.std(boot_acc)\n",
    "moe_boot = 1.96 * se_boot  # 95% CI\n",
    "\n",
    "print(\"\\n--- Margin of Error Calculation: Bootstrapping ---\")\n",
    "print(f\"Bootstrapped Mean Accuracy: {np.mean(boot_acc):.4f}\")\n",
    "print(f\"Standard Error (Bootstrap): {se_boot:.4f}\")\n",
    "print(f\"Margin of Error (Bootstrap, 95% CI): {moe_boot:.4f}\")\n",
    "\n",
    "# 8. Compute the Margin of Error for Weighted F1 Score\n",
    "##########################################\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Compute the Weighted F1 Score on the test set.\n",
    "weighted_f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "print(\"\\nWeighted F1 Score:\", weighted_f1)\n",
    "\n",
    "# Bootstrapping for Weighted F1 Score.\n",
    "boot_f1s = []\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_samples), n_samples, replace=True)\n",
    "    f1_bs = f1_score(np.array(all_targets)[indices], np.array(all_preds)[indices], average='weighted')\n",
    "    boot_f1s.append(f1_bs)\n",
    "boot_f1s = np.array(boot_f1s)\n",
    "se_boot_f1 = np.std(boot_f1s)\n",
    "moe_boot_f1 = 1.96 * se_boot_f1\n",
    "print(\"\\nBootstrapping for Weighted F1 Score:\")\n",
    "print(\"Bootstrapped Mean Weighted F1:\", np.mean(boot_f1s))\n",
    "print(\"Standard Error (Bootstrap) for Weighted F1:\", se_boot_f1)\n",
    "print(\"Margin of Error (Bootstrap, 95% CI) for Weighted F1:\", moe_boot_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f663bd4d-0962-4950-bff7-184b19f6cf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Classes: ['AD' 'CN' 'MCI']\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 126\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Create TensorDatasets.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(train_features, train_labels)\n\u001b[0;32m--> 126\u001b[0m val_dataset   \u001b[38;5;241m=\u001b[39m \u001b[43mTensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m test_dataset  \u001b[38;5;241m=\u001b[39m TensorDataset(test_features, test_labels)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Create DataLoaders.\u001b[39;00m\n",
      "File \u001b[0;32m~/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/torch/utils/data/dataset.py:205\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    206\u001b[0m         tensors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors\n\u001b[1;32m    207\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "\u001b[0;31mAssertionError\u001b[0m: Size mismatch between tensors"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import math\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Check device.\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Concatenate Features\n",
    "##########################################\n",
    "# Load DeiT features (expected shape: [num_samples, 768])\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")\n",
    "\n",
    "# Load FTTransformer features (expected shape: [num_samples, 192])\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")\n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "# Load CSV labels\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"train_data_3.csv\")  # Use train_data_3.csv for validation (adjust if needed)\n",
    "test_data  = pd.read_csv(\"train_data_3.csv\")  # Adjust if using a different file for test\n",
    "\n",
    "label_col = \"Group\"\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "\n",
    "# Initialize scalers for each modality\n",
    "scaler_deit = StandardScaler()\n",
    "scaler_ft = StandardScaler()\n",
    "\n",
    "# Fit scalers on training data and transform\n",
    "train_features_deit = scaler_deit.fit_transform(deit_train)\n",
    "val_features_deit = scaler_deit.transform(deit_val)\n",
    "test_features_deit = scaler_deit.transform(deit_test)\n",
    "\n",
    "train_features_ft = scaler_ft.fit_transform(ft_train)\n",
    "val_features_ft = scaler_ft.transform(ft_val)\n",
    "test_features_ft = scaler_ft.transform(ft_test)\n",
    "\n",
    "# 4. Define the Concatenation Fusion Model\n",
    "class ConcatenationFusionModel(nn.Module):\n",
    "    def __init__(self, deit_input_size, ft_input_size, num_classes):\n",
    "        super(ConcatenationFusionModel, self).__init__()\n",
    "        # DeiT branch (image modality)\n",
    "        self.deit_fc = nn.Sequential(\n",
    "            nn.Linear(deit_input_size, 308),   # Dimensionality reduction from 768 to 384\n",
    "            nn.BatchNorm1d(308),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        # FTTransformer branch (tabular modality)\n",
    "        self.ft_fc = nn.Sequential(\n",
    "            nn.Linear(ft_input_size, 128),   # Dimensionality reduction from 192 to 128\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        # Classifier: concatenated features from both branches (384+128=512)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(436, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, deit_input, ft_input):\n",
    "        # Process each modality separately\n",
    "        deit_output = self.deit_fc(deit_input)    # (batch_size, 384)\n",
    "        ft_output = self.ft_fc(ft_input)           # (batch_size, 128)\n",
    "        # Concatenate features from both modalities\n",
    "        concatenated_features = torch.cat([deit_output, ft_output], dim=1)  # (batch_size, 512)\n",
    "        # Classify\n",
    "        logits = self.classifier(concatenated_features)  # (batch_size, num_classes)\n",
    "        return logits\n",
    "\n",
    "# Initialize the model.\n",
    "image_input_size = train_features_deit.shape[1]  # e.g., 768\n",
    "tab_input_size = train_features_ft.shape[1]        # e.g., 192\n",
    "\n",
    "model = ConcatenationFusionModel(image_input_size, tab_input_size, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "##########################################\n",
    "# 3. Create PyTorch Datasets and DataLoaders\n",
    "##########################################\n",
    "# Convert features and labels to tensors.\n",
    "train_features = torch.tensor(np.concatenate((train_features_deit, train_features_ft), axis=1), dtype=torch.float32)\n",
    "val_features   = torch.tensor(np.concatenate((val_features_deit, val_features_ft), axis=1), dtype=torch.float32)\n",
    "test_features  = torch.tensor(np.concatenate((test_features_deit, test_features_ft), axis=1), dtype=torch.float32)\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels   = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_labels  = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets.\n",
    "train_dataset = TensorDataset(train_features, train_labels)\n",
    "val_dataset   = TensorDataset(val_features, val_labels)\n",
    "test_dataset  = TensorDataset(test_features, test_labels)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Training Setup\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "early_stopping_patience = 10\n",
    "epochs_no_improve = 0\n",
    "best_val_loss = np.Inf\n",
    "num_epochs = 100\n",
    "\n",
    "##########################################\n",
    "# 5. Training Loop with Early Stopping\n",
    "##########################################\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_train_correct = 0\n",
    "    for deit_inputs, ft_inputs, labels in train_loader:\n",
    "        deit_inputs = deit_inputs.to(device)\n",
    "        ft_inputs = ft_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(deit_inputs, ft_inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item() * labels.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total_train_correct += torch.sum(preds == labels.data)\n",
    "    \n",
    "    train_loss = total_train_loss / len(train_dataset)\n",
    "    train_accuracy = total_train_correct.double() / len(train_dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    total_val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for deit_inputs, ft_inputs, labels in val_loader:\n",
    "            deit_inputs = deit_inputs.to(device)\n",
    "            ft_inputs = ft_inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(deit_inputs, ft_inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item() * labels.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_val_correct += torch.sum(preds == labels.data)\n",
    "    \n",
    "    val_loss = total_val_loss / len(val_dataset)\n",
    "    val_accuracy = total_val_correct.double() / len(val_dataset)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f} | \" +\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model state.\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model.eval()\n",
    "total_test_correct = 0\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for deit_inputs, ft_inputs, labels in test_loader:\n",
    "        deit_inputs = deit_inputs.to(device)\n",
    "        ft_inputs = ft_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(deit_inputs, ft_inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        total_test_correct += torch.sum(preds == labels.data)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_targets, all_preds))\n",
    "\n",
    "##########################################\n",
    "# 7. Compute the Margin of Error for Test Accuracy\n",
    "##########################################\n",
    "# Approach 1: Direct Calculation using the binomial formula.\n",
    "n_test = len(test_dataset)\n",
    "se_accuracy = math.sqrt(test_acc * (1 - test_acc) / n_test)\n",
    "moe_accuracy = 1.96 * se_accuracy\n",
    "print(\"\\nDirect Calculation for Accuracy:\")\n",
    "print(\"Standard Error:\", se_accuracy)\n",
    "print(\"Margin of Error (95% CI):\", moe_accuracy)\n",
    "print(\"95% Confidence Interval for Accuracy: [{:.4f}, {:.4f}]\".format(test_acc - moe_accuracy, test_acc + moe_accuracy))\n",
    "\n",
    "# Approach 2: Bootstrapping for Accuracy.\n",
    "n_bootstrap = 1000\n",
    "boot_acc = []\n",
    "n_samples = len(all_targets)\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_samples), n_samples, replace=True)\n",
    "    boot_accuracy = accuracy_score(np.array(all_targets)[indices], np.array(all_preds)[indices])\n",
    "    boot_acc.append(boot_accuracy)\n",
    "boot_acc = np.array(boot_acc)\n",
    "se_boot = np.std(boot_acc)\n",
    "moe_boot = 1.96 * se_boot\n",
    "print(\"\\nBootstrapping for Accuracy:\")\n",
    "print(\"Bootstrapped Mean Accuracy:\", np.mean(boot_acc))\n",
    "print(\"Standard Error (Bootstrap):\", se_boot)\n",
    "print(\"Margin of Error (Bootstrap, 95% CI):\", moe_boot)\n",
    "\n",
    "##########################################\n",
    "# 8. Compute the Margin of Error for Weighted F1 Score\n",
    "##########################################\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Compute the Weighted F1 Score on the test set.\n",
    "weighted_f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "print(\"\\nWeighted F1 Score:\", weighted_f1)\n",
    "\n",
    "# Bootstrapping for Weighted F1 Score.\n",
    "boot_f1s = []\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_samples), n_samples, replace=True)\n",
    "    f1_bs = f1_score(np.array(all_targets)[indices], np.array(all_preds)[indices], average='weighted')\n",
    "    boot_f1s.append(f1_bs)\n",
    "boot_f1s = np.array(boot_f1s)\n",
    "se_boot_f1 = np.std(boot_f1s)\n",
    "moe_boot_f1 = 1.96 * se_boot_f1\n",
    "print(\"\\nBootstrapping for Weighted F1 Score:\")\n",
    "print(\"Bootstrapped Mean Weighted F1:\", np.mean(boot_f1s))\n",
    "print(\"Standard Error (Bootstrap) for Weighted F1:\", se_boot_f1)\n",
    "print(\"Margin of Error (Bootstrap, 95% CI) for Weighted F1:\", moe_boot_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04c58a51-b35e-4d72-85ea-c028656e1dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Classes: ['AD' 'CN' 'MCI']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Train Loss: 1.0254, Train Acc: 0.4729 Val Loss: 0.9001, Val Acc: 0.7587\n",
      "Best model saved.\n",
      "Epoch [2/100] Train Loss: 0.8606, Train Acc: 0.6617 Val Loss: 0.7841, Val Acc: 0.8430\n",
      "Best model saved.\n",
      "Epoch [3/100] Train Loss: 0.7354, Train Acc: 0.7751 Val Loss: 0.6766, Val Acc: 0.8517\n",
      "Best model saved.\n",
      "Epoch [4/100] Train Loss: 0.6467, Train Acc: 0.8224 Val Loss: 0.6163, Val Acc: 0.8488\n",
      "Best model saved.\n",
      "Epoch [5/100] Train Loss: 0.5909, Train Acc: 0.8293 Val Loss: 0.5412, Val Acc: 0.8576\n",
      "Best model saved.\n",
      "Epoch [6/100] Train Loss: 0.5464, Train Acc: 0.8505 Val Loss: 0.4938, Val Acc: 0.8547\n",
      "Best model saved.\n",
      "Epoch [7/100] Train Loss: 0.4950, Train Acc: 0.8679 Val Loss: 0.4852, Val Acc: 0.8547\n",
      "Best model saved.\n",
      "Epoch [8/100] Train Loss: 0.4632, Train Acc: 0.8760 Val Loss: 0.4881, Val Acc: 0.8547\n",
      "Epoch [9/100] Train Loss: 0.4428, Train Acc: 0.8766 Val Loss: 0.4319, Val Acc: 0.8517\n",
      "Best model saved.\n",
      "Epoch [10/100] Train Loss: 0.4066, Train Acc: 0.8810 Val Loss: 0.4142, Val Acc: 0.8517\n",
      "Best model saved.\n",
      "Epoch [11/100] Train Loss: 0.3917, Train Acc: 0.8804 Val Loss: 0.3843, Val Acc: 0.8576\n",
      "Best model saved.\n",
      "Epoch [12/100] Train Loss: 0.3854, Train Acc: 0.8860 Val Loss: 0.3832, Val Acc: 0.8605\n",
      "Best model saved.\n",
      "Epoch [13/100] Train Loss: 0.3692, Train Acc: 0.8941 Val Loss: 0.3807, Val Acc: 0.8576\n",
      "Best model saved.\n",
      "Epoch [14/100] Train Loss: 0.3730, Train Acc: 0.8872 Val Loss: 0.3622, Val Acc: 0.8576\n",
      "Best model saved.\n",
      "Epoch [15/100] Train Loss: 0.3574, Train Acc: 0.8879 Val Loss: 0.3684, Val Acc: 0.8576\n",
      "Epoch [16/100] Train Loss: 0.3654, Train Acc: 0.8910 Val Loss: 0.3540, Val Acc: 0.8605\n",
      "Best model saved.\n",
      "Epoch [17/100] Train Loss: 0.3433, Train Acc: 0.8947 Val Loss: 0.3456, Val Acc: 0.8721\n",
      "Best model saved.\n",
      "Epoch [18/100] Train Loss: 0.3281, Train Acc: 0.8953 Val Loss: 0.3488, Val Acc: 0.8692\n",
      "Epoch [19/100] Train Loss: 0.3483, Train Acc: 0.8866 Val Loss: 0.3375, Val Acc: 0.8634\n",
      "Best model saved.\n",
      "Epoch [20/100] Train Loss: 0.3158, Train Acc: 0.9034 Val Loss: 0.3333, Val Acc: 0.8634\n",
      "Best model saved.\n",
      "Epoch [21/100] Train Loss: 0.3143, Train Acc: 0.9028 Val Loss: 0.3240, Val Acc: 0.8663\n",
      "Best model saved.\n",
      "Epoch [22/100] Train Loss: 0.2997, Train Acc: 0.9078 Val Loss: 0.3134, Val Acc: 0.8721\n",
      "Best model saved.\n",
      "Epoch [23/100] Train Loss: 0.2983, Train Acc: 0.9097 Val Loss: 0.3228, Val Acc: 0.8692\n",
      "Epoch [24/100] Train Loss: 0.2962, Train Acc: 0.9034 Val Loss: 0.3108, Val Acc: 0.8750\n",
      "Best model saved.\n",
      "Epoch [25/100] Train Loss: 0.2965, Train Acc: 0.9016 Val Loss: 0.3105, Val Acc: 0.8634\n",
      "Best model saved.\n",
      "Epoch [26/100] Train Loss: 0.2929, Train Acc: 0.9078 Val Loss: 0.3040, Val Acc: 0.8808\n",
      "Best model saved.\n",
      "Epoch [27/100] Train Loss: 0.2903, Train Acc: 0.9047 Val Loss: 0.3007, Val Acc: 0.8779\n",
      "Best model saved.\n",
      "Epoch [28/100] Train Loss: 0.2719, Train Acc: 0.9146 Val Loss: 0.2971, Val Acc: 0.8837\n",
      "Best model saved.\n",
      "Epoch [29/100] Train Loss: 0.2653, Train Acc: 0.9128 Val Loss: 0.2993, Val Acc: 0.8837\n",
      "Epoch [30/100] Train Loss: 0.2637, Train Acc: 0.9190 Val Loss: 0.2916, Val Acc: 0.8924\n",
      "Best model saved.\n",
      "Epoch [31/100] Train Loss: 0.2658, Train Acc: 0.9121 Val Loss: 0.2840, Val Acc: 0.8837\n",
      "Best model saved.\n",
      "Epoch [32/100] Train Loss: 0.2517, Train Acc: 0.9246 Val Loss: 0.2879, Val Acc: 0.8895\n",
      "Epoch [33/100] Train Loss: 0.2636, Train Acc: 0.9146 Val Loss: 0.2759, Val Acc: 0.8924\n",
      "Best model saved.\n",
      "Epoch [34/100] Train Loss: 0.2465, Train Acc: 0.9190 Val Loss: 0.2820, Val Acc: 0.8924\n",
      "Epoch [35/100] Train Loss: 0.2292, Train Acc: 0.9240 Val Loss: 0.2777, Val Acc: 0.8866\n",
      "Epoch [36/100] Train Loss: 0.2357, Train Acc: 0.9277 Val Loss: 0.2855, Val Acc: 0.8924\n",
      "Epoch [37/100] Train Loss: 0.2326, Train Acc: 0.9308 Val Loss: 0.2749, Val Acc: 0.8953\n",
      "Best model saved.\n",
      "Epoch [38/100] Train Loss: 0.2042, Train Acc: 0.9383 Val Loss: 0.2741, Val Acc: 0.9012\n",
      "Best model saved.\n",
      "Epoch [39/100] Train Loss: 0.2153, Train Acc: 0.9389 Val Loss: 0.2676, Val Acc: 0.8953\n",
      "Best model saved.\n",
      "Epoch [40/100] Train Loss: 0.2147, Train Acc: 0.9252 Val Loss: 0.2693, Val Acc: 0.8924\n",
      "Epoch [41/100] Train Loss: 0.2299, Train Acc: 0.9290 Val Loss: 0.2666, Val Acc: 0.8983\n",
      "Best model saved.\n",
      "Epoch [42/100] Train Loss: 0.2043, Train Acc: 0.9427 Val Loss: 0.2687, Val Acc: 0.8953\n",
      "Epoch [43/100] Train Loss: 0.2074, Train Acc: 0.9377 Val Loss: 0.2633, Val Acc: 0.9012\n",
      "Best model saved.\n",
      "Epoch [44/100] Train Loss: 0.2130, Train Acc: 0.9371 Val Loss: 0.2602, Val Acc: 0.9070\n",
      "Best model saved.\n",
      "Epoch [45/100] Train Loss: 0.2213, Train Acc: 0.9290 Val Loss: 0.2705, Val Acc: 0.9012\n",
      "Epoch [46/100] Train Loss: 0.1940, Train Acc: 0.9396 Val Loss: 0.2661, Val Acc: 0.9012\n",
      "Epoch [47/100] Train Loss: 0.1944, Train Acc: 0.9364 Val Loss: 0.2657, Val Acc: 0.9070\n",
      "Epoch [48/100] Train Loss: 0.1980, Train Acc: 0.9427 Val Loss: 0.2638, Val Acc: 0.9041\n",
      "Epoch [49/100] Train Loss: 0.1946, Train Acc: 0.9389 Val Loss: 0.2621, Val Acc: 0.9012\n",
      "Epoch [50/100] Train Loss: 0.1835, Train Acc: 0.9483 Val Loss: 0.2615, Val Acc: 0.9041\n",
      "Epoch [51/100] Train Loss: 0.1933, Train Acc: 0.9458 Val Loss: 0.2593, Val Acc: 0.9070\n",
      "Best model saved.\n",
      "Epoch [52/100] Train Loss: 0.1858, Train Acc: 0.9396 Val Loss: 0.2600, Val Acc: 0.9041\n",
      "Epoch [53/100] Train Loss: 0.1997, Train Acc: 0.9421 Val Loss: 0.2616, Val Acc: 0.9070\n",
      "Epoch [54/100] Train Loss: 0.1893, Train Acc: 0.9421 Val Loss: 0.2633, Val Acc: 0.9041\n",
      "Epoch [55/100] Train Loss: 0.1842, Train Acc: 0.9421 Val Loss: 0.2619, Val Acc: 0.9012\n",
      "Epoch [56/100] Train Loss: 0.1878, Train Acc: 0.9464 Val Loss: 0.2624, Val Acc: 0.9041\n",
      "Epoch [57/100] Train Loss: 0.1865, Train Acc: 0.9483 Val Loss: 0.2636, Val Acc: 0.9041\n",
      "Epoch [58/100] Train Loss: 0.1774, Train Acc: 0.9445 Val Loss: 0.2589, Val Acc: 0.9070\n",
      "Best model saved.\n",
      "Epoch [59/100] Train Loss: 0.1819, Train Acc: 0.9427 Val Loss: 0.2622, Val Acc: 0.9041\n",
      "Epoch [60/100] Train Loss: 0.1823, Train Acc: 0.9508 Val Loss: 0.2596, Val Acc: 0.9099\n",
      "Epoch [61/100] Train Loss: 0.1900, Train Acc: 0.9408 Val Loss: 0.2582, Val Acc: 0.9041\n",
      "Best model saved.\n",
      "Epoch [62/100] Train Loss: 0.1965, Train Acc: 0.9371 Val Loss: 0.2630, Val Acc: 0.9041\n",
      "Epoch [63/100] Train Loss: 0.1710, Train Acc: 0.9539 Val Loss: 0.2644, Val Acc: 0.8924\n",
      "Epoch [64/100] Train Loss: 0.1884, Train Acc: 0.9470 Val Loss: 0.2632, Val Acc: 0.9012\n",
      "Epoch [65/100] Train Loss: 0.1811, Train Acc: 0.9489 Val Loss: 0.2606, Val Acc: 0.9041\n",
      "Epoch [66/100] Train Loss: 0.1833, Train Acc: 0.9433 Val Loss: 0.2617, Val Acc: 0.9070\n",
      "Epoch [67/100] Train Loss: 0.1918, Train Acc: 0.9445 Val Loss: 0.2713, Val Acc: 0.8895\n",
      "Epoch [68/100] Train Loss: 0.1908, Train Acc: 0.9402 Val Loss: 0.2594, Val Acc: 0.9070\n",
      "Epoch [69/100] Train Loss: 0.2085, Train Acc: 0.9458 Val Loss: 0.2682, Val Acc: 0.8924\n",
      "Epoch [70/100] Train Loss: 0.1887, Train Acc: 0.9396 Val Loss: 0.2646, Val Acc: 0.9012\n",
      "Epoch [71/100] Train Loss: 0.1950, Train Acc: 0.9458 Val Loss: 0.2593, Val Acc: 0.9041\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 0.9246\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.87      0.85      0.86        72\n",
      "          CN       0.99      0.95      0.97       106\n",
      "         MCI       0.91      0.94      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.92      0.91      0.92       345\n",
      "weighted avg       0.93      0.92      0.92       345\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 61   0  11]\n",
      " [  0 101   5]\n",
      " [  9   1 157]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115198/265524843.py:221: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load DeiT features (expected shape: [num_samples, 768])\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")\n",
    "\n",
    "# Load FTTransformer features (expected shape: [num_samples, 192])\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")\n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "# Load CSV labels\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "\n",
    "# Initialize scalers for each modality\n",
    "scaler_deit = StandardScaler()\n",
    "scaler_ft = StandardScaler()\n",
    "\n",
    "# Fit scalers on training data and transform\n",
    "train_features_deit = scaler_deit.fit_transform(deit_train)\n",
    "val_features_deit = scaler_deit.transform(deit_val)\n",
    "test_features_deit = scaler_deit.transform(deit_test)\n",
    "\n",
    "train_features_ft = scaler_ft.fit_transform(ft_train)\n",
    "val_features_ft = scaler_ft.transform(ft_val)\n",
    "test_features_ft = scaler_ft.transform(ft_test)\n",
    "\n",
    "# 4. Define the Concatenation Fusion Model\n",
    "class ConcatenationFusionModel(nn.Module):\n",
    "    def __init__(self, deit_input_size, ft_input_size, num_classes):\n",
    "        super(ConcatenationFusionModel, self).__init__()\n",
    "        # DeiT branch (image modality)\n",
    "        self.deit_fc = nn.Sequential(\n",
    "            nn.Linear(deit_input_size, 308),   # Dimensionality reduction from 768 to 384\n",
    "            nn.BatchNorm1d(308),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        # FTTransformer branch (tabular modality)\n",
    "        self.ft_fc = nn.Sequential(\n",
    "            nn.Linear(ft_input_size, 128),   # Dimensionality reduction from 192 to 128\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        # Classifier: concatenated features from both branches (384+128=512)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(436, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, deit_input, ft_input):\n",
    "        # Process each modality separately\n",
    "        deit_output = self.deit_fc(deit_input)    # (batch_size, 384)\n",
    "        ft_output = self.ft_fc(ft_input)           # (batch_size, 128)\n",
    "        # Concatenate features from both modalities\n",
    "        concatenated_features = torch.cat([deit_output, ft_output], dim=1)  # (batch_size, 512)\n",
    "        # Classify\n",
    "        logits = self.classifier(concatenated_features)  # (batch_size, num_classes)\n",
    "        return logits\n",
    "\n",
    "# Initialize the model\n",
    "image_input_size = train_features_deit.shape[1]  # 768\n",
    "tab_input_size = train_features_ft.shape[1]        # 192\n",
    "\n",
    "model = ConcatenationFusionModel(image_input_size, tab_input_size, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# 5. Prepare the data loaders\n",
    "# Convert to tensors\n",
    "train_deit_tensor = torch.tensor(train_features_deit, dtype=torch.float32)\n",
    "val_deit_tensor = torch.tensor(val_features_deit, dtype=torch.float32)\n",
    "test_deit_tensor = torch.tensor(test_features_deit, dtype=torch.float32)\n",
    "\n",
    "train_ft_tensor = torch.tensor(train_features_ft, dtype=torch.float32)\n",
    "val_ft_tensor = torch.tensor(val_features_ft, dtype=torch.float32)\n",
    "test_ft_tensor = torch.tensor(test_features_ft, dtype=torch.float32)\n",
    "\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(train_deit_tensor, train_ft_tensor, train_labels_tensor)\n",
    "val_dataset = TensorDataset(val_deit_tensor, val_ft_tensor, val_labels_tensor)\n",
    "test_dataset = TensorDataset(test_deit_tensor, test_ft_tensor, test_labels_tensor)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 6. Training Setup\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_patience = 10\n",
    "epochs_no_improve = 0\n",
    "best_val_loss = np.Inf\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# 7. Training Loop with Validation and Early Stopping mechanism\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_train_correct = 0\n",
    "    for deit_inputs, ft_inputs, labels in train_loader:\n",
    "        deit_inputs = deit_inputs.to(device)\n",
    "        ft_inputs = ft_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(deit_inputs, ft_inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item() * labels.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total_train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    train_loss = total_train_loss / len(train_dataset)\n",
    "    train_accuracy = total_train_correct.double() / len(train_dataset)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    total_val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for deit_inputs, ft_inputs, labels in val_loader:\n",
    "            deit_inputs = deit_inputs.to(device)\n",
    "            ft_inputs = ft_inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(deit_inputs, ft_inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_val_loss += loss.item() * labels.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    val_loss = total_val_loss / len(val_dataset)\n",
    "    val_accuracy = total_val_correct.double() / len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f} \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# 8. Evaluate the Model on the Test Set\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "total_test_correct = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for deit_inputs, ft_inputs, labels in test_loader:\n",
    "        deit_inputs = deit_inputs.to(device)\n",
    "        ft_inputs = ft_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(deit_inputs, ft_inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        total_test_correct += torch.sum(preds == labels.data)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_accuracy = total_test_correct.double() / len(test_dataset)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ac27d-4be2-4eb2-ba1e-e6fa8215ae33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f065f3f-14c9-433f-b0d7-aedac88368ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Classes: ['AD' 'CN' 'MCI']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Train Loss: 1.0254, Train Acc: 0.4729 Val Loss: 0.9001, Val Acc: 0.7587\n",
      "Best model saved.\n",
      "Epoch [2/100] Train Loss: 0.8606, Train Acc: 0.6617 Val Loss: 0.7841, Val Acc: 0.8430\n",
      "Best model saved.\n",
      "Epoch [3/100] Train Loss: 0.7354, Train Acc: 0.7751 Val Loss: 0.6766, Val Acc: 0.8517\n",
      "Best model saved.\n",
      "Epoch [4/100] Train Loss: 0.6467, Train Acc: 0.8224 Val Loss: 0.6163, Val Acc: 0.8488\n",
      "Best model saved.\n",
      "Epoch [5/100] Train Loss: 0.5909, Train Acc: 0.8293 Val Loss: 0.5412, Val Acc: 0.8576\n",
      "Best model saved.\n",
      "Epoch [6/100] Train Loss: 0.5464, Train Acc: 0.8505 Val Loss: 0.4938, Val Acc: 0.8547\n",
      "Best model saved.\n",
      "Epoch [7/100] Train Loss: 0.4950, Train Acc: 0.8679 Val Loss: 0.4852, Val Acc: 0.8547\n",
      "Best model saved.\n",
      "Epoch [8/100] Train Loss: 0.4632, Train Acc: 0.8760 Val Loss: 0.4881, Val Acc: 0.8547\n",
      "Epoch [9/100] Train Loss: 0.4428, Train Acc: 0.8766 Val Loss: 0.4319, Val Acc: 0.8517\n",
      "Best model saved.\n",
      "Epoch [10/100] Train Loss: 0.4066, Train Acc: 0.8810 Val Loss: 0.4142, Val Acc: 0.8517\n",
      "Best model saved.\n",
      "Epoch [11/100] Train Loss: 0.3917, Train Acc: 0.8804 Val Loss: 0.3843, Val Acc: 0.8576\n",
      "Best model saved.\n",
      "Epoch [12/100] Train Loss: 0.3854, Train Acc: 0.8860 Val Loss: 0.3832, Val Acc: 0.8605\n",
      "Best model saved.\n",
      "Epoch [13/100] Train Loss: 0.3692, Train Acc: 0.8941 Val Loss: 0.3807, Val Acc: 0.8576\n",
      "Best model saved.\n",
      "Epoch [14/100] Train Loss: 0.3730, Train Acc: 0.8872 Val Loss: 0.3622, Val Acc: 0.8576\n",
      "Best model saved.\n",
      "Epoch [15/100] Train Loss: 0.3574, Train Acc: 0.8879 Val Loss: 0.3684, Val Acc: 0.8576\n",
      "Epoch [16/100] Train Loss: 0.3654, Train Acc: 0.8910 Val Loss: 0.3540, Val Acc: 0.8605\n",
      "Best model saved.\n",
      "Epoch [17/100] Train Loss: 0.3433, Train Acc: 0.8947 Val Loss: 0.3456, Val Acc: 0.8721\n",
      "Best model saved.\n",
      "Epoch [18/100] Train Loss: 0.3281, Train Acc: 0.8953 Val Loss: 0.3488, Val Acc: 0.8692\n",
      "Epoch [19/100] Train Loss: 0.3483, Train Acc: 0.8866 Val Loss: 0.3375, Val Acc: 0.8634\n",
      "Best model saved.\n",
      "Epoch [20/100] Train Loss: 0.3158, Train Acc: 0.9034 Val Loss: 0.3333, Val Acc: 0.8634\n",
      "Best model saved.\n",
      "Epoch [21/100] Train Loss: 0.3143, Train Acc: 0.9028 Val Loss: 0.3240, Val Acc: 0.8663\n",
      "Best model saved.\n",
      "Epoch [22/100] Train Loss: 0.2997, Train Acc: 0.9078 Val Loss: 0.3134, Val Acc: 0.8721\n",
      "Best model saved.\n",
      "Epoch [23/100] Train Loss: 0.2983, Train Acc: 0.9097 Val Loss: 0.3228, Val Acc: 0.8692\n",
      "Epoch [24/100] Train Loss: 0.2962, Train Acc: 0.9034 Val Loss: 0.3108, Val Acc: 0.8750\n",
      "Best model saved.\n",
      "Epoch [25/100] Train Loss: 0.2965, Train Acc: 0.9016 Val Loss: 0.3105, Val Acc: 0.8634\n",
      "Best model saved.\n",
      "Epoch [26/100] Train Loss: 0.2929, Train Acc: 0.9078 Val Loss: 0.3040, Val Acc: 0.8808\n",
      "Best model saved.\n",
      "Epoch [27/100] Train Loss: 0.2903, Train Acc: 0.9047 Val Loss: 0.3007, Val Acc: 0.8779\n",
      "Best model saved.\n",
      "Epoch [28/100] Train Loss: 0.2719, Train Acc: 0.9146 Val Loss: 0.2971, Val Acc: 0.8837\n",
      "Best model saved.\n",
      "Epoch [29/100] Train Loss: 0.2653, Train Acc: 0.9128 Val Loss: 0.2993, Val Acc: 0.8837\n",
      "Epoch [30/100] Train Loss: 0.2637, Train Acc: 0.9190 Val Loss: 0.2916, Val Acc: 0.8924\n",
      "Best model saved.\n",
      "Epoch [31/100] Train Loss: 0.2658, Train Acc: 0.9121 Val Loss: 0.2840, Val Acc: 0.8837\n",
      "Best model saved.\n",
      "Epoch [32/100] Train Loss: 0.2517, Train Acc: 0.9246 Val Loss: 0.2879, Val Acc: 0.8895\n",
      "Epoch [33/100] Train Loss: 0.2636, Train Acc: 0.9146 Val Loss: 0.2759, Val Acc: 0.8924\n",
      "Best model saved.\n",
      "Epoch [34/100] Train Loss: 0.2465, Train Acc: 0.9190 Val Loss: 0.2820, Val Acc: 0.8924\n",
      "Epoch [35/100] Train Loss: 0.2292, Train Acc: 0.9240 Val Loss: 0.2777, Val Acc: 0.8866\n",
      "Epoch [36/100] Train Loss: 0.2357, Train Acc: 0.9277 Val Loss: 0.2855, Val Acc: 0.8924\n",
      "Epoch [37/100] Train Loss: 0.2326, Train Acc: 0.9308 Val Loss: 0.2749, Val Acc: 0.8953\n",
      "Best model saved.\n",
      "Epoch [38/100] Train Loss: 0.2042, Train Acc: 0.9383 Val Loss: 0.2741, Val Acc: 0.9012\n",
      "Best model saved.\n",
      "Epoch [39/100] Train Loss: 0.2153, Train Acc: 0.9389 Val Loss: 0.2676, Val Acc: 0.8953\n",
      "Best model saved.\n",
      "Epoch [40/100] Train Loss: 0.2147, Train Acc: 0.9252 Val Loss: 0.2693, Val Acc: 0.8924\n",
      "Epoch [41/100] Train Loss: 0.2299, Train Acc: 0.9290 Val Loss: 0.2666, Val Acc: 0.8983\n",
      "Best model saved.\n",
      "Epoch [42/100] Train Loss: 0.2043, Train Acc: 0.9427 Val Loss: 0.2687, Val Acc: 0.8953\n",
      "Epoch [43/100] Train Loss: 0.2074, Train Acc: 0.9377 Val Loss: 0.2633, Val Acc: 0.9012\n",
      "Best model saved.\n",
      "Epoch [44/100] Train Loss: 0.2130, Train Acc: 0.9371 Val Loss: 0.2602, Val Acc: 0.9070\n",
      "Best model saved.\n",
      "Epoch [45/100] Train Loss: 0.2213, Train Acc: 0.9290 Val Loss: 0.2705, Val Acc: 0.9012\n",
      "Epoch [46/100] Train Loss: 0.1940, Train Acc: 0.9396 Val Loss: 0.2661, Val Acc: 0.9012\n",
      "Epoch [47/100] Train Loss: 0.1944, Train Acc: 0.9364 Val Loss: 0.2657, Val Acc: 0.9070\n",
      "Epoch [48/100] Train Loss: 0.1980, Train Acc: 0.9427 Val Loss: 0.2638, Val Acc: 0.9041\n",
      "Epoch [49/100] Train Loss: 0.1946, Train Acc: 0.9389 Val Loss: 0.2621, Val Acc: 0.9012\n",
      "Epoch [50/100] Train Loss: 0.1835, Train Acc: 0.9483 Val Loss: 0.2615, Val Acc: 0.9041\n",
      "Epoch [51/100] Train Loss: 0.1933, Train Acc: 0.9458 Val Loss: 0.2593, Val Acc: 0.9070\n",
      "Best model saved.\n",
      "Epoch [52/100] Train Loss: 0.1858, Train Acc: 0.9396 Val Loss: 0.2600, Val Acc: 0.9041\n",
      "Epoch [53/100] Train Loss: 0.1997, Train Acc: 0.9421 Val Loss: 0.2616, Val Acc: 0.9070\n",
      "Epoch [54/100] Train Loss: 0.1893, Train Acc: 0.9421 Val Loss: 0.2633, Val Acc: 0.9041\n",
      "Epoch [55/100] Train Loss: 0.1842, Train Acc: 0.9421 Val Loss: 0.2619, Val Acc: 0.9012\n",
      "Epoch [56/100] Train Loss: 0.1878, Train Acc: 0.9464 Val Loss: 0.2624, Val Acc: 0.9041\n",
      "Epoch [57/100] Train Loss: 0.1865, Train Acc: 0.9483 Val Loss: 0.2636, Val Acc: 0.9041\n",
      "Epoch [58/100] Train Loss: 0.1774, Train Acc: 0.9445 Val Loss: 0.2589, Val Acc: 0.9070\n",
      "Best model saved.\n",
      "Epoch [59/100] Train Loss: 0.1819, Train Acc: 0.9427 Val Loss: 0.2622, Val Acc: 0.9041\n",
      "Epoch [60/100] Train Loss: 0.1823, Train Acc: 0.9508 Val Loss: 0.2596, Val Acc: 0.9099\n",
      "Epoch [61/100] Train Loss: 0.1900, Train Acc: 0.9408 Val Loss: 0.2582, Val Acc: 0.9041\n",
      "Best model saved.\n",
      "Epoch [62/100] Train Loss: 0.1965, Train Acc: 0.9371 Val Loss: 0.2630, Val Acc: 0.9041\n",
      "Epoch [63/100] Train Loss: 0.1710, Train Acc: 0.9539 Val Loss: 0.2644, Val Acc: 0.8924\n",
      "Epoch [64/100] Train Loss: 0.1884, Train Acc: 0.9470 Val Loss: 0.2632, Val Acc: 0.9012\n",
      "Epoch [65/100] Train Loss: 0.1811, Train Acc: 0.9489 Val Loss: 0.2606, Val Acc: 0.9041\n",
      "Epoch [66/100] Train Loss: 0.1833, Train Acc: 0.9433 Val Loss: 0.2617, Val Acc: 0.9070\n",
      "Epoch [67/100] Train Loss: 0.1918, Train Acc: 0.9445 Val Loss: 0.2713, Val Acc: 0.8895\n",
      "Epoch [68/100] Train Loss: 0.1908, Train Acc: 0.9402 Val Loss: 0.2594, Val Acc: 0.9070\n",
      "Epoch [69/100] Train Loss: 0.2085, Train Acc: 0.9458 Val Loss: 0.2682, Val Acc: 0.8924\n",
      "Epoch [70/100] Train Loss: 0.1887, Train Acc: 0.9396 Val Loss: 0.2646, Val Acc: 0.9012\n",
      "Epoch [71/100] Train Loss: 0.1950, Train Acc: 0.9458 Val Loss: 0.2593, Val Acc: 0.9041\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 0.9246\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.87      0.85      0.86        72\n",
      "          CN       0.99      0.95      0.97       106\n",
      "         MCI       0.91      0.94      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.92      0.91      0.92       345\n",
      "weighted avg       0.93      0.92      0.92       345\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 61   0  11]\n",
      " [  0 101   5]\n",
      " [  9   1 157]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_110455/265524843.py:221: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load DeiT features (expected shape: [num_samples, 768])\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")\n",
    "\n",
    "# Load FTTransformer features (expected shape: [num_samples, 192])\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")\n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "# Load CSV labels\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "\n",
    "# Initialize scalers for each modality\n",
    "scaler_deit = StandardScaler()\n",
    "scaler_ft = StandardScaler()\n",
    "\n",
    "# Fit scalers on training data and transform\n",
    "train_features_deit = scaler_deit.fit_transform(deit_train)\n",
    "val_features_deit = scaler_deit.transform(deit_val)\n",
    "test_features_deit = scaler_deit.transform(deit_test)\n",
    "\n",
    "train_features_ft = scaler_ft.fit_transform(ft_train)\n",
    "val_features_ft = scaler_ft.transform(ft_val)\n",
    "test_features_ft = scaler_ft.transform(ft_test)\n",
    "\n",
    "# 4. Define the Concatenation Fusion Model\n",
    "class ConcatenationFusionModel(nn.Module):\n",
    "    def __init__(self, deit_input_size, ft_input_size, num_classes):\n",
    "        super(ConcatenationFusionModel, self).__init__()\n",
    "        # DeiT branch (image modality)\n",
    "        self.deit_fc = nn.Sequential(\n",
    "            nn.Linear(deit_input_size, 308),   # Dimensionality reduction from 768 to 384\n",
    "            nn.BatchNorm1d(308),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        # FTTransformer branch (tabular modality)\n",
    "        self.ft_fc = nn.Sequential(\n",
    "            nn.Linear(ft_input_size, 128),   # Dimensionality reduction from 192 to 128\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        # Classifier: concatenated features from both branches (384+128=512)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(436, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, deit_input, ft_input):\n",
    "        # Process each modality separately\n",
    "        deit_output = self.deit_fc(deit_input)    # (batch_size, 384)\n",
    "        ft_output = self.ft_fc(ft_input)           # (batch_size, 128)\n",
    "        # Concatenate features from both modalities\n",
    "        concatenated_features = torch.cat([deit_output, ft_output], dim=1)  # (batch_size, 512)\n",
    "        # Classify\n",
    "        logits = self.classifier(concatenated_features)  # (batch_size, num_classes)\n",
    "        return logits\n",
    "\n",
    "# Initialize the model\n",
    "image_input_size = train_features_deit.shape[1]  # 768\n",
    "tab_input_size = train_features_ft.shape[1]        # 192\n",
    "\n",
    "model = ConcatenationFusionModel(image_input_size, tab_input_size, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# 5. Prepare the data loaders\n",
    "# Convert to tensors\n",
    "train_deit_tensor = torch.tensor(train_features_deit, dtype=torch.float32)\n",
    "val_deit_tensor = torch.tensor(val_features_deit, dtype=torch.float32)\n",
    "test_deit_tensor = torch.tensor(test_features_deit, dtype=torch.float32)\n",
    "\n",
    "train_ft_tensor = torch.tensor(train_features_ft, dtype=torch.float32)\n",
    "val_ft_tensor = torch.tensor(val_features_ft, dtype=torch.float32)\n",
    "test_ft_tensor = torch.tensor(test_features_ft, dtype=torch.float32)\n",
    "\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(train_deit_tensor, train_ft_tensor, train_labels_tensor)\n",
    "val_dataset = TensorDataset(val_deit_tensor, val_ft_tensor, val_labels_tensor)\n",
    "test_dataset = TensorDataset(test_deit_tensor, test_ft_tensor, test_labels_tensor)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 6. Training Setup\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_patience = 10\n",
    "epochs_no_improve = 0\n",
    "best_val_loss = np.Inf\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# 7. Training Loop with Validation and Early Stopping mechanism\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_train_correct = 0\n",
    "    for deit_inputs, ft_inputs, labels in train_loader:\n",
    "        deit_inputs = deit_inputs.to(device)\n",
    "        ft_inputs = ft_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(deit_inputs, ft_inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item() * labels.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total_train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    train_loss = total_train_loss / len(train_dataset)\n",
    "    train_accuracy = total_train_correct.double() / len(train_dataset)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    total_val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for deit_inputs, ft_inputs, labels in val_loader:\n",
    "            deit_inputs = deit_inputs.to(device)\n",
    "            ft_inputs = ft_inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(deit_inputs, ft_inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_val_loss += loss.item() * labels.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    val_loss = total_val_loss / len(val_dataset)\n",
    "    val_accuracy = total_val_correct.double() / len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f} \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# 8. Evaluate the Model on the Test Set\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "total_test_correct = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for deit_inputs, ft_inputs, labels in test_loader:\n",
    "        deit_inputs = deit_inputs.to(device)\n",
    "        ft_inputs = ft_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(deit_inputs, ft_inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        total_test_correct += torch.sum(preds == labels.data)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_accuracy = total_test_correct.double() / len(test_dataset)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c576ee2f-d486-440b-9fc5-6a37563b3263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da86c2cb-12d0-4fd8-ae63-1171910f571f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53433557-dd7e-4843-b58e-058b71b5f96c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b00ec00-59d1-4ac1-bd06-250bb3e1fe5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89982be2-92f7-4e64-ad96-1ce8f628d3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9246\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.87      0.85      0.86        72\n",
      "          CN       0.99      0.95      0.97       106\n",
      "         MCI       0.91      0.94      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.92      0.91      0.92       345\n",
      "weighted avg       0.93      0.92      0.92       345\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 61   0  11]\n",
      " [  0 101   5]\n",
      " [  9   1 157]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, label_binarize\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... [Your existing imports and code remain the same] ...\n",
    "\n",
    "# After you finish evaluating on the test set and have all_preds, all_labels:\n",
    "# Instead of all_preds being only the predicted class, we also need predicted probabilities\n",
    "# for multi-class ROC. So, we collect both.\n",
    "\n",
    "model.eval()\n",
    "total_test_correct = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for deit_inputs, ft_inputs, labels in test_loader:\n",
    "        deit_inputs = deit_inputs.to(device)\n",
    "        ft_inputs = ft_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(deit_inputs, ft_inputs)  # shape: (batch_size, num_classes)\n",
    "        probs = torch.softmax(outputs, dim=1)     # convert logits to probabilities\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        total_test_correct += torch.sum(preds == labels.data)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "test_accuracy = total_test_correct.double() / len(test_dataset)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
    "\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# ------------------- ROC and AUC Calculation -------------------\n",
    "# We have num_classes=3. We'll compute a one-vs-rest ROC for each class, plus a macro-average.\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Binarize the labels for each class: shape (num_samples, num_classes)\n",
    "y_true = label_binarize(all_labels, classes=list(range(num_classes)))  # e.g., [0,1,2]\n",
    "\n",
    "# Convert all_probs to a numpy array if not already\n",
    "y_score = np.array(all_probs)  # shape: (num_samples, num_classes)\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "# Compute ROC curve and AUC for each class\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute macro-average ROC\n",
    "# First aggregate all FPRs and TPRs\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))\n",
    "\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(num_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "mean_tpr /= num_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot the ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = [\"darkorange\", \"green\", \"blue\"]\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color,\n",
    "             label=f\"Class {label_encoder.classes_[i]} (AUC = {roc_auc[i]:.3f})\")\n",
    "\n",
    "# Plot macro-average ROC\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"], color=\"red\", linestyle=\"--\",\n",
    "         label=f\"Macro-average (AUC = {roc_auc['macro']:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", linestyle=\":\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves for Mid-Fusion Approach\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3381e891-3ddf-4a46-ad53-5bcad6556996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Classes: ['AD' 'CN' 'MCI']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Train Loss: 1.0254, Train Acc: 0.4729 Val Loss: 0.9001, Val Acc: 0.7587\n",
      "Best model saved.\n",
      "Epoch [2/100] Train Loss: 0.8606, Train Acc: 0.6617 Val Loss: 0.7841, Val Acc: 0.8430\n",
      "Best model saved.\n",
      "Epoch [3/100] Train Loss: 0.7354, Train Acc: 0.7751 Val Loss: 0.6766, Val Acc: 0.8517\n",
      "Best model saved.\n",
      "Epoch [4/100] Train Loss: 0.6467, Train Acc: 0.8224 Val Loss: 0.6163, Val Acc: 0.8488\n",
      "Best model saved.\n",
      "Epoch [5/100] Train Loss: 0.5909, Train Acc: 0.8293 Val Loss: 0.5412, Val Acc: 0.8576\n",
      "Best model saved.\n",
      "Epoch [6/100] Train Loss: 0.5464, Train Acc: 0.8505 Val Loss: 0.4938, Val Acc: 0.8547\n",
      "Best model saved.\n",
      "Epoch [7/100] Train Loss: 0.4950, Train Acc: 0.8679 Val Loss: 0.4852, Val Acc: 0.8547\n",
      "Best model saved.\n",
      "Epoch [8/100] Train Loss: 0.4632, Train Acc: 0.8760 Val Loss: 0.4881, Val Acc: 0.8547\n",
      "Epoch [9/100] Train Loss: 0.4428, Train Acc: 0.8766 Val Loss: 0.4319, Val Acc: 0.8517\n",
      "Best model saved.\n",
      "Epoch [10/100] Train Loss: 0.4066, Train Acc: 0.8810 Val Loss: 0.4142, Val Acc: 0.8517\n",
      "Best model saved.\n",
      "Epoch [11/100] Train Loss: 0.3917, Train Acc: 0.8804 Val Loss: 0.3843, Val Acc: 0.8576\n",
      "Best model saved.\n",
      "Epoch [12/100] Train Loss: 0.3854, Train Acc: 0.8860 Val Loss: 0.3832, Val Acc: 0.8605\n",
      "Best model saved.\n",
      "Epoch [13/100] Train Loss: 0.3692, Train Acc: 0.8941 Val Loss: 0.3807, Val Acc: 0.8576\n",
      "Best model saved.\n",
      "Epoch [14/100] Train Loss: 0.3730, Train Acc: 0.8872 Val Loss: 0.3622, Val Acc: 0.8576\n",
      "Best model saved.\n",
      "Epoch [15/100] Train Loss: 0.3574, Train Acc: 0.8879 Val Loss: 0.3684, Val Acc: 0.8576\n",
      "Epoch [16/100] Train Loss: 0.3654, Train Acc: 0.8910 Val Loss: 0.3540, Val Acc: 0.8605\n",
      "Best model saved.\n",
      "Epoch [17/100] Train Loss: 0.3433, Train Acc: 0.8947 Val Loss: 0.3456, Val Acc: 0.8721\n",
      "Best model saved.\n",
      "Epoch [18/100] Train Loss: 0.3281, Train Acc: 0.8953 Val Loss: 0.3488, Val Acc: 0.8692\n",
      "Epoch [19/100] Train Loss: 0.3483, Train Acc: 0.8866 Val Loss: 0.3375, Val Acc: 0.8634\n",
      "Best model saved.\n",
      "Epoch [20/100] Train Loss: 0.3158, Train Acc: 0.9034 Val Loss: 0.3333, Val Acc: 0.8634\n",
      "Best model saved.\n",
      "Epoch [21/100] Train Loss: 0.3143, Train Acc: 0.9028 Val Loss: 0.3240, Val Acc: 0.8663\n",
      "Best model saved.\n",
      "Epoch [22/100] Train Loss: 0.2997, Train Acc: 0.9078 Val Loss: 0.3134, Val Acc: 0.8721\n",
      "Best model saved.\n",
      "Epoch [23/100] Train Loss: 0.2983, Train Acc: 0.9097 Val Loss: 0.3228, Val Acc: 0.8692\n",
      "Epoch [24/100] Train Loss: 0.2962, Train Acc: 0.9034 Val Loss: 0.3108, Val Acc: 0.8750\n",
      "Best model saved.\n",
      "Epoch [25/100] Train Loss: 0.2965, Train Acc: 0.9016 Val Loss: 0.3105, Val Acc: 0.8634\n",
      "Best model saved.\n",
      "Epoch [26/100] Train Loss: 0.2929, Train Acc: 0.9078 Val Loss: 0.3040, Val Acc: 0.8808\n",
      "Best model saved.\n",
      "Epoch [27/100] Train Loss: 0.2903, Train Acc: 0.9047 Val Loss: 0.3007, Val Acc: 0.8779\n",
      "Best model saved.\n",
      "Epoch [28/100] Train Loss: 0.2719, Train Acc: 0.9146 Val Loss: 0.2971, Val Acc: 0.8837\n",
      "Best model saved.\n",
      "Epoch [29/100] Train Loss: 0.2653, Train Acc: 0.9128 Val Loss: 0.2993, Val Acc: 0.8837\n",
      "Epoch [30/100] Train Loss: 0.2637, Train Acc: 0.9190 Val Loss: 0.2916, Val Acc: 0.8924\n",
      "Best model saved.\n",
      "Epoch [31/100] Train Loss: 0.2658, Train Acc: 0.9121 Val Loss: 0.2840, Val Acc: 0.8837\n",
      "Best model saved.\n",
      "Epoch [32/100] Train Loss: 0.2517, Train Acc: 0.9246 Val Loss: 0.2879, Val Acc: 0.8895\n",
      "Epoch [33/100] Train Loss: 0.2636, Train Acc: 0.9146 Val Loss: 0.2759, Val Acc: 0.8924\n",
      "Best model saved.\n",
      "Epoch [34/100] Train Loss: 0.2465, Train Acc: 0.9190 Val Loss: 0.2820, Val Acc: 0.8924\n",
      "Epoch [35/100] Train Loss: 0.2292, Train Acc: 0.9240 Val Loss: 0.2777, Val Acc: 0.8866\n",
      "Epoch [36/100] Train Loss: 0.2357, Train Acc: 0.9277 Val Loss: 0.2855, Val Acc: 0.8924\n",
      "Epoch [37/100] Train Loss: 0.2326, Train Acc: 0.9308 Val Loss: 0.2749, Val Acc: 0.8953\n",
      "Best model saved.\n",
      "Epoch [38/100] Train Loss: 0.2042, Train Acc: 0.9383 Val Loss: 0.2741, Val Acc: 0.9012\n",
      "Best model saved.\n",
      "Epoch [39/100] Train Loss: 0.2153, Train Acc: 0.9389 Val Loss: 0.2676, Val Acc: 0.8953\n",
      "Best model saved.\n",
      "Epoch [40/100] Train Loss: 0.2147, Train Acc: 0.9252 Val Loss: 0.2693, Val Acc: 0.8924\n",
      "Epoch [41/100] Train Loss: 0.2299, Train Acc: 0.9290 Val Loss: 0.2666, Val Acc: 0.8983\n",
      "Best model saved.\n",
      "Epoch [42/100] Train Loss: 0.2043, Train Acc: 0.9427 Val Loss: 0.2687, Val Acc: 0.8953\n",
      "Epoch [43/100] Train Loss: 0.2074, Train Acc: 0.9377 Val Loss: 0.2633, Val Acc: 0.9012\n",
      "Best model saved.\n",
      "Epoch [44/100] Train Loss: 0.2130, Train Acc: 0.9371 Val Loss: 0.2602, Val Acc: 0.9070\n",
      "Best model saved.\n",
      "Epoch [45/100] Train Loss: 0.2213, Train Acc: 0.9290 Val Loss: 0.2705, Val Acc: 0.9012\n",
      "Epoch [46/100] Train Loss: 0.1940, Train Acc: 0.9396 Val Loss: 0.2661, Val Acc: 0.9012\n",
      "Epoch [47/100] Train Loss: 0.1944, Train Acc: 0.9364 Val Loss: 0.2657, Val Acc: 0.9070\n",
      "Epoch [48/100] Train Loss: 0.1980, Train Acc: 0.9427 Val Loss: 0.2638, Val Acc: 0.9041\n",
      "Epoch [49/100] Train Loss: 0.1946, Train Acc: 0.9389 Val Loss: 0.2621, Val Acc: 0.9012\n",
      "Epoch [50/100] Train Loss: 0.1835, Train Acc: 0.9483 Val Loss: 0.2615, Val Acc: 0.9041\n",
      "Epoch [51/100] Train Loss: 0.1933, Train Acc: 0.9458 Val Loss: 0.2593, Val Acc: 0.9070\n",
      "Best model saved.\n",
      "Epoch [52/100] Train Loss: 0.1858, Train Acc: 0.9396 Val Loss: 0.2600, Val Acc: 0.9041\n",
      "Epoch [53/100] Train Loss: 0.1997, Train Acc: 0.9421 Val Loss: 0.2616, Val Acc: 0.9070\n",
      "Epoch [54/100] Train Loss: 0.1893, Train Acc: 0.9421 Val Loss: 0.2633, Val Acc: 0.9041\n",
      "Epoch [55/100] Train Loss: 0.1842, Train Acc: 0.9421 Val Loss: 0.2619, Val Acc: 0.9012\n",
      "Epoch [56/100] Train Loss: 0.1878, Train Acc: 0.9464 Val Loss: 0.2624, Val Acc: 0.9041\n",
      "Epoch [57/100] Train Loss: 0.1865, Train Acc: 0.9483 Val Loss: 0.2636, Val Acc: 0.9041\n",
      "Epoch [58/100] Train Loss: 0.1774, Train Acc: 0.9445 Val Loss: 0.2589, Val Acc: 0.9070\n",
      "Best model saved.\n",
      "Epoch [59/100] Train Loss: 0.1819, Train Acc: 0.9427 Val Loss: 0.2622, Val Acc: 0.9041\n",
      "Epoch [60/100] Train Loss: 0.1823, Train Acc: 0.9508 Val Loss: 0.2596, Val Acc: 0.9099\n",
      "Epoch [61/100] Train Loss: 0.1900, Train Acc: 0.9408 Val Loss: 0.2582, Val Acc: 0.9041\n",
      "Best model saved.\n",
      "Epoch [62/100] Train Loss: 0.1965, Train Acc: 0.9371 Val Loss: 0.2630, Val Acc: 0.9041\n",
      "Epoch [63/100] Train Loss: 0.1710, Train Acc: 0.9539 Val Loss: 0.2644, Val Acc: 0.8924\n",
      "Epoch [64/100] Train Loss: 0.1884, Train Acc: 0.9470 Val Loss: 0.2632, Val Acc: 0.9012\n",
      "Epoch [65/100] Train Loss: 0.1811, Train Acc: 0.9489 Val Loss: 0.2606, Val Acc: 0.9041\n",
      "Epoch [66/100] Train Loss: 0.1833, Train Acc: 0.9433 Val Loss: 0.2617, Val Acc: 0.9070\n",
      "Epoch [67/100] Train Loss: 0.1918, Train Acc: 0.9445 Val Loss: 0.2713, Val Acc: 0.8895\n",
      "Epoch [68/100] Train Loss: 0.1908, Train Acc: 0.9402 Val Loss: 0.2594, Val Acc: 0.9070\n",
      "Epoch [69/100] Train Loss: 0.2085, Train Acc: 0.9458 Val Loss: 0.2682, Val Acc: 0.8924\n",
      "Epoch [70/100] Train Loss: 0.1887, Train Acc: 0.9396 Val Loss: 0.2646, Val Acc: 0.9012\n",
      "Epoch [71/100] Train Loss: 0.1950, Train Acc: 0.9458 Val Loss: 0.2593, Val Acc: 0.9041\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 0.9246\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.87      0.85      0.86        72\n",
      "          CN       0.99      0.95      0.97       106\n",
      "         MCI       0.91      0.94      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.92      0.91      0.92       345\n",
      "weighted avg       0.93      0.92      0.92       345\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 61   0  11]\n",
      " [  0 101   5]\n",
      " [  9   1 157]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37572/265524843.py:221: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load DeiT features (expected shape: [num_samples, 768])\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")\n",
    "\n",
    "# Load FTTransformer features (expected shape: [num_samples, 192])\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")\n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "# Load CSV labels\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "\n",
    "# Initialize scalers for each modality\n",
    "scaler_deit = StandardScaler()\n",
    "scaler_ft = StandardScaler()\n",
    "\n",
    "# Fit scalers on training data and transform\n",
    "train_features_deit = scaler_deit.fit_transform(deit_train)\n",
    "val_features_deit = scaler_deit.transform(deit_val)\n",
    "test_features_deit = scaler_deit.transform(deit_test)\n",
    "\n",
    "train_features_ft = scaler_ft.fit_transform(ft_train)\n",
    "val_features_ft = scaler_ft.transform(ft_val)\n",
    "test_features_ft = scaler_ft.transform(ft_test)\n",
    "\n",
    "# 4. Define the Concatenation Fusion Model\n",
    "class ConcatenationFusionModel(nn.Module):\n",
    "    def __init__(self, deit_input_size, ft_input_size, num_classes):\n",
    "        super(ConcatenationFusionModel, self).__init__()\n",
    "        # DeiT branch (image modality)\n",
    "        self.deit_fc = nn.Sequential(\n",
    "            nn.Linear(deit_input_size, 308),   # Dimensionality reduction from 768 to 384\n",
    "            nn.BatchNorm1d(308),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        # FTTransformer branch (tabular modality)\n",
    "        self.ft_fc = nn.Sequential(\n",
    "            nn.Linear(ft_input_size, 128),   # Dimensionality reduction from 192 to 128\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        # Classifier: concatenated features from both branches (384+128=512)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(436, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, deit_input, ft_input):\n",
    "        # Process each modality separately\n",
    "        deit_output = self.deit_fc(deit_input)    # (batch_size, 384)\n",
    "        ft_output = self.ft_fc(ft_input)           # (batch_size, 128)\n",
    "        # Concatenate features from both modalities\n",
    "        concatenated_features = torch.cat([deit_output, ft_output], dim=1)  # (batch_size, 512)\n",
    "        # Classify\n",
    "        logits = self.classifier(concatenated_features)  # (batch_size, num_classes)\n",
    "        return logits\n",
    "\n",
    "# Initialize the model\n",
    "image_input_size = train_features_deit.shape[1]  # 768\n",
    "tab_input_size = train_features_ft.shape[1]        # 192\n",
    "\n",
    "model = ConcatenationFusionModel(image_input_size, tab_input_size, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# 5. Prepare the data loaders\n",
    "# Convert to tensors\n",
    "train_deit_tensor = torch.tensor(train_features_deit, dtype=torch.float32)\n",
    "val_deit_tensor = torch.tensor(val_features_deit, dtype=torch.float32)\n",
    "test_deit_tensor = torch.tensor(test_features_deit, dtype=torch.float32)\n",
    "\n",
    "train_ft_tensor = torch.tensor(train_features_ft, dtype=torch.float32)\n",
    "val_ft_tensor = torch.tensor(val_features_ft, dtype=torch.float32)\n",
    "test_ft_tensor = torch.tensor(test_features_ft, dtype=torch.float32)\n",
    "\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(train_deit_tensor, train_ft_tensor, train_labels_tensor)\n",
    "val_dataset = TensorDataset(val_deit_tensor, val_ft_tensor, val_labels_tensor)\n",
    "test_dataset = TensorDataset(test_deit_tensor, test_ft_tensor, test_labels_tensor)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 6. Training Setup\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_patience = 10\n",
    "epochs_no_improve = 0\n",
    "best_val_loss = np.Inf\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# 7. Training Loop with Validation and Early Stopping mechanism\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_train_correct = 0\n",
    "    for deit_inputs, ft_inputs, labels in train_loader:\n",
    "        deit_inputs = deit_inputs.to(device)\n",
    "        ft_inputs = ft_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(deit_inputs, ft_inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item() * labels.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total_train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    train_loss = total_train_loss / len(train_dataset)\n",
    "    train_accuracy = total_train_correct.double() / len(train_dataset)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    total_val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for deit_inputs, ft_inputs, labels in val_loader:\n",
    "            deit_inputs = deit_inputs.to(device)\n",
    "            ft_inputs = ft_inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(deit_inputs, ft_inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_val_loss += loss.item() * labels.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    val_loss = total_val_loss / len(val_dataset)\n",
    "    val_accuracy = total_val_correct.double() / len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f} \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# 8. Evaluate the Model on the Test Set\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "total_test_correct = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for deit_inputs, ft_inputs, labels in test_loader:\n",
    "        deit_inputs = deit_inputs.to(device)\n",
    "        ft_inputs = ft_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(deit_inputs, ft_inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        total_test_correct += torch.sum(preds == labels.data)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_accuracy = total_test_correct.double() / len(test_dataset)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e1c43ac-68df-44bd-80f9-1dd88db397cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# First, compute probabilities for the test set\n",
    "all_probs = []\n",
    "with torch.no_grad():\n",
    "    for deit_inputs, ft_inputs, labels in test_loader:\n",
    "        deit_inputs = deit_inputs.to(device)\n",
    "        ft_inputs = ft_inputs.to(device)\n",
    "        outputs = model(deit_inputs, ft_inputs)\n",
    "        # Apply softmax to get probabilities\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "all_probs = np.concatenate(all_probs, axis=0)\n",
    "\n",
    "# Convert true test labels to numpy array (already collected in all_labels earlier)\n",
    "y_true = np.array(all_labels)\n",
    "\n",
    "if num_classes == 2:\n",
    "    # Binary classification: use probability of the positive class (assumed to be column 1)\n",
    "    y_score = all_probs[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "else:\n",
    "    # Multiclass classification: compute one-vs-rest ROC for each class.\n",
    "    # Binarize the output labels\n",
    "    y_true_bin = label_binarize(y_true, classes=range(num_classes))\n",
    "    \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(num_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], all_probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Plot ROC curve for each class\n",
    "    plt.figure()\n",
    "    for i in range(num_classes):\n",
    "        plt.plot(fpr[i], tpr[i], lw=2, \n",
    "                 label='Class {0} ROC (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Multiclass Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8f38551-ba3c-4140-a7be-c5ead6728859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 AUC: 0.9764 which is 97.64%\n",
      "Class 1 AUC: 0.9899 which is 98.99%\n",
      "Class 2 AUC: 0.9687 which is 96.87%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Binarize the labels for one-vs-rest evaluation\n",
    "y_true_bin = label_binarize(y_true, classes=range(num_classes))\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], all_probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    auc_percentage = roc_auc[i] * 100\n",
    "    print(f\"Class {i} AUC: {roc_auc[i]:.4f} which is {auc_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2140d7ff-16d5-44c4-92dc-f754640a9ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro AUC: 97.83%\n"
     ]
    }
   ],
   "source": [
    "macro_auc = np.mean(list(roc_auc.values()))\n",
    "print(f\"Macro AUC: {macro_auc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b47866-8175-40b0-9440-306d2fe969f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
