{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd16bdcb-1310-450c-88dd-eb99b6beb334",
   "metadata": {},
   "source": [
    "# UniModal Images Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6a2d50-c364-446c-b4d6-5fefae19bc05",
   "metadata": {},
   "source": [
    "## DEIT Model End to end training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fd897b4-9d30-49b7-b048-d9ec408594cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 05:45:45.859845: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-17 05:45:45.897912: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-17 05:45:46.446734: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 1.0628, Train Acc: 0.4654 | Val Loss: 1.0162, Val Acc: 0.5174\n",
      "Best model saved.\n",
      "Epoch 2/10 | Train Loss: 0.9562, Train Acc: 0.5340 | Val Loss: 0.9823, Val Acc: 0.5291\n",
      "Best model saved.\n",
      "Epoch 3/10 | Train Loss: 0.5160, Train Acc: 0.8062 | Val Loss: 0.8068, Val Acc: 0.6512\n",
      "Best model saved.\n",
      "Epoch 4/10 | Train Loss: 0.0677, Train Acc: 0.9931 | Val Loss: 0.9239, Val Acc: 0.6657\n",
      "Epoch 5/10 | Train Loss: 0.0105, Train Acc: 1.0000 | Val Loss: 0.9995, Val Acc: 0.6628\n",
      "Epoch 6/10 | Train Loss: 0.0044, Train Acc: 1.0000 | Val Loss: 1.0697, Val Acc: 0.6541\n",
      "Early stopping triggered.\n",
      "ðŸ”¥ Test Accuracy: 0.6348\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, DeiTForImageClassification\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# âœ… Check CUDA availability\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths to datasets and preprocessed images\n",
    "train_csv = \"train_data_3.csv\"\n",
    "val_csv = \"val_data_3.csv\"\n",
    "test_csv = \"test_data_3.csv\"\n",
    "image_dir = \"preprocessed_images_3\"\n",
    "\n",
    "# Load Data Splits\n",
    "train_data = pd.read_csv(train_csv)\n",
    "val_data = pd.read_csv(val_csv)\n",
    "test_data = pd.read_csv(test_csv)\n",
    "\n",
    "# âœ… Load DeiT-Base Model and Image Processor with modified classifier head\n",
    "checkpoint = \"facebook/deit-base-distilled-patch16-224\"  # Using DeiT base distilled model\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "model = DeiTForImageClassification.from_pretrained(\n",
    "    checkpoint, num_labels=3, ignore_mismatched_sizes=True  # Set num_labels=3\n",
    ").to(device)\n",
    "\n",
    "# Define label mapping (example: \"CN\": Cognitively Normal, \"MCI\": Mild Cognitive Impairment, \"AD\": Alzheimerâ€™s Disease)\n",
    "label_map = {\"CN\": 0, \"MCI\": 1, \"AD\": 2}\n",
    "\n",
    "class MRIImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        img_path = os.path.join(self.image_dir, f\"{row['Image Data ID']}.png\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = label_map[row[\"Group\"]]\n",
    "        return image, label\n",
    "\n",
    "# âœ… Define Transformations for DeiT-Base\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224 as expected by DeiT-Base\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "])\n",
    "\n",
    "# âœ… Create PyTorch Dataloaders\n",
    "batch_size = 8\n",
    "train_dataset = MRIImageDataset(train_data, image_dir, transform=transform)\n",
    "val_dataset = MRIImageDataset(val_data, image_dir, transform=transform)\n",
    "test_dataset = MRIImageDataset(test_data, image_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# âœ… Define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "# âœ… Training Loop with Early Stopping\n",
    "epochs = 10\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_state = None\n",
    "patience = 3\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct_val += predicted.eq(labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    val_loss = running_val_loss / len(val_dataset)\n",
    "    val_acc = correct_val / total_val\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        wait = 0\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "if best_model_state:\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "# âœ… Evaluate Model on Test Set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images).logits\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_acc = correct / total\n",
    "print(f\"ðŸ”¥ Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7095d18d-8fa3-41ac-a6f4-b117f72936ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape: (1605, 768)\n",
      "Val features shape: (344, 768)\n",
      "Test features shape: (345, 768)\n",
      "Epoch 1/200: Train Loss = 1.0845, Val Loss = 1.0459, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 2/200: Train Loss = 1.0504, Val Loss = 1.0399, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 3/200: Train Loss = 1.0425, Val Loss = 1.0383, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 4/200: Train Loss = 1.0377, Val Loss = 1.0367, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 5/200: Train Loss = 1.0433, Val Loss = 1.0344, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 6/200: Train Loss = 1.0375, Val Loss = 1.0325, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 7/200: Train Loss = 1.0306, Val Loss = 1.0305, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 8/200: Train Loss = 1.0296, Val Loss = 1.0286, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 9/200: Train Loss = 1.0274, Val Loss = 1.0279, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 10/200: Train Loss = 1.0288, Val Loss = 1.0273, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 11/200: Train Loss = 1.0256, Val Loss = 1.0233, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 12/200: Train Loss = 1.0281, Val Loss = 1.0220, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 13/200: Train Loss = 1.0234, Val Loss = 1.0196, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 14/200: Train Loss = 1.0236, Val Loss = 1.0183, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 15/200: Train Loss = 1.0199, Val Loss = 1.0160, Val Acc = 0.4913\n",
      "  -> New best model saved.\n",
      "Epoch 16/200: Train Loss = 1.0122, Val Loss = 1.0149, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 17/200: Train Loss = 1.0170, Val Loss = 1.0132, Val Acc = 0.4884\n",
      "  -> New best model saved.\n",
      "Epoch 18/200: Train Loss = 1.0085, Val Loss = 1.0123, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 19/200: Train Loss = 1.0121, Val Loss = 1.0094, Val Acc = 0.5000\n",
      "  -> New best model saved.\n",
      "Epoch 20/200: Train Loss = 1.0061, Val Loss = 1.0074, Val Acc = 0.5116\n",
      "  -> New best model saved.\n",
      "Epoch 21/200: Train Loss = 1.0032, Val Loss = 1.0057, Val Acc = 0.5058\n",
      "  -> New best model saved.\n",
      "Epoch 22/200: Train Loss = 1.0013, Val Loss = 1.0034, Val Acc = 0.5116\n",
      "  -> New best model saved.\n",
      "Epoch 23/200: Train Loss = 0.9951, Val Loss = 1.0012, Val Acc = 0.5087\n",
      "  -> New best model saved.\n",
      "Epoch 24/200: Train Loss = 0.9941, Val Loss = 1.0000, Val Acc = 0.5058\n",
      "  -> New best model saved.\n",
      "Epoch 25/200: Train Loss = 0.9867, Val Loss = 0.9978, Val Acc = 0.5174\n",
      "  -> New best model saved.\n",
      "Epoch 26/200: Train Loss = 0.9859, Val Loss = 0.9958, Val Acc = 0.5203\n",
      "  -> New best model saved.\n",
      "Epoch 27/200: Train Loss = 0.9823, Val Loss = 0.9937, Val Acc = 0.5262\n",
      "  -> New best model saved.\n",
      "Epoch 28/200: Train Loss = 0.9749, Val Loss = 0.9919, Val Acc = 0.5203\n",
      "  -> New best model saved.\n",
      "Epoch 29/200: Train Loss = 0.9757, Val Loss = 0.9913, Val Acc = 0.5349\n",
      "  -> New best model saved.\n",
      "Epoch 30/200: Train Loss = 0.9699, Val Loss = 0.9907, Val Acc = 0.5174\n",
      "  -> New best model saved.\n",
      "Epoch 31/200: Train Loss = 0.9695, Val Loss = 0.9869, Val Acc = 0.5407\n",
      "  -> New best model saved.\n",
      "Epoch 32/200: Train Loss = 0.9660, Val Loss = 0.9867, Val Acc = 0.5262\n",
      "  -> New best model saved.\n",
      "Epoch 33/200: Train Loss = 0.9647, Val Loss = 0.9825, Val Acc = 0.5436\n",
      "  -> New best model saved.\n",
      "Epoch 34/200: Train Loss = 0.9574, Val Loss = 0.9826, Val Acc = 0.5349\n",
      "Epoch 35/200: Train Loss = 0.9629, Val Loss = 0.9798, Val Acc = 0.5436\n",
      "  -> New best model saved.\n",
      "Epoch 36/200: Train Loss = 0.9519, Val Loss = 0.9787, Val Acc = 0.5465\n",
      "  -> New best model saved.\n",
      "Epoch 37/200: Train Loss = 0.9414, Val Loss = 0.9770, Val Acc = 0.5552\n",
      "  -> New best model saved.\n",
      "Epoch 38/200: Train Loss = 0.9466, Val Loss = 0.9763, Val Acc = 0.5436\n",
      "  -> New best model saved.\n",
      "Epoch 39/200: Train Loss = 0.9302, Val Loss = 0.9724, Val Acc = 0.5581\n",
      "  -> New best model saved.\n",
      "Epoch 40/200: Train Loss = 0.9346, Val Loss = 0.9707, Val Acc = 0.5640\n",
      "  -> New best model saved.\n",
      "Epoch 41/200: Train Loss = 0.9237, Val Loss = 0.9688, Val Acc = 0.5523\n",
      "  -> New best model saved.\n",
      "Epoch 42/200: Train Loss = 0.9234, Val Loss = 0.9681, Val Acc = 0.5581\n",
      "  -> New best model saved.\n",
      "Epoch 43/200: Train Loss = 0.9201, Val Loss = 0.9701, Val Acc = 0.5552\n",
      "Epoch 44/200: Train Loss = 0.9084, Val Loss = 0.9624, Val Acc = 0.5727\n",
      "  -> New best model saved.\n",
      "Epoch 45/200: Train Loss = 0.9053, Val Loss = 0.9673, Val Acc = 0.5465\n",
      "Epoch 46/200: Train Loss = 0.9045, Val Loss = 0.9646, Val Acc = 0.5552\n",
      "Epoch 47/200: Train Loss = 0.9049, Val Loss = 0.9647, Val Acc = 0.5581\n",
      "Epoch 48/200: Train Loss = 0.8941, Val Loss = 0.9604, Val Acc = 0.5610\n",
      "  -> New best model saved.\n",
      "Epoch 49/200: Train Loss = 0.8909, Val Loss = 0.9567, Val Acc = 0.5669\n",
      "  -> New best model saved.\n",
      "Epoch 50/200: Train Loss = 0.8823, Val Loss = 0.9550, Val Acc = 0.5727\n",
      "  -> New best model saved.\n",
      "Epoch 51/200: Train Loss = 0.8788, Val Loss = 0.9507, Val Acc = 0.5814\n",
      "  -> New best model saved.\n",
      "Epoch 52/200: Train Loss = 0.8751, Val Loss = 0.9511, Val Acc = 0.5814\n",
      "Epoch 53/200: Train Loss = 0.8672, Val Loss = 0.9470, Val Acc = 0.5669\n",
      "  -> New best model saved.\n",
      "Epoch 54/200: Train Loss = 0.8619, Val Loss = 0.9476, Val Acc = 0.5698\n",
      "Epoch 55/200: Train Loss = 0.8479, Val Loss = 0.9456, Val Acc = 0.5727\n",
      "  -> New best model saved.\n",
      "Epoch 56/200: Train Loss = 0.8585, Val Loss = 0.9428, Val Acc = 0.5669\n",
      "  -> New best model saved.\n",
      "Epoch 57/200: Train Loss = 0.8494, Val Loss = 0.9511, Val Acc = 0.5640\n",
      "Epoch 58/200: Train Loss = 0.8429, Val Loss = 0.9437, Val Acc = 0.5640\n",
      "Epoch 59/200: Train Loss = 0.8335, Val Loss = 0.9409, Val Acc = 0.5814\n",
      "  -> New best model saved.\n",
      "Epoch 60/200: Train Loss = 0.8363, Val Loss = 0.9374, Val Acc = 0.5785\n",
      "  -> New best model saved.\n",
      "Epoch 61/200: Train Loss = 0.8275, Val Loss = 0.9358, Val Acc = 0.5756\n",
      "  -> New best model saved.\n",
      "Epoch 62/200: Train Loss = 0.8215, Val Loss = 0.9352, Val Acc = 0.5698\n",
      "  -> New best model saved.\n",
      "Epoch 63/200: Train Loss = 0.8182, Val Loss = 0.9300, Val Acc = 0.5756\n",
      "  -> New best model saved.\n",
      "Epoch 64/200: Train Loss = 0.8119, Val Loss = 0.9313, Val Acc = 0.5843\n",
      "Epoch 65/200: Train Loss = 0.8134, Val Loss = 0.9344, Val Acc = 0.5785\n",
      "Epoch 66/200: Train Loss = 0.8023, Val Loss = 0.9336, Val Acc = 0.5727\n",
      "Epoch 67/200: Train Loss = 0.7919, Val Loss = 0.9277, Val Acc = 0.5727\n",
      "  -> New best model saved.\n",
      "Epoch 68/200: Train Loss = 0.7936, Val Loss = 0.9235, Val Acc = 0.5901\n",
      "  -> New best model saved.\n",
      "Epoch 69/200: Train Loss = 0.7798, Val Loss = 0.9182, Val Acc = 0.5872\n",
      "  -> New best model saved.\n",
      "Epoch 70/200: Train Loss = 0.7785, Val Loss = 0.9185, Val Acc = 0.5669\n",
      "Epoch 71/200: Train Loss = 0.7761, Val Loss = 0.9144, Val Acc = 0.5756\n",
      "  -> New best model saved.\n",
      "Epoch 72/200: Train Loss = 0.7668, Val Loss = 0.9137, Val Acc = 0.5814\n",
      "  -> New best model saved.\n",
      "Epoch 73/200: Train Loss = 0.7561, Val Loss = 0.9143, Val Acc = 0.5930\n",
      "Epoch 74/200: Train Loss = 0.7564, Val Loss = 0.9201, Val Acc = 0.5785\n",
      "Epoch 75/200: Train Loss = 0.7550, Val Loss = 0.9152, Val Acc = 0.6163\n",
      "Epoch 76/200: Train Loss = 0.7296, Val Loss = 0.9071, Val Acc = 0.5727\n",
      "  -> New best model saved.\n",
      "Epoch 77/200: Train Loss = 0.7275, Val Loss = 0.9040, Val Acc = 0.5901\n",
      "  -> New best model saved.\n",
      "Epoch 78/200: Train Loss = 0.7246, Val Loss = 0.9057, Val Acc = 0.5814\n",
      "Epoch 79/200: Train Loss = 0.7223, Val Loss = 0.9027, Val Acc = 0.5930\n",
      "  -> New best model saved.\n",
      "Epoch 80/200: Train Loss = 0.7195, Val Loss = 0.9034, Val Acc = 0.5901\n",
      "Epoch 81/200: Train Loss = 0.7112, Val Loss = 0.8981, Val Acc = 0.5988\n",
      "  -> New best model saved.\n",
      "Epoch 82/200: Train Loss = 0.7036, Val Loss = 0.8972, Val Acc = 0.6105\n",
      "  -> New best model saved.\n",
      "Epoch 83/200: Train Loss = 0.6877, Val Loss = 0.8985, Val Acc = 0.6076\n",
      "Epoch 84/200: Train Loss = 0.6921, Val Loss = 0.8954, Val Acc = 0.5988\n",
      "  -> New best model saved.\n",
      "Epoch 85/200: Train Loss = 0.6786, Val Loss = 0.8872, Val Acc = 0.6105\n",
      "  -> New best model saved.\n",
      "Epoch 86/200: Train Loss = 0.6688, Val Loss = 0.8886, Val Acc = 0.6047\n",
      "Epoch 87/200: Train Loss = 0.6787, Val Loss = 0.8858, Val Acc = 0.6017\n",
      "  -> New best model saved.\n",
      "Epoch 88/200: Train Loss = 0.6552, Val Loss = 0.8834, Val Acc = 0.6017\n",
      "  -> New best model saved.\n",
      "Epoch 89/200: Train Loss = 0.6476, Val Loss = 0.8826, Val Acc = 0.6134\n",
      "  -> New best model saved.\n",
      "Epoch 90/200: Train Loss = 0.6469, Val Loss = 0.8772, Val Acc = 0.6105\n",
      "  -> New best model saved.\n",
      "Epoch 91/200: Train Loss = 0.6423, Val Loss = 0.8775, Val Acc = 0.6105\n",
      "Epoch 92/200: Train Loss = 0.6353, Val Loss = 0.8779, Val Acc = 0.6047\n",
      "Epoch 93/200: Train Loss = 0.6390, Val Loss = 0.8957, Val Acc = 0.6134\n",
      "Epoch 94/200: Train Loss = 0.6133, Val Loss = 0.8729, Val Acc = 0.6134\n",
      "  -> New best model saved.\n",
      "Epoch 95/200: Train Loss = 0.6126, Val Loss = 0.8789, Val Acc = 0.6250\n",
      "Epoch 96/200: Train Loss = 0.6151, Val Loss = 0.8687, Val Acc = 0.6192\n",
      "  -> New best model saved.\n",
      "Epoch 97/200: Train Loss = 0.5936, Val Loss = 0.8811, Val Acc = 0.6250\n",
      "Epoch 98/200: Train Loss = 0.5904, Val Loss = 0.8622, Val Acc = 0.6250\n",
      "  -> New best model saved.\n",
      "Epoch 99/200: Train Loss = 0.5921, Val Loss = 0.8625, Val Acc = 0.6250\n",
      "Epoch 100/200: Train Loss = 0.5911, Val Loss = 0.8655, Val Acc = 0.6395\n",
      "Epoch 101/200: Train Loss = 0.5814, Val Loss = 0.8558, Val Acc = 0.6250\n",
      "  -> New best model saved.\n",
      "Epoch 102/200: Train Loss = 0.5821, Val Loss = 0.8661, Val Acc = 0.6366\n",
      "Epoch 103/200: Train Loss = 0.5601, Val Loss = 0.8613, Val Acc = 0.6308\n",
      "Epoch 104/200: Train Loss = 0.5554, Val Loss = 0.8522, Val Acc = 0.6279\n",
      "  -> New best model saved.\n",
      "Epoch 105/200: Train Loss = 0.5451, Val Loss = 0.8520, Val Acc = 0.6308\n",
      "  -> New best model saved.\n",
      "Epoch 106/200: Train Loss = 0.5437, Val Loss = 0.8489, Val Acc = 0.6395\n",
      "  -> New best model saved.\n",
      "Epoch 107/200: Train Loss = 0.5332, Val Loss = 0.8538, Val Acc = 0.6337\n",
      "Epoch 108/200: Train Loss = 0.5278, Val Loss = 0.8427, Val Acc = 0.6424\n",
      "  -> New best model saved.\n",
      "Epoch 109/200: Train Loss = 0.5216, Val Loss = 0.8547, Val Acc = 0.6366\n",
      "Epoch 110/200: Train Loss = 0.5107, Val Loss = 0.8664, Val Acc = 0.6424\n",
      "Epoch 111/200: Train Loss = 0.5106, Val Loss = 0.8576, Val Acc = 0.6483\n",
      "Epoch 112/200: Train Loss = 0.4991, Val Loss = 0.8475, Val Acc = 0.6453\n",
      "Epoch 113/200: Train Loss = 0.4970, Val Loss = 0.8465, Val Acc = 0.6453\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 0.5826086956521739\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.62      0.39      0.48        72\n",
      "          CN       0.54      0.53      0.54       106\n",
      "         MCI       0.59      0.70      0.64       167\n",
      "\n",
      "    accuracy                           0.58       345\n",
      "   macro avg       0.59      0.54      0.55       345\n",
      "weighted avg       0.58      0.58      0.58       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Concatenate Features\n",
    "##########################################\n",
    "# Load DeiT features.\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")  # shape: (num_train_samples, deit_feature_dim)\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")      # shape: (num_val_samples,   deit_feature_dim)\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")     # shape: (num_test_samples,  deit_feature_dim)\n",
    "\n",
    "\n",
    "# Concatenate features along the last axis.\n",
    "train_features = deit_train\n",
    "val_features   = deit_val\n",
    "test_features  = deit_test\n",
    "\n",
    "print(\"Train features shape:\", train_features.shape)\n",
    "print(\"Val features shape:\", val_features.shape)\n",
    "print(\"Test features shape:\", test_features.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "# Read the CSV files (adjust filenames if needed)\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "# We assume the label column is named \"Group\"\n",
    "label_col = \"Group\"\n",
    "\n",
    "# Encode labels using LabelEncoder.\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create PyTorch Datasets and DataLoaders\n",
    "##########################################\n",
    "# Convert features and labels to tensors.\n",
    "train_features = torch.tensor(train_features, dtype=torch.float32)\n",
    "val_features   = torch.tensor(val_features, dtype=torch.float32)\n",
    "test_features  = torch.tensor(test_features, dtype=torch.float32)\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels   = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_labels  = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets.\n",
    "train_dataset = TensorDataset(train_features, train_labels)\n",
    "val_dataset   = TensorDataset(val_features, val_labels)\n",
    "test_dataset  = TensorDataset(test_features, test_labels)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define an MLP Classifier for Fused Features\n",
    "##########################################\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),  # First hidden layer with 256 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),        # Second hidden layer with 128 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes) # Output layer with num_classes neurons\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Example usage:\n",
    "input_dim = train_features.shape[1]  # Assuming train_features is defined and extracted\n",
    "model_mlp = MLPClassifier(input_dim, num_classes)\n",
    "model_mlp.to(device)\n",
    "\n",
    "\n",
    "##########################################\n",
    "# 5. Train the MLP Classifier with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_mlp.parameters(), lr=2e-5)\n",
    "num_epochs = 200\n",
    "patience = 5  # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_mlp.train()\n",
    "    train_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_mlp(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Evaluate on the validation set.\n",
    "    model_mlp.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model_mlp(x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    val_acc = accuracy_score(all_targets, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping check.\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model_mlp.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model state.\n",
    "model_mlp.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model_mlp.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model_mlp(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585529c9-7c4d-467a-b492-13b482db26c4",
   "metadata": {},
   "source": [
    "# Uni Modal Tabular Data performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4969a38e-8b4c-408c-a036-526503a0f2a3",
   "metadata": {},
   "source": [
    "## Ft Transformer end to end training and evalaution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e66305b4-d952-47ac-aca0-b30d0147cfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.0525 | Val Loss: 1.0485\n",
      "Epoch 2: Train Loss: 1.0433 | Val Loss: 1.0154\n",
      "Epoch 3: Train Loss: 0.9019 | Val Loss: 0.7926\n",
      "Epoch 4: Train Loss: 0.6360 | Val Loss: 0.3987\n",
      "Epoch 5: Train Loss: 0.4732 | Val Loss: 0.3601\n",
      "Epoch 6: Train Loss: 0.4218 | Val Loss: 0.3683\n",
      "Epoch 7: Train Loss: 0.4189 | Val Loss: 0.3567\n",
      "Epoch 8: Train Loss: 0.3831 | Val Loss: 0.3702\n",
      "Epoch 9: Train Loss: 0.3674 | Val Loss: 0.3834\n",
      "Epoch 10: Train Loss: 0.3757 | Val Loss: 0.3512\n",
      "Epoch 11: Train Loss: 0.3577 | Val Loss: 0.3434\n",
      "Epoch 12: Train Loss: 0.3724 | Val Loss: 0.3739\n",
      "Epoch 13: Train Loss: 0.3661 | Val Loss: 0.3467\n",
      "Epoch 14: Train Loss: 0.3517 | Val Loss: 0.3650\n",
      "Epoch 15: Train Loss: 0.3407 | Val Loss: 0.3582\n",
      "Epoch 16: Train Loss: 0.3345 | Val Loss: 0.3301\n",
      "Epoch 17: Train Loss: 0.3264 | Val Loss: 0.3586\n",
      "Epoch 18: Train Loss: 0.3287 | Val Loss: 0.3415\n",
      "Epoch 19: Train Loss: 0.3192 | Val Loss: 0.3271\n",
      "Epoch 20: Train Loss: 0.3345 | Val Loss: 0.3588\n",
      "Epoch 21: Train Loss: 0.3111 | Val Loss: 0.3530\n",
      "Epoch 22: Train Loss: 0.3283 | Val Loss: 0.3621\n",
      "Epoch 23: Train Loss: 0.3384 | Val Loss: 0.3327\n",
      "Epoch 24: Train Loss: 0.3181 | Val Loss: 0.3441\n",
      "Early stopping triggered.\n",
      "Trained model saved to best_ft_transformer_classification.pt\n",
      "Test Accuracy: 0.9130434782608695\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.82      0.88      0.85        72\n",
      "          CN       0.98      0.95      0.97       106\n",
      "         MCI       0.92      0.90      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.90      0.91      0.91       345\n",
      "weighted avg       0.92      0.91      0.91       345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2616349/921975057.py:191: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(save_model_path))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Import the FTTransformer model from rtdl_revisiting_models.\n",
    "from rtdl_revisiting_models import FTTransformer\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Preprocess the Data\n",
    "##########################################\n",
    "# Load CSV files for train, validation, and test splits.\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "# Define feature columns and target label.\n",
    "numerical_features = [\"Age\", \"CDGLOBAL\", \"CDRSB\", \"MMSCORE\", \"HMSCORE\", \"NPISCORE\", \"GDTOTAL\"]\n",
    "categorical_features = [\"GENOTYPE\"]\n",
    "label = \"Group\"\n",
    "\n",
    "# Subset dataframes to desired columns.\n",
    "cols = numerical_features + categorical_features + [label]\n",
    "train_data = train_data[cols]\n",
    "val_data   = val_data[cols]\n",
    "test_data  = test_data[cols]\n",
    "\n",
    "# Handle missingness for numerical features.\n",
    "cols_with_missing = [\"CDRSB\", \"MMSCORE\", \"HMSCORE\", \"NPISCORE\", \"GDTOTAL\"]\n",
    "for col in cols_with_missing:\n",
    "    for df in [train_data, val_data, test_data]:\n",
    "        df[col + \"_is_missing\"] = df[col].isnull().astype(int)\n",
    "        df[col] = df[col].fillna(-999)\n",
    "\n",
    "# Extend continuous features to include missing indicators.\n",
    "numerical_features_extended = numerical_features + [col + \"_is_missing\" for col in cols_with_missing]\n",
    "\n",
    "# Encode categorical features using LabelEncoder.\n",
    "cat_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    train_data[col] = le.fit_transform(train_data[col].astype(str))\n",
    "    val_data[col]   = le.transform(val_data[col].astype(str))\n",
    "    test_data[col]  = le.transform(test_data[col].astype(str))\n",
    "    cat_encoders[col] = le\n",
    "\n",
    "# Encode the target.\n",
    "label_encoder = LabelEncoder()\n",
    "train_data[label] = label_encoder.fit_transform(train_data[label])\n",
    "val_data[label]   = label_encoder.transform(val_data[label])\n",
    "test_data[label]  = label_encoder.transform(test_data[label])\n",
    "num_classes = len(label_encoder.classes_)  # e.g., 3 for classification\n",
    "\n",
    "##########################################\n",
    "# 2. Prepare NumPy Arrays and Create Dataset\n",
    "##########################################\n",
    "# Continuous features (including missing indicators).\n",
    "X_train_cont = train_data[numerical_features_extended].values.astype(np.float32)\n",
    "X_val_cont   = val_data[numerical_features_extended].values.astype(np.float32)\n",
    "X_test_cont  = test_data[numerical_features_extended].values.astype(np.float32)\n",
    "\n",
    "# Categorical features.\n",
    "X_train_cat = train_data[categorical_features].values.astype(np.int64)\n",
    "X_val_cat   = val_data[categorical_features].values.astype(np.int64)\n",
    "X_test_cat  = test_data[categorical_features].values.astype(np.int64)\n",
    "\n",
    "# Labels.\n",
    "y_train = train_data[label].values.astype(np.int64)\n",
    "y_val   = val_data[label].values.astype(np.int64)\n",
    "y_test  = test_data[label].values.astype(np.int64)\n",
    "\n",
    "# Create a simple PyTorch Dataset.\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, cont, cat, labels):\n",
    "        self.cont = cont\n",
    "        self.cat = cat\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"cont\": torch.tensor(self.cont[idx], dtype=torch.float32),\n",
    "            \"cat\": torch.tensor(self.cat[idx], dtype=torch.long),\n",
    "            \"target\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = TabularDataset(X_train_cont, X_train_cat, y_train)\n",
    "val_dataset   = TabularDataset(X_val_cont, X_val_cat, y_val)\n",
    "test_dataset  = TabularDataset(X_test_cont, X_test_cat, y_test)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 3. Initialize and Train the FTTransformer Classifier\n",
    "##########################################\n",
    "# Get the number of continuous features.\n",
    "n_cont_features = X_train_cont.shape[1]\n",
    "# Determine the cardinalities for each categorical feature.\n",
    "cat_cardinalities = [int(train_data[col].nunique()) for col in categorical_features]\n",
    "\n",
    "# For classification, set d_out = number of classes.\n",
    "d_out = num_classes\n",
    "\n",
    "# Instantiate the FTTransformer.\n",
    "model = FTTransformer(\n",
    "    n_cont_features=n_cont_features,\n",
    "    cat_cardinalities=cat_cardinalities,\n",
    "    d_out=d_out,\n",
    "    n_blocks=3,\n",
    "    d_block=192,                # Backbone (hidden) dimension\n",
    "    attention_n_heads=8,\n",
    "    attention_dropout=0.2,\n",
    "    ffn_d_hidden=None,          # Defaults internally if None.\n",
    "    ffn_d_hidden_multiplier=4/3,\n",
    "    ffn_dropout=0.1,\n",
    "    residual_dropout=0.0\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set up optimizer, loss function, and scheduler.\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# Training loop with early stopping.\n",
    "max_epochs = 100\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        cont = batch[\"cont\"].to(device)\n",
    "        cat = batch[\"cat\"].to(device)\n",
    "        targets = batch[\"target\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(cont, cat)  # Forward pass returns logits (shape: [batch_size, d_out])\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * cont.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            cont = batch[\"cont\"].to(device)\n",
    "            cat = batch[\"cat\"].to(device)\n",
    "            targets = batch[\"target\"].to(device)\n",
    "            \n",
    "            logits = model(cont, cat)\n",
    "            loss = criterion(logits, targets)\n",
    "            val_loss += loss.item() * cont.size(0)\n",
    "    val_loss /= len(val_dataset)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Save the best model.\n",
    "save_model_path = \"best_ft_transformer_classification.pt\"\n",
    "torch.save(best_model_state, save_model_path)\n",
    "print(\"Trained model saved to\", save_model_path)\n",
    "\n",
    "# Load the best model (optional).\n",
    "model.load_state_dict(torch.load(save_model_path))\n",
    "\n",
    "# Evaluate classification performance on the test set.\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        cont = batch[\"cont\"].to(device)\n",
    "        cat = batch[\"cat\"].to(device)\n",
    "        targets = batch[\"target\"].to(device)\n",
    "        \n",
    "        logits = model(cont, cat)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Classification Report (Test):\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20f6340c-ba46-4d79-9596-0d644c25ec18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape: (1605, 192)\n",
      "Val features shape: (344, 192)\n",
      "Test features shape: (345, 192)\n",
      "Epoch 1/200: Train Loss = 1.0271, Val Loss = 0.9500, Val Acc = 0.7762\n",
      "  -> New best model saved.\n",
      "Epoch 2/200: Train Loss = 0.8852, Val Loss = 0.7952, Val Acc = 0.8110\n",
      "  -> New best model saved.\n",
      "Epoch 3/200: Train Loss = 0.7312, Val Loss = 0.6371, Val Acc = 0.8169\n",
      "  -> New best model saved.\n",
      "Epoch 4/200: Train Loss = 0.5813, Val Loss = 0.5089, Val Acc = 0.8605\n",
      "  -> New best model saved.\n",
      "Epoch 5/200: Train Loss = 0.4699, Val Loss = 0.4264, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 6/200: Train Loss = 0.3984, Val Loss = 0.3807, Val Acc = 0.8663\n",
      "  -> New best model saved.\n",
      "Epoch 7/200: Train Loss = 0.3575, Val Loss = 0.3569, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 8/200: Train Loss = 0.3407, Val Loss = 0.3461, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 9/200: Train Loss = 0.3286, Val Loss = 0.3420, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 10/200: Train Loss = 0.3247, Val Loss = 0.3400, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 11/200: Train Loss = 0.3181, Val Loss = 0.3389, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 12/200: Train Loss = 0.3097, Val Loss = 0.3377, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 13/200: Train Loss = 0.3100, Val Loss = 0.3368, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 14/200: Train Loss = 0.3092, Val Loss = 0.3369, Val Acc = 0.8547\n",
      "Epoch 15/200: Train Loss = 0.3083, Val Loss = 0.3369, Val Acc = 0.8547\n",
      "Epoch 16/200: Train Loss = 0.3098, Val Loss = 0.3361, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 17/200: Train Loss = 0.3011, Val Loss = 0.3360, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 18/200: Train Loss = 0.3020, Val Loss = 0.3357, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 19/200: Train Loss = 0.3069, Val Loss = 0.3346, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 20/200: Train Loss = 0.3067, Val Loss = 0.3361, Val Acc = 0.8547\n",
      "Epoch 21/200: Train Loss = 0.3033, Val Loss = 0.3346, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 22/200: Train Loss = 0.3097, Val Loss = 0.3344, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 23/200: Train Loss = 0.3057, Val Loss = 0.3330, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 24/200: Train Loss = 0.3038, Val Loss = 0.3340, Val Acc = 0.8547\n",
      "Epoch 25/200: Train Loss = 0.3007, Val Loss = 0.3335, Val Acc = 0.8547\n",
      "Epoch 26/200: Train Loss = 0.3001, Val Loss = 0.3326, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 27/200: Train Loss = 0.3023, Val Loss = 0.3327, Val Acc = 0.8547\n",
      "Epoch 28/200: Train Loss = 0.3051, Val Loss = 0.3320, Val Acc = 0.8547\n",
      "  -> New best model saved.\n",
      "Epoch 29/200: Train Loss = 0.3031, Val Loss = 0.3310, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 30/200: Train Loss = 0.3017, Val Loss = 0.3315, Val Acc = 0.8576\n",
      "Epoch 31/200: Train Loss = 0.2980, Val Loss = 0.3305, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 32/200: Train Loss = 0.2964, Val Loss = 0.3306, Val Acc = 0.8576\n",
      "Epoch 33/200: Train Loss = 0.3031, Val Loss = 0.3298, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 34/200: Train Loss = 0.2991, Val Loss = 0.3301, Val Acc = 0.8576\n",
      "Epoch 35/200: Train Loss = 0.2962, Val Loss = 0.3294, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 36/200: Train Loss = 0.2953, Val Loss = 0.3296, Val Acc = 0.8576\n",
      "Epoch 37/200: Train Loss = 0.2991, Val Loss = 0.3284, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 38/200: Train Loss = 0.2965, Val Loss = 0.3283, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 39/200: Train Loss = 0.3032, Val Loss = 0.3283, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 40/200: Train Loss = 0.2966, Val Loss = 0.3281, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 41/200: Train Loss = 0.2982, Val Loss = 0.3275, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 42/200: Train Loss = 0.2998, Val Loss = 0.3278, Val Acc = 0.8576\n",
      "Epoch 43/200: Train Loss = 0.2998, Val Loss = 0.3269, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 44/200: Train Loss = 0.3001, Val Loss = 0.3282, Val Acc = 0.8547\n",
      "Epoch 45/200: Train Loss = 0.2996, Val Loss = 0.3270, Val Acc = 0.8576\n",
      "Epoch 46/200: Train Loss = 0.2938, Val Loss = 0.3279, Val Acc = 0.8576\n",
      "Epoch 47/200: Train Loss = 0.2914, Val Loss = 0.3274, Val Acc = 0.8576\n",
      "Epoch 48/200: Train Loss = 0.2979, Val Loss = 0.3275, Val Acc = 0.8576\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 0.9130434782608695\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.83      0.86      0.84        72\n",
      "          CN       0.98      0.95      0.97       106\n",
      "         MCI       0.91      0.91      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.91      0.91      0.91       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Concatenate Features\n",
    "##########################################\n",
    "# Load DeiT features.\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")        # shape: (num_train_samples, ft_feature_dim)\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")          # shape: (num_val_samples,   ft_feature_dim)\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")   \n",
    "\n",
    "\n",
    "# Concatenate features along the last axis.\n",
    "train_features = ft_train\n",
    "val_features   = ft_val\n",
    "test_features  = ft_test\n",
    "\n",
    "print(\"Train features shape:\", train_features.shape)\n",
    "print(\"Val features shape:\", val_features.shape)\n",
    "print(\"Test features shape:\", test_features.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "# Read the CSV files (adjust filenames if needed)\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "# We assume the label column is named \"Group\"\n",
    "label_col = \"Group\"\n",
    "\n",
    "# Encode labels using LabelEncoder.\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create PyTorch Datasets and DataLoaders\n",
    "##########################################\n",
    "# Convert features and labels to tensors.\n",
    "train_features = torch.tensor(train_features, dtype=torch.float32)\n",
    "val_features   = torch.tensor(val_features, dtype=torch.float32)\n",
    "test_features  = torch.tensor(test_features, dtype=torch.float32)\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels   = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_labels  = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets.\n",
    "train_dataset = TensorDataset(train_features, train_labels)\n",
    "val_dataset   = TensorDataset(val_features, val_labels)\n",
    "test_dataset  = TensorDataset(test_features, test_labels)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define an MLP Classifier for Fused Features\n",
    "##########################################\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),  # First hidden layer with 256 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),        # Second hidden layer with 128 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes) # Output layer with num_classes neurons\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Example usage:\n",
    "input_dim = train_features.shape[1]  # Assuming train_features is defined and extracted\n",
    "model_mlp = MLPClassifier(input_dim, num_classes)\n",
    "model_mlp.to(device)\n",
    "\n",
    "\n",
    "##########################################\n",
    "# 5. Train the MLP Classifier with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_mlp.parameters(), lr=2e-5)\n",
    "num_epochs = 200\n",
    "patience = 5  # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_mlp.train()\n",
    "    train_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_mlp(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Evaluate on the validation set.\n",
    "    model_mlp.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model_mlp(x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    val_acc = accuracy_score(all_targets, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping check.\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model_mlp.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model state.\n",
    "model_mlp.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model_mlp.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model_mlp(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c294f1e-e08f-4605-8e50-2efcfcd2b4cd",
   "metadata": {},
   "source": [
    "## Early Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b6dfd4d-21bf-4f58-a411-dd0c0ab290e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused train features shape: (1605, 960)\n",
      "Fused val features shape: (344, 960)\n",
      "Fused test features shape: (345, 960)\n",
      "Epoch 1/100: Train Loss = 1.0067, Val Loss = 0.9130, Val Acc = 0.6017\n",
      "  -> New best model saved.\n",
      "Epoch 2/100: Train Loss = 0.8532, Val Loss = 0.7509, Val Acc = 0.7674\n",
      "  -> New best model saved.\n",
      "Epoch 3/100: Train Loss = 0.7080, Val Loss = 0.5965, Val Acc = 0.7791\n",
      "  -> New best model saved.\n",
      "Epoch 4/100: Train Loss = 0.5748, Val Loss = 0.4862, Val Acc = 0.8052\n",
      "  -> New best model saved.\n",
      "Epoch 5/100: Train Loss = 0.4803, Val Loss = 0.4129, Val Acc = 0.8459\n",
      "  -> New best model saved.\n",
      "Epoch 6/100: Train Loss = 0.4219, Val Loss = 0.3661, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 7/100: Train Loss = 0.3768, Val Loss = 0.3411, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 8/100: Train Loss = 0.3560, Val Loss = 0.3246, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 9/100: Train Loss = 0.3415, Val Loss = 0.3150, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 10/100: Train Loss = 0.3240, Val Loss = 0.3082, Val Acc = 0.8866\n",
      "  -> New best model saved.\n",
      "Epoch 11/100: Train Loss = 0.3108, Val Loss = 0.3057, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 12/100: Train Loss = 0.3130, Val Loss = 0.3087, Val Acc = 0.8721\n",
      "Epoch 13/100: Train Loss = 0.2930, Val Loss = 0.3009, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 14/100: Train Loss = 0.2933, Val Loss = 0.2998, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 15/100: Train Loss = 0.2959, Val Loss = 0.3020, Val Acc = 0.8721\n",
      "Epoch 16/100: Train Loss = 0.2783, Val Loss = 0.3040, Val Acc = 0.8721\n",
      "Epoch 17/100: Train Loss = 0.2815, Val Loss = 0.2991, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 18/100: Train Loss = 0.2733, Val Loss = 0.3000, Val Acc = 0.8866\n",
      "Epoch 19/100: Train Loss = 0.2835, Val Loss = 0.2989, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 20/100: Train Loss = 0.2700, Val Loss = 0.2977, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 21/100: Train Loss = 0.2724, Val Loss = 0.2961, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 22/100: Train Loss = 0.2652, Val Loss = 0.2954, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 23/100: Train Loss = 0.2602, Val Loss = 0.2966, Val Acc = 0.8779\n",
      "Epoch 24/100: Train Loss = 0.2678, Val Loss = 0.2951, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 25/100: Train Loss = 0.2618, Val Loss = 0.2947, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 26/100: Train Loss = 0.2653, Val Loss = 0.2960, Val Acc = 0.8750\n",
      "Epoch 27/100: Train Loss = 0.2721, Val Loss = 0.3029, Val Acc = 0.8721\n",
      "Epoch 28/100: Train Loss = 0.2544, Val Loss = 0.2928, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 29/100: Train Loss = 0.2570, Val Loss = 0.2940, Val Acc = 0.8808\n",
      "Epoch 30/100: Train Loss = 0.2538, Val Loss = 0.2907, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 31/100: Train Loss = 0.2476, Val Loss = 0.2911, Val Acc = 0.8779\n",
      "Epoch 32/100: Train Loss = 0.2541, Val Loss = 0.2903, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 33/100: Train Loss = 0.2437, Val Loss = 0.2901, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 34/100: Train Loss = 0.2404, Val Loss = 0.2911, Val Acc = 0.8779\n",
      "Epoch 35/100: Train Loss = 0.2455, Val Loss = 0.2908, Val Acc = 0.8779\n",
      "Epoch 36/100: Train Loss = 0.2493, Val Loss = 0.2909, Val Acc = 0.8750\n",
      "Epoch 37/100: Train Loss = 0.2372, Val Loss = 0.2914, Val Acc = 0.8750\n",
      "Epoch 38/100: Train Loss = 0.2372, Val Loss = 0.2872, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 39/100: Train Loss = 0.2350, Val Loss = 0.2879, Val Acc = 0.8779\n",
      "Epoch 40/100: Train Loss = 0.2279, Val Loss = 0.2883, Val Acc = 0.8750\n",
      "Epoch 41/100: Train Loss = 0.2259, Val Loss = 0.2885, Val Acc = 0.8779\n",
      "Epoch 42/100: Train Loss = 0.2286, Val Loss = 0.2874, Val Acc = 0.8779\n",
      "Epoch 43/100: Train Loss = 0.2226, Val Loss = 0.2855, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 44/100: Train Loss = 0.2269, Val Loss = 0.2852, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 45/100: Train Loss = 0.2220, Val Loss = 0.2842, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 46/100: Train Loss = 0.2232, Val Loss = 0.2850, Val Acc = 0.8837\n",
      "Epoch 47/100: Train Loss = 0.2188, Val Loss = 0.2837, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 48/100: Train Loss = 0.2255, Val Loss = 0.2839, Val Acc = 0.8895\n",
      "Epoch 49/100: Train Loss = 0.2176, Val Loss = 0.2859, Val Acc = 0.8837\n",
      "Epoch 50/100: Train Loss = 0.2162, Val Loss = 0.2825, Val Acc = 0.8866\n",
      "  -> New best model saved.\n",
      "Epoch 51/100: Train Loss = 0.2109, Val Loss = 0.2856, Val Acc = 0.8779\n",
      "Epoch 52/100: Train Loss = 0.2118, Val Loss = 0.2802, Val Acc = 0.8924\n",
      "  -> New best model saved.\n",
      "Epoch 53/100: Train Loss = 0.2064, Val Loss = 0.2820, Val Acc = 0.8924\n",
      "Epoch 54/100: Train Loss = 0.2078, Val Loss = 0.2851, Val Acc = 0.8837\n",
      "Epoch 55/100: Train Loss = 0.2046, Val Loss = 0.2856, Val Acc = 0.8837\n",
      "Epoch 56/100: Train Loss = 0.2048, Val Loss = 0.2839, Val Acc = 0.8837\n",
      "Epoch 57/100: Train Loss = 0.2089, Val Loss = 0.2787, Val Acc = 0.8924\n",
      "  -> New best model saved.\n",
      "Epoch 58/100: Train Loss = 0.1993, Val Loss = 0.2876, Val Acc = 0.8895\n",
      "Epoch 59/100: Train Loss = 0.1934, Val Loss = 0.2837, Val Acc = 0.8866\n",
      "Epoch 60/100: Train Loss = 0.1902, Val Loss = 0.2790, Val Acc = 0.8924\n",
      "Epoch 61/100: Train Loss = 0.1977, Val Loss = 0.2809, Val Acc = 0.8924\n",
      "Epoch 62/100: Train Loss = 0.1910, Val Loss = 0.2802, Val Acc = 0.8924\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 0.9159420289855073\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.85      0.83      0.84        72\n",
      "          CN       0.99      0.95      0.97       106\n",
      "         MCI       0.90      0.93      0.91       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.91      0.90      0.91       345\n",
      "weighted avg       0.92      0.92      0.92       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Concatenate Features\n",
    "##########################################\n",
    "# Load DeiT features.\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")  # shape: (num_train_samples, deit_feature_dim)\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")      # shape: (num_val_samples,   deit_feature_dim)\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")     # shape: (num_test_samples,  deit_feature_dim)\n",
    "\n",
    "# Load FTTransformer features.\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")        # shape: (num_train_samples, ft_feature_dim)\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")          # shape: (num_val_samples,   ft_feature_dim)\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")         # shape: (num_test_samples,  ft_feature_dim)\n",
    "\n",
    "# Concatenate features along the last axis.\n",
    "train_features = np.concatenate((deit_train, ft_train), axis=1)\n",
    "val_features   = np.concatenate((deit_val, ft_val), axis=1)\n",
    "test_features  = np.concatenate((deit_test, ft_test), axis=1)\n",
    "\n",
    "print(\"Fused train features shape:\", train_features.shape)\n",
    "print(\"Fused val features shape:\", val_features.shape)\n",
    "print(\"Fused test features shape:\", test_features.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "# Read the CSV files (adjust filenames if needed)\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "# We assume the label column is named \"Group\"\n",
    "label_col = \"Group\"\n",
    "\n",
    "# Encode labels using LabelEncoder.\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create PyTorch Datasets and DataLoaders\n",
    "##########################################\n",
    "# Convert features and labels to tensors.\n",
    "train_features = torch.tensor(train_features, dtype=torch.float32)\n",
    "val_features   = torch.tensor(val_features, dtype=torch.float32)\n",
    "test_features  = torch.tensor(test_features, dtype=torch.float32)\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels   = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_labels  = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets.\n",
    "train_dataset = TensorDataset(train_features, train_labels)\n",
    "val_dataset   = TensorDataset(val_features, val_labels)\n",
    "test_dataset  = TensorDataset(test_features, test_labels)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define an MLP Classifier for Fused Features\n",
    "##########################################\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),  # First hidden layer with 256 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),        # Second hidden layer with 128 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes) # Output layer with num_classes neurons\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Example usage:\n",
    "input_dim = train_features.shape[1]  # Assuming train_features is defined and extracted\n",
    "model_mlp = MLPClassifier(input_dim, num_classes)\n",
    "model_mlp.to(device)\n",
    "\n",
    "\n",
    "##########################################\n",
    "# 5. Train the MLP Classifier with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_mlp.parameters(), lr=2e-5)\n",
    "num_epochs = 100\n",
    "patience = 5  # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_mlp.train()\n",
    "    train_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_mlp(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Evaluate on the validation set.\n",
    "    model_mlp.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model_mlp(x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    val_acc = accuracy_score(all_targets, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping check.\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model_mlp.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model state.\n",
    "model_mlp.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model_mlp.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model_mlp(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6496b2-d8c3-4bbb-bbe5-5497281cdfb5",
   "metadata": {},
   "source": [
    "## Mid Fusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54ed03fd-aa9f-4ad3-9ab7-bfe1fe4ab3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Classes: ['AD' 'CN' 'MCI']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Train Loss: 1.0254, Train Acc: 0.4729 Val Loss: 0.9001, Val Acc: 0.7587\n",
      "Best model saved.\n",
      "Epoch [2/100] Train Loss: 0.8606, Train Acc: 0.6617 Val Loss: 0.7841, Val Acc: 0.8430\n",
      "Best model saved.\n",
      "Epoch [3/100] Train Loss: 0.7354, Train Acc: 0.7751 Val Loss: 0.6766, Val Acc: 0.8517\n",
      "Best model saved.\n",
      "Epoch [4/100] Train Loss: 0.6467, Train Acc: 0.8224 Val Loss: 0.6163, Val Acc: 0.8488\n",
      "Best model saved.\n",
      "Epoch [5/100] Train Loss: 0.5909, Train Acc: 0.8293 Val Loss: 0.5412, Val Acc: 0.8576\n",
      "Best model saved.\n",
      "Epoch [6/100] Train Loss: 0.5464, Train Acc: 0.8505 Val Loss: 0.4938, Val Acc: 0.8547\n",
      "Best model saved.\n",
      "Epoch [7/100] Train Loss: 0.4950, Train Acc: 0.8679 Val Loss: 0.4852, Val Acc: 0.8547\n",
      "Best model saved.\n",
      "Epoch [8/100] Train Loss: 0.4632, Train Acc: 0.8760 Val Loss: 0.4881, Val Acc: 0.8547\n",
      "Epoch [9/100] Train Loss: 0.4428, Train Acc: 0.8766 Val Loss: 0.4319, Val Acc: 0.8517\n",
      "Best model saved.\n",
      "Epoch [10/100] Train Loss: 0.4066, Train Acc: 0.8810 Val Loss: 0.4142, Val Acc: 0.8517\n",
      "Best model saved.\n",
      "Epoch [11/100] Train Loss: 0.3917, Train Acc: 0.8804 Val Loss: 0.3843, Val Acc: 0.8576\n",
      "Best model saved.\n",
      "Epoch [12/100] Train Loss: 0.3854, Train Acc: 0.8860 Val Loss: 0.3832, Val Acc: 0.8605\n",
      "Best model saved.\n",
      "Epoch [13/100] Train Loss: 0.3692, Train Acc: 0.8941 Val Loss: 0.3807, Val Acc: 0.8576\n",
      "Best model saved.\n",
      "Epoch [14/100] Train Loss: 0.3730, Train Acc: 0.8872 Val Loss: 0.3622, Val Acc: 0.8576\n",
      "Best model saved.\n",
      "Epoch [15/100] Train Loss: 0.3574, Train Acc: 0.8879 Val Loss: 0.3684, Val Acc: 0.8576\n",
      "Epoch [16/100] Train Loss: 0.3654, Train Acc: 0.8910 Val Loss: 0.3540, Val Acc: 0.8605\n",
      "Best model saved.\n",
      "Epoch [17/100] Train Loss: 0.3433, Train Acc: 0.8947 Val Loss: 0.3456, Val Acc: 0.8721\n",
      "Best model saved.\n",
      "Epoch [18/100] Train Loss: 0.3281, Train Acc: 0.8953 Val Loss: 0.3488, Val Acc: 0.8692\n",
      "Epoch [19/100] Train Loss: 0.3483, Train Acc: 0.8866 Val Loss: 0.3375, Val Acc: 0.8634\n",
      "Best model saved.\n",
      "Epoch [20/100] Train Loss: 0.3158, Train Acc: 0.9034 Val Loss: 0.3333, Val Acc: 0.8634\n",
      "Best model saved.\n",
      "Epoch [21/100] Train Loss: 0.3143, Train Acc: 0.9028 Val Loss: 0.3240, Val Acc: 0.8663\n",
      "Best model saved.\n",
      "Epoch [22/100] Train Loss: 0.2997, Train Acc: 0.9078 Val Loss: 0.3134, Val Acc: 0.8721\n",
      "Best model saved.\n",
      "Epoch [23/100] Train Loss: 0.2983, Train Acc: 0.9097 Val Loss: 0.3228, Val Acc: 0.8692\n",
      "Epoch [24/100] Train Loss: 0.2962, Train Acc: 0.9034 Val Loss: 0.3108, Val Acc: 0.8750\n",
      "Best model saved.\n",
      "Epoch [25/100] Train Loss: 0.2965, Train Acc: 0.9016 Val Loss: 0.3105, Val Acc: 0.8634\n",
      "Best model saved.\n",
      "Epoch [26/100] Train Loss: 0.2929, Train Acc: 0.9078 Val Loss: 0.3040, Val Acc: 0.8808\n",
      "Best model saved.\n",
      "Epoch [27/100] Train Loss: 0.2903, Train Acc: 0.9047 Val Loss: 0.3007, Val Acc: 0.8779\n",
      "Best model saved.\n",
      "Epoch [28/100] Train Loss: 0.2719, Train Acc: 0.9146 Val Loss: 0.2971, Val Acc: 0.8837\n",
      "Best model saved.\n",
      "Epoch [29/100] Train Loss: 0.2653, Train Acc: 0.9128 Val Loss: 0.2993, Val Acc: 0.8837\n",
      "Epoch [30/100] Train Loss: 0.2637, Train Acc: 0.9190 Val Loss: 0.2916, Val Acc: 0.8924\n",
      "Best model saved.\n",
      "Epoch [31/100] Train Loss: 0.2658, Train Acc: 0.9121 Val Loss: 0.2840, Val Acc: 0.8837\n",
      "Best model saved.\n",
      "Epoch [32/100] Train Loss: 0.2517, Train Acc: 0.9246 Val Loss: 0.2879, Val Acc: 0.8895\n",
      "Epoch [33/100] Train Loss: 0.2636, Train Acc: 0.9146 Val Loss: 0.2759, Val Acc: 0.8924\n",
      "Best model saved.\n",
      "Epoch [34/100] Train Loss: 0.2465, Train Acc: 0.9190 Val Loss: 0.2820, Val Acc: 0.8924\n",
      "Epoch [35/100] Train Loss: 0.2292, Train Acc: 0.9240 Val Loss: 0.2777, Val Acc: 0.8866\n",
      "Epoch [36/100] Train Loss: 0.2357, Train Acc: 0.9277 Val Loss: 0.2855, Val Acc: 0.8924\n",
      "Epoch [37/100] Train Loss: 0.2326, Train Acc: 0.9308 Val Loss: 0.2749, Val Acc: 0.8953\n",
      "Best model saved.\n",
      "Epoch [38/100] Train Loss: 0.2042, Train Acc: 0.9383 Val Loss: 0.2741, Val Acc: 0.9012\n",
      "Best model saved.\n",
      "Epoch [39/100] Train Loss: 0.2153, Train Acc: 0.9389 Val Loss: 0.2676, Val Acc: 0.8953\n",
      "Best model saved.\n",
      "Epoch [40/100] Train Loss: 0.2147, Train Acc: 0.9252 Val Loss: 0.2693, Val Acc: 0.8924\n",
      "Epoch [41/100] Train Loss: 0.2299, Train Acc: 0.9290 Val Loss: 0.2666, Val Acc: 0.8983\n",
      "Best model saved.\n",
      "Epoch [42/100] Train Loss: 0.2043, Train Acc: 0.9427 Val Loss: 0.2687, Val Acc: 0.8953\n",
      "Epoch [43/100] Train Loss: 0.2074, Train Acc: 0.9377 Val Loss: 0.2633, Val Acc: 0.9012\n",
      "Best model saved.\n",
      "Epoch [44/100] Train Loss: 0.2130, Train Acc: 0.9371 Val Loss: 0.2602, Val Acc: 0.9070\n",
      "Best model saved.\n",
      "Epoch [45/100] Train Loss: 0.2213, Train Acc: 0.9290 Val Loss: 0.2705, Val Acc: 0.9012\n",
      "Epoch [46/100] Train Loss: 0.1940, Train Acc: 0.9396 Val Loss: 0.2661, Val Acc: 0.9012\n",
      "Epoch [47/100] Train Loss: 0.1944, Train Acc: 0.9364 Val Loss: 0.2657, Val Acc: 0.9070\n",
      "Epoch [48/100] Train Loss: 0.1980, Train Acc: 0.9427 Val Loss: 0.2638, Val Acc: 0.9041\n",
      "Epoch [49/100] Train Loss: 0.1946, Train Acc: 0.9389 Val Loss: 0.2621, Val Acc: 0.9012\n",
      "Epoch [50/100] Train Loss: 0.1835, Train Acc: 0.9483 Val Loss: 0.2615, Val Acc: 0.9041\n",
      "Epoch [51/100] Train Loss: 0.1933, Train Acc: 0.9458 Val Loss: 0.2593, Val Acc: 0.9070\n",
      "Best model saved.\n",
      "Epoch [52/100] Train Loss: 0.1858, Train Acc: 0.9396 Val Loss: 0.2600, Val Acc: 0.9041\n",
      "Epoch [53/100] Train Loss: 0.1997, Train Acc: 0.9421 Val Loss: 0.2616, Val Acc: 0.9070\n",
      "Epoch [54/100] Train Loss: 0.1893, Train Acc: 0.9421 Val Loss: 0.2633, Val Acc: 0.9041\n",
      "Epoch [55/100] Train Loss: 0.1842, Train Acc: 0.9421 Val Loss: 0.2619, Val Acc: 0.9012\n",
      "Epoch [56/100] Train Loss: 0.1878, Train Acc: 0.9464 Val Loss: 0.2624, Val Acc: 0.9041\n",
      "Epoch [57/100] Train Loss: 0.1865, Train Acc: 0.9483 Val Loss: 0.2636, Val Acc: 0.9041\n",
      "Epoch [58/100] Train Loss: 0.1774, Train Acc: 0.9445 Val Loss: 0.2589, Val Acc: 0.9070\n",
      "Best model saved.\n",
      "Epoch [59/100] Train Loss: 0.1819, Train Acc: 0.9427 Val Loss: 0.2622, Val Acc: 0.9041\n",
      "Epoch [60/100] Train Loss: 0.1823, Train Acc: 0.9508 Val Loss: 0.2596, Val Acc: 0.9099\n",
      "Epoch [61/100] Train Loss: 0.1900, Train Acc: 0.9408 Val Loss: 0.2582, Val Acc: 0.9041\n",
      "Best model saved.\n",
      "Epoch [62/100] Train Loss: 0.1965, Train Acc: 0.9371 Val Loss: 0.2630, Val Acc: 0.9041\n",
      "Epoch [63/100] Train Loss: 0.1710, Train Acc: 0.9539 Val Loss: 0.2644, Val Acc: 0.8924\n",
      "Epoch [64/100] Train Loss: 0.1884, Train Acc: 0.9470 Val Loss: 0.2632, Val Acc: 0.9012\n",
      "Epoch [65/100] Train Loss: 0.1811, Train Acc: 0.9489 Val Loss: 0.2606, Val Acc: 0.9041\n",
      "Epoch [66/100] Train Loss: 0.1833, Train Acc: 0.9433 Val Loss: 0.2617, Val Acc: 0.9070\n",
      "Epoch [67/100] Train Loss: 0.1918, Train Acc: 0.9445 Val Loss: 0.2713, Val Acc: 0.8895\n",
      "Epoch [68/100] Train Loss: 0.1908, Train Acc: 0.9402 Val Loss: 0.2594, Val Acc: 0.9070\n",
      "Epoch [69/100] Train Loss: 0.2085, Train Acc: 0.9458 Val Loss: 0.2682, Val Acc: 0.8924\n",
      "Epoch [70/100] Train Loss: 0.1887, Train Acc: 0.9396 Val Loss: 0.2646, Val Acc: 0.9012\n",
      "Epoch [71/100] Train Loss: 0.1950, Train Acc: 0.9458 Val Loss: 0.2593, Val Acc: 0.9041\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 0.9246\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.87      0.85      0.86        72\n",
      "          CN       0.99      0.95      0.97       106\n",
      "         MCI       0.91      0.94      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.92      0.91      0.92       345\n",
      "weighted avg       0.93      0.92      0.92       345\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 61   0  11]\n",
      " [  0 101   5]\n",
      " [  9   1 157]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2616349/265524843.py:221: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load DeiT features (expected shape: [num_samples, 768])\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")\n",
    "\n",
    "# Load FTTransformer features (expected shape: [num_samples, 192])\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")\n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "# Load CSV labels\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "\n",
    "# Initialize scalers for each modality\n",
    "scaler_deit = StandardScaler()\n",
    "scaler_ft = StandardScaler()\n",
    "\n",
    "# Fit scalers on training data and transform\n",
    "train_features_deit = scaler_deit.fit_transform(deit_train)\n",
    "val_features_deit = scaler_deit.transform(deit_val)\n",
    "test_features_deit = scaler_deit.transform(deit_test)\n",
    "\n",
    "train_features_ft = scaler_ft.fit_transform(ft_train)\n",
    "val_features_ft = scaler_ft.transform(ft_val)\n",
    "test_features_ft = scaler_ft.transform(ft_test)\n",
    "\n",
    "# 4. Define the Concatenation Fusion Model\n",
    "class ConcatenationFusionModel(nn.Module):\n",
    "    def __init__(self, deit_input_size, ft_input_size, num_classes):\n",
    "        super(ConcatenationFusionModel, self).__init__()\n",
    "        # DeiT branch (image modality)\n",
    "        self.deit_fc = nn.Sequential(\n",
    "            nn.Linear(deit_input_size, 308),   # Dimensionality reduction from 768 to 384\n",
    "            nn.BatchNorm1d(308),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        # FTTransformer branch (tabular modality)\n",
    "        self.ft_fc = nn.Sequential(\n",
    "            nn.Linear(ft_input_size, 128),   # Dimensionality reduction from 192 to 128\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        # Classifier: concatenated features from both branches (384+128=512)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(436, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, deit_input, ft_input):\n",
    "        # Process each modality separately\n",
    "        deit_output = self.deit_fc(deit_input)    # (batch_size, 384)\n",
    "        ft_output = self.ft_fc(ft_input)           # (batch_size, 128)\n",
    "        # Concatenate features from both modalities\n",
    "        concatenated_features = torch.cat([deit_output, ft_output], dim=1)  # (batch_size, 512)\n",
    "        # Classify\n",
    "        logits = self.classifier(concatenated_features)  # (batch_size, num_classes)\n",
    "        return logits\n",
    "\n",
    "# Initialize the model\n",
    "image_input_size = train_features_deit.shape[1]  # 768\n",
    "tab_input_size = train_features_ft.shape[1]        # 192\n",
    "\n",
    "model = ConcatenationFusionModel(image_input_size, tab_input_size, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# 5. Prepare the data loaders\n",
    "# Convert to tensors\n",
    "train_deit_tensor = torch.tensor(train_features_deit, dtype=torch.float32)\n",
    "val_deit_tensor = torch.tensor(val_features_deit, dtype=torch.float32)\n",
    "test_deit_tensor = torch.tensor(test_features_deit, dtype=torch.float32)\n",
    "\n",
    "train_ft_tensor = torch.tensor(train_features_ft, dtype=torch.float32)\n",
    "val_ft_tensor = torch.tensor(val_features_ft, dtype=torch.float32)\n",
    "test_ft_tensor = torch.tensor(test_features_ft, dtype=torch.float32)\n",
    "\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(train_deit_tensor, train_ft_tensor, train_labels_tensor)\n",
    "val_dataset = TensorDataset(val_deit_tensor, val_ft_tensor, val_labels_tensor)\n",
    "test_dataset = TensorDataset(test_deit_tensor, test_ft_tensor, test_labels_tensor)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 6. Training Setup\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_patience = 10\n",
    "epochs_no_improve = 0\n",
    "best_val_loss = np.Inf\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# 7. Training Loop with Validation and Early Stopping mechanism\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_train_correct = 0\n",
    "    for deit_inputs, ft_inputs, labels in train_loader:\n",
    "        deit_inputs = deit_inputs.to(device)\n",
    "        ft_inputs = ft_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(deit_inputs, ft_inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item() * labels.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total_train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    train_loss = total_train_loss / len(train_dataset)\n",
    "    train_accuracy = total_train_correct.double() / len(train_dataset)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    total_val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for deit_inputs, ft_inputs, labels in val_loader:\n",
    "            deit_inputs = deit_inputs.to(device)\n",
    "            ft_inputs = ft_inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(deit_inputs, ft_inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_val_loss += loss.item() * labels.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    val_loss = total_val_loss / len(val_dataset)\n",
    "    val_accuracy = total_val_correct.double() / len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f} \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# 8. Evaluate the Model on the Test Set\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "total_test_correct = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for deit_inputs, ft_inputs, labels in test_loader:\n",
    "        deit_inputs = deit_inputs.to(device)\n",
    "        ft_inputs = ft_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(deit_inputs, ft_inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        total_test_correct += torch.sum(preds == labels.data)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_accuracy = total_test_correct.double() / len(test_dataset)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f065f3f-14c9-433f-b0d7-aedac88368ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
