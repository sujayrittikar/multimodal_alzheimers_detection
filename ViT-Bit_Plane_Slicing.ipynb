{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51807170-1d84-4c68-b8ff-6827b6599159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import scipy.ndimage\n",
    "from monai.networks.nets import resnet18\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e579606-2ae8-43dd-95f9-9f9f25530b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_files_with_substring(directory, substring):\n",
    "    matching_files = [f for f in os.listdir(directory) if substring in f]\n",
    "    return matching_files\n",
    "\n",
    "def get_nib_image(adni_file_name):\n",
    "    return nib.load(adni_file_name).get_fdata()\n",
    "\n",
    "def visualize_image(nib_image):\n",
    "    plt.imshow(nib_image[:,:,nib_image.shape[2]//2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52f021e5-6aab-4e1c-89d1-6453028f7d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_file_names_for_subject(subject_id, date=None):\n",
    "    os.path.expanduser(\"~/adni_flat_dataset/adni_flat_dataset\")\n",
    "    dir_ = \"/home/rittikar-s/adni_flat_dataset/adni_flat_dataset\"\n",
    "    files = find_files_with_substring(dir_, subject_id)\n",
    "    if date:\n",
    "        files = [file for file in files if date in file]\n",
    "    file_paths = [f\"{dir_}/{file}\" for file in files]\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f01308a7-4fa8-4e4a-b42a-ccb401de8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"ADNI1_Complete_1Yr_1.5T_1_26_2025.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c4be2a0-6cf7-422c-bb27-23a77776d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets.vitautoenc import ViTAutoEnc\n",
    "\n",
    "vit_model = ViTAutoEnc(in_channels=1, patch_size=(16,16,16), img_size=(128,128,128))\n",
    "\n",
    "def get_vit_embedding(img):\n",
    "    return vit_model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9723c9f4-ec96-41be-b7c8-9f24de00ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NiftiDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, target_shape=(128, 128, 128)):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def apply_multi_bit_plane_slicing(self, img_3d, bit_planes=[6, 7]):\n",
    "        \"\"\"\n",
    "        Apply multi-bit-plane slicing to a 3D MRI image.\n",
    "        \n",
    "        Args:\n",
    "            img_3d (torch.Tensor or np.ndarray): A 3D image (shape: [depth, height, width]).\n",
    "            bit_planes (list of int): List of bit-planes to extract (0 = LSB, 7 = MSB).\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: The combined bit-plane image.\n",
    "        \"\"\"\n",
    "        X, Y, Z = img_3d.shape\n",
    "        processed_img = np.zeros_like(img_3d, dtype=np.uint8)\n",
    "        \n",
    "        if isinstance(img_3d, torch.Tensor):\n",
    "            img_3d = img_3d.cpu().numpy()  # Convert to NumPy if it's a tensor\n",
    "    \n",
    "        for z in range(Z):  # Iterate through each slice dynamically\n",
    "            slice_img = img_3d[:, :, z]  # Extract the 2D slice\n",
    "            \n",
    "            # Ensure the slice is in 8-bit format\n",
    "            slice_img = (slice_img / np.max(slice_img) * 255).astype(np.uint8)  \n",
    "            \n",
    "            bit_sliced = np.zeros_like(slice_img, dtype=np.uint8)\n",
    "    \n",
    "            # Combine selected bit planes\n",
    "            for bit in bit_planes:\n",
    "                bit_sliced |= ((slice_img >> bit) & 1) << bit  \n",
    "    \n",
    "            processed_img[:, :, z] = bit_sliced  # Store processed slice back\n",
    "    \n",
    "        return processed_img\n",
    "    \n",
    "    def preprocess_nifti(self, nifti_path, threshold_value=80):\n",
    "        # Load the NIfTI file\n",
    "        img = nib.load(nifti_path).get_fdata()\n",
    "    \n",
    "        # Resize the image to the target shape\n",
    "        img_resized = scipy.ndimage.zoom(img, np.array(self.target_shape) / np.array(img.shape), order=1)\n",
    "    \n",
    "        # Normalize intensity to [0, 1] for neural network\n",
    "        img_normalized = (img_resized - np.min(img_resized)) / (np.max(img_resized) - np.min(img_resized) + 1e-8)\n",
    "        # img_normalized = (img_normalized * 255).astype(np.uint8)  # You can adjust this based on model input needs\n",
    "    \n",
    "        # Create an empty array to store the processed result\n",
    "        processed_img = self.apply_multi_bit_plane_slicing(img_normalized, [5, 6, 7])\n",
    "    \n",
    "        # Convert the processed image to a tensor and add a channel dimension (assuming 1 channel)\n",
    "        return torch.tensor(processed_img, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.preprocess_nifti(self.image_paths[idx])\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        embedding = get_vit_embedding(image.reshape(1,1,128,128,128))\n",
    "        return embedding, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62036a9f-e976-4fd0-aef5-4ea71b88072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_label = {\n",
    "    \"CN\": 0,\n",
    "    \"MCI\": 1,\n",
    "    \"AD\": 2\n",
    "}\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    subject = row[\"Subject\"]\n",
    "    date = row[\"Acq Date\"]\n",
    "    date = date.replace(\"/\", \"-\")\n",
    "    image_path = get_image_file_names_for_subject(subject, date)[0]\n",
    "    image_paths.append(image_path)\n",
    "    labels.append(class_to_label[row[\"Group\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc86ce3e-ef67-4492-b81e-4174b4f8084f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2294"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac5306e8-85f3-4a47-9e2d-048b87bdb7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2294"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ff88cf5-1448-4d1f-aa93-964640bfa9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(image_paths, labels, test_size=0.3, random_state=42, stratify=labels)\n",
    "val_paths, test_paths, val_labels, test_labels = train_test_split(test_paths, test_labels, test_size=0.5, random_state=42, stratify=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6107f004-c758-45c2-a4c9-6c352027cb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Batches: 402, Val Batches: 86, Test Batches: 87\n"
     ]
    }
   ],
   "source": [
    "# Create train & test datasets\n",
    "train_dataset = NiftiDataset(train_paths, train_labels)\n",
    "val_dataset = NiftiDataset(val_paths, val_labels)\n",
    "test_dataset = NiftiDataset(test_paths, test_labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, pin_memory=True)\n",
    "\n",
    "print(f\"Train Batches: {len(train_loader)}, Val Batches: {len(val_loader)}, Test Batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "175d94f2-5922-4e14-9b95-f428d1555e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def save_embeddings_hdf5(dataloader, filename):\n",
    "    \"\"\"Save embeddings (from list format) and labels incrementally to an HDF5 file.\"\"\"\n",
    "    with h5py.File(filename, \"w\") as f:\n",
    "        first_batch = True\n",
    "        for i, (embedding_list, label) in enumerate(dataloader):\n",
    "            # Extract the last layer embeddings\n",
    "            embedding_tensor = embedding_list[1][-1]  # Extract final layer embeddings\n",
    "            embedding_tensor = embedding_tensor.cpu().detach()  # Move to CPU\n",
    "            \n",
    "            embedding_numpy = embedding_tensor.numpy()  # Convert to NumPy\n",
    "            label_numpy = label.cpu().numpy()\n",
    "\n",
    "            # Reshape embeddings if needed\n",
    "            embedding_numpy = embedding_numpy.reshape(embedding_numpy.shape[0], -1)  # (4, 1, 512, 768) → (4, 512 * 768)\n",
    "\n",
    "            if first_batch:\n",
    "                # Create expandable datasets with correct shape\n",
    "                f.create_dataset(\"embeddings\", data=embedding_numpy, \n",
    "                                 maxshape=(None, embedding_numpy.shape[1]))  # Now 2D\n",
    "                f.create_dataset(\"labels\", data=label_numpy, maxshape=(None,))\n",
    "                first_batch = False\n",
    "            else:\n",
    "                # Resize and append new embeddings\n",
    "                f[\"embeddings\"].resize((f[\"embeddings\"].shape[0] + embedding_numpy.shape[0]), axis=0)\n",
    "                f[\"embeddings\"][-embedding_numpy.shape[0]:] = embedding_numpy\n",
    "\n",
    "                f[\"labels\"].resize((f[\"labels\"].shape[0] + label_numpy.shape[0]), axis=0)\n",
    "                f[\"labels\"][-label_numpy.shape[0]:] = label_numpy\n",
    "            \n",
    "            print(f\"Saved embeddings for batch: {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a120898e-b143-40a4-a31f-3fabcb857139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings for batch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in divide\n",
      "invalid value encountered in cast\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings for batch: 2\n",
      "Saved embeddings for batch: 3\n",
      "Saved embeddings for batch: 4\n",
      "Saved embeddings for batch: 5\n",
      "Saved embeddings for batch: 6\n",
      "Saved embeddings for batch: 7\n",
      "Saved embeddings for batch: 8\n",
      "Saved embeddings for batch: 9\n",
      "Saved embeddings for batch: 10\n",
      "Saved embeddings for batch: 11\n",
      "Saved embeddings for batch: 12\n",
      "Saved embeddings for batch: 13\n",
      "Saved embeddings for batch: 14\n",
      "Saved embeddings for batch: 15\n",
      "Saved embeddings for batch: 16\n",
      "Saved embeddings for batch: 17\n",
      "Saved embeddings for batch: 18\n",
      "Saved embeddings for batch: 19\n",
      "Saved embeddings for batch: 20\n",
      "Saved embeddings for batch: 21\n",
      "Saved embeddings for batch: 22\n",
      "Saved embeddings for batch: 23\n",
      "Saved embeddings for batch: 24\n",
      "Saved embeddings for batch: 25\n",
      "Saved embeddings for batch: 26\n",
      "Saved embeddings for batch: 27\n",
      "Saved embeddings for batch: 28\n",
      "Saved embeddings for batch: 29\n",
      "Saved embeddings for batch: 30\n",
      "Saved embeddings for batch: 31\n",
      "Saved embeddings for batch: 32\n",
      "Saved embeddings for batch: 33\n",
      "Saved embeddings for batch: 34\n",
      "Saved embeddings for batch: 35\n",
      "Saved embeddings for batch: 36\n",
      "Saved embeddings for batch: 37\n",
      "Saved embeddings for batch: 38\n",
      "Saved embeddings for batch: 39\n",
      "Saved embeddings for batch: 40\n",
      "Saved embeddings for batch: 41\n",
      "Saved embeddings for batch: 42\n",
      "Saved embeddings for batch: 43\n",
      "Saved embeddings for batch: 44\n",
      "Saved embeddings for batch: 45\n",
      "Saved embeddings for batch: 46\n",
      "Saved embeddings for batch: 47\n",
      "Saved embeddings for batch: 48\n",
      "Saved embeddings for batch: 49\n",
      "Saved embeddings for batch: 50\n",
      "Saved embeddings for batch: 51\n",
      "Saved embeddings for batch: 52\n",
      "Saved embeddings for batch: 53\n",
      "Saved embeddings for batch: 54\n",
      "Saved embeddings for batch: 55\n",
      "Saved embeddings for batch: 56\n",
      "Saved embeddings for batch: 57\n",
      "Saved embeddings for batch: 58\n",
      "Saved embeddings for batch: 59\n",
      "Saved embeddings for batch: 60\n",
      "Saved embeddings for batch: 61\n",
      "Saved embeddings for batch: 62\n",
      "Saved embeddings for batch: 63\n",
      "Saved embeddings for batch: 64\n",
      "Saved embeddings for batch: 65\n",
      "Saved embeddings for batch: 66\n",
      "Saved embeddings for batch: 67\n",
      "Saved embeddings for batch: 68\n",
      "Saved embeddings for batch: 69\n",
      "Saved embeddings for batch: 70\n",
      "Saved embeddings for batch: 71\n",
      "Saved embeddings for batch: 72\n",
      "Saved embeddings for batch: 73\n",
      "Saved embeddings for batch: 74\n",
      "Saved embeddings for batch: 75\n",
      "Saved embeddings for batch: 76\n",
      "Saved embeddings for batch: 77\n",
      "Saved embeddings for batch: 78\n",
      "Saved embeddings for batch: 79\n",
      "Saved embeddings for batch: 80\n",
      "Saved embeddings for batch: 81\n",
      "Saved embeddings for batch: 82\n",
      "Saved embeddings for batch: 83\n",
      "Saved embeddings for batch: 84\n",
      "Saved embeddings for batch: 85\n",
      "Saved embeddings for batch: 86\n",
      "Saved embeddings for batch: 87\n",
      "Saved embeddings for batch: 88\n",
      "Saved embeddings for batch: 89\n",
      "Saved embeddings for batch: 90\n",
      "Saved embeddings for batch: 91\n",
      "Saved embeddings for batch: 92\n",
      "Saved embeddings for batch: 93\n",
      "Saved embeddings for batch: 94\n",
      "Saved embeddings for batch: 95\n",
      "Saved embeddings for batch: 96\n",
      "Saved embeddings for batch: 97\n",
      "Saved embeddings for batch: 98\n",
      "Saved embeddings for batch: 99\n",
      "Saved embeddings for batch: 100\n",
      "Saved embeddings for batch: 101\n",
      "Saved embeddings for batch: 102\n",
      "Saved embeddings for batch: 103\n",
      "Saved embeddings for batch: 104\n",
      "Saved embeddings for batch: 105\n",
      "Saved embeddings for batch: 106\n",
      "Saved embeddings for batch: 107\n",
      "Saved embeddings for batch: 108\n",
      "Saved embeddings for batch: 109\n",
      "Saved embeddings for batch: 110\n",
      "Saved embeddings for batch: 111\n",
      "Saved embeddings for batch: 112\n",
      "Saved embeddings for batch: 113\n",
      "Saved embeddings for batch: 114\n",
      "Saved embeddings for batch: 115\n",
      "Saved embeddings for batch: 116\n",
      "Saved embeddings for batch: 117\n",
      "Saved embeddings for batch: 118\n",
      "Saved embeddings for batch: 119\n",
      "Saved embeddings for batch: 120\n",
      "Saved embeddings for batch: 121\n",
      "Saved embeddings for batch: 122\n",
      "Saved embeddings for batch: 123\n",
      "Saved embeddings for batch: 124\n",
      "Saved embeddings for batch: 125\n",
      "Saved embeddings for batch: 126\n",
      "Saved embeddings for batch: 127\n",
      "Saved embeddings for batch: 128\n",
      "Saved embeddings for batch: 129\n",
      "Saved embeddings for batch: 130\n",
      "Saved embeddings for batch: 131\n",
      "Saved embeddings for batch: 132\n",
      "Saved embeddings for batch: 133\n",
      "Saved embeddings for batch: 134\n",
      "Saved embeddings for batch: 135\n",
      "Saved embeddings for batch: 136\n",
      "Saved embeddings for batch: 137\n",
      "Saved embeddings for batch: 138\n",
      "Saved embeddings for batch: 139\n",
      "Saved embeddings for batch: 140\n",
      "Saved embeddings for batch: 141\n",
      "Saved embeddings for batch: 142\n",
      "Saved embeddings for batch: 143\n",
      "Saved embeddings for batch: 144\n",
      "Saved embeddings for batch: 145\n",
      "Saved embeddings for batch: 146\n",
      "Saved embeddings for batch: 147\n",
      "Saved embeddings for batch: 148\n",
      "Saved embeddings for batch: 149\n",
      "Saved embeddings for batch: 150\n",
      "Saved embeddings for batch: 151\n",
      "Saved embeddings for batch: 152\n",
      "Saved embeddings for batch: 153\n",
      "Saved embeddings for batch: 154\n",
      "Saved embeddings for batch: 155\n",
      "Saved embeddings for batch: 156\n",
      "Saved embeddings for batch: 157\n",
      "Saved embeddings for batch: 158\n",
      "Saved embeddings for batch: 159\n",
      "Saved embeddings for batch: 160\n",
      "Saved embeddings for batch: 161\n",
      "Saved embeddings for batch: 162\n",
      "Saved embeddings for batch: 163\n",
      "Saved embeddings for batch: 164\n",
      "Saved embeddings for batch: 165\n",
      "Saved embeddings for batch: 166\n",
      "Saved embeddings for batch: 167\n",
      "Saved embeddings for batch: 168\n",
      "Saved embeddings for batch: 169\n",
      "Saved embeddings for batch: 170\n",
      "Saved embeddings for batch: 171\n",
      "Saved embeddings for batch: 172\n",
      "Saved embeddings for batch: 173\n",
      "Saved embeddings for batch: 174\n",
      "Saved embeddings for batch: 175\n",
      "Saved embeddings for batch: 176\n",
      "Saved embeddings for batch: 177\n",
      "Saved embeddings for batch: 178\n",
      "Saved embeddings for batch: 179\n",
      "Saved embeddings for batch: 180\n",
      "Saved embeddings for batch: 181\n",
      "Saved embeddings for batch: 182\n",
      "Saved embeddings for batch: 183\n",
      "Saved embeddings for batch: 184\n",
      "Saved embeddings for batch: 185\n",
      "Saved embeddings for batch: 186\n",
      "Saved embeddings for batch: 187\n",
      "Saved embeddings for batch: 188\n",
      "Saved embeddings for batch: 189\n",
      "Saved embeddings for batch: 190\n",
      "Saved embeddings for batch: 191\n",
      "Saved embeddings for batch: 192\n",
      "Saved embeddings for batch: 193\n",
      "Saved embeddings for batch: 194\n",
      "Saved embeddings for batch: 195\n",
      "Saved embeddings for batch: 196\n",
      "Saved embeddings for batch: 197\n",
      "Saved embeddings for batch: 198\n",
      "Saved embeddings for batch: 199\n",
      "Saved embeddings for batch: 200\n",
      "Saved embeddings for batch: 201\n",
      "Saved embeddings for batch: 202\n",
      "Saved embeddings for batch: 203\n",
      "Saved embeddings for batch: 204\n",
      "Saved embeddings for batch: 205\n",
      "Saved embeddings for batch: 206\n",
      "Saved embeddings for batch: 207\n",
      "Saved embeddings for batch: 208\n",
      "Saved embeddings for batch: 209\n",
      "Saved embeddings for batch: 210\n",
      "Saved embeddings for batch: 211\n",
      "Saved embeddings for batch: 212\n",
      "Saved embeddings for batch: 213\n",
      "Saved embeddings for batch: 214\n",
      "Saved embeddings for batch: 215\n",
      "Saved embeddings for batch: 216\n",
      "Saved embeddings for batch: 217\n",
      "Saved embeddings for batch: 218\n",
      "Saved embeddings for batch: 219\n",
      "Saved embeddings for batch: 220\n",
      "Saved embeddings for batch: 221\n",
      "Saved embeddings for batch: 222\n",
      "Saved embeddings for batch: 223\n",
      "Saved embeddings for batch: 224\n",
      "Saved embeddings for batch: 225\n",
      "Saved embeddings for batch: 226\n",
      "Saved embeddings for batch: 227\n",
      "Saved embeddings for batch: 228\n",
      "Saved embeddings for batch: 229\n",
      "Saved embeddings for batch: 230\n",
      "Saved embeddings for batch: 231\n",
      "Saved embeddings for batch: 232\n",
      "Saved embeddings for batch: 233\n",
      "Saved embeddings for batch: 234\n",
      "Saved embeddings for batch: 235\n",
      "Saved embeddings for batch: 236\n",
      "Saved embeddings for batch: 237\n",
      "Saved embeddings for batch: 238\n",
      "Saved embeddings for batch: 239\n",
      "Saved embeddings for batch: 240\n",
      "Saved embeddings for batch: 241\n",
      "Saved embeddings for batch: 242\n",
      "Saved embeddings for batch: 243\n",
      "Saved embeddings for batch: 244\n",
      "Saved embeddings for batch: 245\n",
      "Saved embeddings for batch: 246\n",
      "Saved embeddings for batch: 247\n",
      "Saved embeddings for batch: 248\n",
      "Saved embeddings for batch: 249\n",
      "Saved embeddings for batch: 250\n",
      "Saved embeddings for batch: 251\n",
      "Saved embeddings for batch: 252\n",
      "Saved embeddings for batch: 253\n",
      "Saved embeddings for batch: 254\n",
      "Saved embeddings for batch: 255\n",
      "Saved embeddings for batch: 256\n",
      "Saved embeddings for batch: 257\n",
      "Saved embeddings for batch: 258\n",
      "Saved embeddings for batch: 259\n",
      "Saved embeddings for batch: 260\n",
      "Saved embeddings for batch: 261\n",
      "Saved embeddings for batch: 262\n",
      "Saved embeddings for batch: 263\n",
      "Saved embeddings for batch: 264\n",
      "Saved embeddings for batch: 265\n",
      "Saved embeddings for batch: 266\n",
      "Saved embeddings for batch: 267\n",
      "Saved embeddings for batch: 268\n",
      "Saved embeddings for batch: 269\n",
      "Saved embeddings for batch: 270\n",
      "Saved embeddings for batch: 271\n",
      "Saved embeddings for batch: 272\n",
      "Saved embeddings for batch: 273\n",
      "Saved embeddings for batch: 274\n",
      "Saved embeddings for batch: 275\n",
      "Saved embeddings for batch: 276\n",
      "Saved embeddings for batch: 277\n",
      "Saved embeddings for batch: 278\n",
      "Saved embeddings for batch: 279\n",
      "Saved embeddings for batch: 280\n",
      "Saved embeddings for batch: 281\n",
      "Saved embeddings for batch: 282\n",
      "Saved embeddings for batch: 283\n",
      "Saved embeddings for batch: 284\n",
      "Saved embeddings for batch: 285\n",
      "Saved embeddings for batch: 286\n",
      "Saved embeddings for batch: 287\n",
      "Saved embeddings for batch: 288\n",
      "Saved embeddings for batch: 289\n",
      "Saved embeddings for batch: 290\n",
      "Saved embeddings for batch: 291\n",
      "Saved embeddings for batch: 292\n",
      "Saved embeddings for batch: 293\n",
      "Saved embeddings for batch: 294\n",
      "Saved embeddings for batch: 295\n",
      "Saved embeddings for batch: 296\n",
      "Saved embeddings for batch: 297\n",
      "Saved embeddings for batch: 298\n",
      "Saved embeddings for batch: 299\n",
      "Saved embeddings for batch: 300\n",
      "Saved embeddings for batch: 301\n",
      "Saved embeddings for batch: 302\n",
      "Saved embeddings for batch: 303\n",
      "Saved embeddings for batch: 304\n",
      "Saved embeddings for batch: 305\n",
      "Saved embeddings for batch: 306\n",
      "Saved embeddings for batch: 307\n",
      "Saved embeddings for batch: 308\n",
      "Saved embeddings for batch: 309\n",
      "Saved embeddings for batch: 310\n",
      "Saved embeddings for batch: 311\n",
      "Saved embeddings for batch: 312\n",
      "Saved embeddings for batch: 313\n",
      "Saved embeddings for batch: 314\n",
      "Saved embeddings for batch: 315\n",
      "Saved embeddings for batch: 316\n",
      "Saved embeddings for batch: 317\n",
      "Saved embeddings for batch: 318\n",
      "Saved embeddings for batch: 319\n",
      "Saved embeddings for batch: 320\n",
      "Saved embeddings for batch: 321\n",
      "Saved embeddings for batch: 322\n",
      "Saved embeddings for batch: 323\n",
      "Saved embeddings for batch: 324\n",
      "Saved embeddings for batch: 325\n",
      "Saved embeddings for batch: 326\n",
      "Saved embeddings for batch: 327\n",
      "Saved embeddings for batch: 328\n",
      "Saved embeddings for batch: 329\n",
      "Saved embeddings for batch: 330\n",
      "Saved embeddings for batch: 331\n",
      "Saved embeddings for batch: 332\n",
      "Saved embeddings for batch: 333\n",
      "Saved embeddings for batch: 334\n",
      "Saved embeddings for batch: 335\n",
      "Saved embeddings for batch: 336\n",
      "Saved embeddings for batch: 337\n",
      "Saved embeddings for batch: 338\n",
      "Saved embeddings for batch: 339\n",
      "Saved embeddings for batch: 340\n",
      "Saved embeddings for batch: 341\n",
      "Saved embeddings for batch: 342\n",
      "Saved embeddings for batch: 343\n",
      "Saved embeddings for batch: 344\n",
      "Saved embeddings for batch: 345\n",
      "Saved embeddings for batch: 346\n",
      "Saved embeddings for batch: 347\n",
      "Saved embeddings for batch: 348\n",
      "Saved embeddings for batch: 349\n",
      "Saved embeddings for batch: 350\n",
      "Saved embeddings for batch: 351\n",
      "Saved embeddings for batch: 352\n",
      "Saved embeddings for batch: 353\n",
      "Saved embeddings for batch: 354\n",
      "Saved embeddings for batch: 355\n",
      "Saved embeddings for batch: 356\n",
      "Saved embeddings for batch: 357\n",
      "Saved embeddings for batch: 358\n",
      "Saved embeddings for batch: 359\n",
      "Saved embeddings for batch: 360\n",
      "Saved embeddings for batch: 361\n",
      "Saved embeddings for batch: 362\n",
      "Saved embeddings for batch: 363\n",
      "Saved embeddings for batch: 364\n",
      "Saved embeddings for batch: 365\n",
      "Saved embeddings for batch: 366\n",
      "Saved embeddings for batch: 367\n",
      "Saved embeddings for batch: 368\n",
      "Saved embeddings for batch: 369\n",
      "Saved embeddings for batch: 370\n",
      "Saved embeddings for batch: 371\n",
      "Saved embeddings for batch: 372\n",
      "Saved embeddings for batch: 373\n",
      "Saved embeddings for batch: 374\n",
      "Saved embeddings for batch: 375\n",
      "Saved embeddings for batch: 376\n",
      "Saved embeddings for batch: 377\n",
      "Saved embeddings for batch: 378\n",
      "Saved embeddings for batch: 379\n",
      "Saved embeddings for batch: 380\n",
      "Saved embeddings for batch: 381\n",
      "Saved embeddings for batch: 382\n",
      "Saved embeddings for batch: 383\n",
      "Saved embeddings for batch: 384\n",
      "Saved embeddings for batch: 385\n",
      "Saved embeddings for batch: 386\n",
      "Saved embeddings for batch: 387\n",
      "Saved embeddings for batch: 388\n",
      "Saved embeddings for batch: 389\n",
      "Saved embeddings for batch: 390\n",
      "Saved embeddings for batch: 391\n",
      "Saved embeddings for batch: 392\n",
      "Saved embeddings for batch: 393\n",
      "Saved embeddings for batch: 394\n",
      "Saved embeddings for batch: 395\n",
      "Saved embeddings for batch: 396\n",
      "Saved embeddings for batch: 397\n",
      "Saved embeddings for batch: 398\n",
      "Saved embeddings for batch: 399\n",
      "Saved embeddings for batch: 400\n",
      "Saved embeddings for batch: 401\n",
      "Saved embeddings for batch: 402\n",
      "Saved embeddings for batch: 1\n",
      "Saved embeddings for batch: 2\n",
      "Saved embeddings for batch: 3\n",
      "Saved embeddings for batch: 4\n",
      "Saved embeddings for batch: 5\n",
      "Saved embeddings for batch: 6\n",
      "Saved embeddings for batch: 7\n",
      "Saved embeddings for batch: 8\n",
      "Saved embeddings for batch: 9\n",
      "Saved embeddings for batch: 10\n",
      "Saved embeddings for batch: 11\n",
      "Saved embeddings for batch: 12\n",
      "Saved embeddings for batch: 13\n",
      "Saved embeddings for batch: 14\n",
      "Saved embeddings for batch: 15\n",
      "Saved embeddings for batch: 16\n",
      "Saved embeddings for batch: 17\n",
      "Saved embeddings for batch: 18\n",
      "Saved embeddings for batch: 19\n",
      "Saved embeddings for batch: 20\n",
      "Saved embeddings for batch: 21\n",
      "Saved embeddings for batch: 22\n",
      "Saved embeddings for batch: 23\n",
      "Saved embeddings for batch: 24\n",
      "Saved embeddings for batch: 25\n",
      "Saved embeddings for batch: 26\n",
      "Saved embeddings for batch: 27\n",
      "Saved embeddings for batch: 28\n",
      "Saved embeddings for batch: 29\n",
      "Saved embeddings for batch: 30\n",
      "Saved embeddings for batch: 31\n",
      "Saved embeddings for batch: 32\n",
      "Saved embeddings for batch: 33\n",
      "Saved embeddings for batch: 34\n",
      "Saved embeddings for batch: 35\n",
      "Saved embeddings for batch: 36\n",
      "Saved embeddings for batch: 37\n",
      "Saved embeddings for batch: 38\n",
      "Saved embeddings for batch: 39\n",
      "Saved embeddings for batch: 40\n",
      "Saved embeddings for batch: 41\n",
      "Saved embeddings for batch: 42\n",
      "Saved embeddings for batch: 43\n",
      "Saved embeddings for batch: 44\n",
      "Saved embeddings for batch: 45\n",
      "Saved embeddings for batch: 46\n",
      "Saved embeddings for batch: 47\n",
      "Saved embeddings for batch: 48\n",
      "Saved embeddings for batch: 49\n",
      "Saved embeddings for batch: 50\n",
      "Saved embeddings for batch: 51\n",
      "Saved embeddings for batch: 52\n",
      "Saved embeddings for batch: 53\n",
      "Saved embeddings for batch: 54\n",
      "Saved embeddings for batch: 55\n",
      "Saved embeddings for batch: 56\n",
      "Saved embeddings for batch: 57\n",
      "Saved embeddings for batch: 58\n",
      "Saved embeddings for batch: 59\n",
      "Saved embeddings for batch: 60\n",
      "Saved embeddings for batch: 61\n",
      "Saved embeddings for batch: 62\n",
      "Saved embeddings for batch: 63\n",
      "Saved embeddings for batch: 64\n",
      "Saved embeddings for batch: 65\n",
      "Saved embeddings for batch: 66\n",
      "Saved embeddings for batch: 67\n",
      "Saved embeddings for batch: 68\n",
      "Saved embeddings for batch: 69\n",
      "Saved embeddings for batch: 70\n",
      "Saved embeddings for batch: 71\n",
      "Saved embeddings for batch: 72\n",
      "Saved embeddings for batch: 73\n",
      "Saved embeddings for batch: 74\n",
      "Saved embeddings for batch: 75\n",
      "Saved embeddings for batch: 76\n",
      "Saved embeddings for batch: 77\n",
      "Saved embeddings for batch: 78\n",
      "Saved embeddings for batch: 79\n",
      "Saved embeddings for batch: 80\n",
      "Saved embeddings for batch: 81\n",
      "Saved embeddings for batch: 82\n",
      "Saved embeddings for batch: 83\n",
      "Saved embeddings for batch: 84\n",
      "Saved embeddings for batch: 85\n",
      "Saved embeddings for batch: 86\n",
      "Saved embeddings for batch: 1\n",
      "Saved embeddings for batch: 2\n",
      "Saved embeddings for batch: 3\n",
      "Saved embeddings for batch: 4\n",
      "Saved embeddings for batch: 5\n",
      "Saved embeddings for batch: 6\n",
      "Saved embeddings for batch: 7\n",
      "Saved embeddings for batch: 8\n",
      "Saved embeddings for batch: 9\n",
      "Saved embeddings for batch: 10\n",
      "Saved embeddings for batch: 11\n",
      "Saved embeddings for batch: 12\n",
      "Saved embeddings for batch: 13\n",
      "Saved embeddings for batch: 14\n",
      "Saved embeddings for batch: 15\n",
      "Saved embeddings for batch: 16\n",
      "Saved embeddings for batch: 17\n",
      "Saved embeddings for batch: 18\n",
      "Saved embeddings for batch: 19\n",
      "Saved embeddings for batch: 20\n",
      "Saved embeddings for batch: 21\n",
      "Saved embeddings for batch: 22\n",
      "Saved embeddings for batch: 23\n",
      "Saved embeddings for batch: 24\n",
      "Saved embeddings for batch: 25\n",
      "Saved embeddings for batch: 26\n",
      "Saved embeddings for batch: 27\n",
      "Saved embeddings for batch: 28\n",
      "Saved embeddings for batch: 29\n",
      "Saved embeddings for batch: 30\n",
      "Saved embeddings for batch: 31\n",
      "Saved embeddings for batch: 32\n",
      "Saved embeddings for batch: 33\n",
      "Saved embeddings for batch: 34\n",
      "Saved embeddings for batch: 35\n",
      "Saved embeddings for batch: 36\n",
      "Saved embeddings for batch: 37\n",
      "Saved embeddings for batch: 38\n",
      "Saved embeddings for batch: 39\n",
      "Saved embeddings for batch: 40\n",
      "Saved embeddings for batch: 41\n",
      "Saved embeddings for batch: 42\n",
      "Saved embeddings for batch: 43\n",
      "Saved embeddings for batch: 44\n",
      "Saved embeddings for batch: 45\n",
      "Saved embeddings for batch: 46\n",
      "Saved embeddings for batch: 47\n",
      "Saved embeddings for batch: 48\n",
      "Saved embeddings for batch: 49\n",
      "Saved embeddings for batch: 50\n",
      "Saved embeddings for batch: 51\n",
      "Saved embeddings for batch: 52\n",
      "Saved embeddings for batch: 53\n",
      "Saved embeddings for batch: 54\n",
      "Saved embeddings for batch: 55\n",
      "Saved embeddings for batch: 56\n",
      "Saved embeddings for batch: 57\n",
      "Saved embeddings for batch: 58\n",
      "Saved embeddings for batch: 59\n",
      "Saved embeddings for batch: 60\n",
      "Saved embeddings for batch: 61\n",
      "Saved embeddings for batch: 62\n",
      "Saved embeddings for batch: 63\n",
      "Saved embeddings for batch: 64\n",
      "Saved embeddings for batch: 65\n",
      "Saved embeddings for batch: 66\n",
      "Saved embeddings for batch: 67\n",
      "Saved embeddings for batch: 68\n",
      "Saved embeddings for batch: 69\n",
      "Saved embeddings for batch: 70\n",
      "Saved embeddings for batch: 71\n",
      "Saved embeddings for batch: 72\n",
      "Saved embeddings for batch: 73\n",
      "Saved embeddings for batch: 74\n",
      "Saved embeddings for batch: 75\n",
      "Saved embeddings for batch: 76\n",
      "Saved embeddings for batch: 77\n",
      "Saved embeddings for batch: 78\n",
      "Saved embeddings for batch: 79\n",
      "Saved embeddings for batch: 80\n",
      "Saved embeddings for batch: 81\n",
      "Saved embeddings for batch: 82\n",
      "Saved embeddings for batch: 83\n",
      "Saved embeddings for batch: 84\n",
      "Saved embeddings for batch: 85\n",
      "Saved embeddings for batch: 86\n",
      "Saved embeddings for batch: 87\n"
     ]
    }
   ],
   "source": [
    "save_embeddings_hdf5(train_loader, \"train_embeddings.h5\")\n",
    "save_embeddings_hdf5(val_loader, \"val_embeddings.h5\")\n",
    "save_embeddings_hdf5(test_loader, \"test_embeddings.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39ae1d98-9812-4221-afa4-17432bf7a7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "504d8865-b072-44d8-bec1-2d1af005019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_hdf5(filename, batch_size=4):\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        num_samples = f[\"embeddings\"].shape[0]  # Total samples\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            X_batch = torch.tensor(f[\"embeddings\"][i : i + batch_size], dtype=torch.float32).to(DEVICE)\n",
    "            y_batch = torch.tensor(f[\"labels\"][i : i + batch_size], dtype=torch.long).to(DEVICE)\n",
    "            yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ebd750f-ea7f-4567-a9ba-681ba901667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1024)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(1024)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(512)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(256)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc4 = nn.Linear(256, num_classes)  # Final layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        if x.shape[0] > 1:  # Apply BatchNorm only if batch size > 1\n",
    "            x = self.batch_norm1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "    \n",
    "        x = self.fc2(x)\n",
    "        if x.shape[0] > 1:\n",
    "            x = self.batch_norm2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "    \n",
    "        x = self.fc3(x)\n",
    "        if x.shape[0] > 1:\n",
    "            x = self.batch_norm3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9af434c0-713f-4747-add1-0dab59e5a131",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = next(load_embeddings_hdf5(\"train_embeddings.h5\"))[0].shape[1]  # Get feature size\n",
    "num_classes = 3  # Adjust based on labels\n",
    "model = MLPClassifier(input_dim, num_classes).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "376a2515-00cd-49fb-8c3d-a5d6a333e8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(\n",
       "  (fc1): Linear(in_features=393216, out_features=1024, bias=True)\n",
       "  (batch_norm1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (batch_norm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (batch_norm3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU()\n",
       "  (dropout3): Dropout(p=0.5, inplace=False)\n",
       "  (fc4): Linear(in_features=256, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53eb668e-d4e1-4963-b601-55afd0f07f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights\n",
    "# classes = np.unique(train_labels)\n",
    "# class_weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=train_labels)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights).to(DEVICE)).to(DEVICE)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e834bcc3-7120-45dd-bfd3-bbae86e4d540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "#     model = torch.nn.DataParallel(model)\n",
    "\n",
    "# model = model.to(\"cuda\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a8a0a0e-c841-46df-9644-88bf0b8a4420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "`torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "`torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.                         | 0/100 [00:00<?, ?it/s]\n",
      "Epoch 1/100 Train: 101it [00:11,  8.64it/s, acc=35.1, loss=0.758]                                                                                           \n",
      "Validation:  22%|██████████████████▋                                                                  | 22/100 [00:00<00:01, 50.72it/s, acc=35.5, loss=1.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]\n",
      "  Train Loss: 0.0727 | Train Acc: 35.08%\n",
      "  Val Loss: 0.0679 | Val Acc: 35.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 Train: 101it [00:11,  8.65it/s, acc=40.7, loss=1.27]                                                                                            \n",
      "Validation:  22%|██████████████████▋                                                                  | 22/100 [00:00<00:01, 50.83it/s, acc=40.4, loss=1.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100]\n",
      "  Train Loss: 0.0678 | Train Acc: 40.75%\n",
      "  Val Loss: 0.0667 | Val Acc: 40.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 Train: 101it [00:11,  8.69it/s, acc=47.5, loss=1.37]                                                                                            \n",
      "Validation:  22%|██████████████████▋                                                                  | 22/100 [00:00<00:01, 50.00it/s, acc=42.2, loss=1.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100]\n",
      "  Train Loss: 0.0624 | Train Acc: 47.48%\n",
      "  Val Loss: 0.0654 | Val Acc: 42.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 Train: 101it [00:11,  8.62it/s, acc=50.2, loss=0.875]                                                                                           \n",
      "Validation:  22%|██████████████████▋                                                                  | 22/100 [00:00<00:01, 51.87it/s, acc=47.4, loss=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100]\n",
      "  Train Loss: 0.0608 | Train Acc: 50.16%\n",
      "  Val Loss: 0.0637 | Val Acc: 47.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 Train: 101it [00:11,  8.64it/s, acc=53.6, loss=1.05]                                                                                            \n",
      "Validation:  22%|███████████████████▎                                                                    | 22/100 [00:00<00:01, 50.31it/s, acc=52.9, loss=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100]\n",
      "  Train Loss: 0.0571 | Train Acc: 53.58%\n",
      "  Val Loss: 0.0621 | Val Acc: 52.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 Train: 101it [00:11,  8.64it/s, acc=60.1, loss=0.983]                                                                                           \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 49.89it/s, acc=53.2, loss=0.992]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100]\n",
      "  Train Loss: 0.0529 | Train Acc: 60.06%\n",
      "  Val Loss: 0.0620 | Val Acc: 53.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 Train: 101it [00:11,  8.63it/s, acc=63.3, loss=0.786]                                                                                           \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 50.44it/s, acc=53.8, loss=0.918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100]\n",
      "  Train Loss: 0.0508 | Train Acc: 63.30%\n",
      "  Val Loss: 0.0606 | Val Acc: 53.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 Train: 101it [00:11,  8.62it/s, acc=66.4, loss=0.697]                                                                                           \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 50.07it/s, acc=56.1, loss=0.913]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100]\n",
      "  Train Loss: 0.0481 | Train Acc: 66.36%\n",
      "  Val Loss: 0.0594 | Val Acc: 56.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 Train: 101it [00:11,  8.63it/s, acc=69, loss=1.02]                                                                                              \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 50.41it/s, acc=58.1, loss=0.906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100]\n",
      "  Train Loss: 0.0455 | Train Acc: 69.03%\n",
      "  Val Loss: 0.0581 | Val Acc: 58.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 Train: 101it [00:11,  8.61it/s, acc=74.3, loss=0.506]                                                                                          \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 51.55it/s, acc=57.6, loss=0.831]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100]\n",
      "  Train Loss: 0.0413 | Train Acc: 74.27%\n",
      "  Val Loss: 0.0576 | Val Acc: 57.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 Train: 101it [00:11,  8.64it/s, acc=78.4, loss=0.48]                                                                                           \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 51.29it/s, acc=59.3, loss=0.831]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100]\n",
      "  Train Loss: 0.0382 | Train Acc: 78.38%\n",
      "  Val Loss: 0.0557 | Val Acc: 59.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 Train: 101it [00:11,  8.64it/s, acc=79.3, loss=0.613]                                                                                          \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 49.25it/s, acc=61.3, loss=0.769]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100]\n",
      "  Train Loss: 0.0353 | Train Acc: 79.31%\n",
      "  Val Loss: 0.0552 | Val Acc: 61.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 Train: 101it [00:11,  8.62it/s, acc=84.5, loss=1.05]                                                                                           \n",
      "Validation:  22%|██████████████████▉                                                                   | 22/100 [00:00<00:01, 49.37it/s, acc=61, loss=0.726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100]\n",
      "  Train Loss: 0.0326 | Train Acc: 84.55%\n",
      "  Val Loss: 0.0546 | Val Acc: 61.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 Train: 101it [00:11,  8.62it/s, acc=87.4, loss=0.537]                                                                                          \n",
      "Validation:  22%|██████████████████▉                                                                   | 22/100 [00:00<00:01, 50.39it/s, acc=64, loss=0.718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100]\n",
      "  Train Loss: 0.0287 | Train Acc: 87.41%\n",
      "  Val Loss: 0.0536 | Val Acc: 63.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100 Train: 101it [00:11,  8.62it/s, acc=89.2, loss=0.511]                                                                                          \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 51.13it/s, acc=63.4, loss=0.695]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100]\n",
      "  Train Loss: 0.0268 | Train Acc: 89.16%\n",
      "  Val Loss: 0.0527 | Val Acc: 63.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 Train: 101it [00:11,  8.63it/s, acc=91.3, loss=0.581]                                                                                          \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 51.59it/s, acc=64.5, loss=0.687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100]\n",
      "  Train Loss: 0.0242 | Train Acc: 91.28%\n",
      "  Val Loss: 0.0521 | Val Acc: 64.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100 Train: 101it [00:11,  8.64it/s, acc=92.3, loss=0.585]                                                                                          \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 51.32it/s, acc=63.1, loss=0.634]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100]\n",
      "  Train Loss: 0.0223 | Train Acc: 92.34%\n",
      "  Val Loss: 0.0527 | Val Acc: 63.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100 Train: 101it [00:11,  8.64it/s, acc=94, loss=0.426]                                                                                            \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 51.00it/s, acc=66.9, loss=0.608]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100]\n",
      "  Train Loss: 0.0201 | Train Acc: 93.96%\n",
      "  Val Loss: 0.0513 | Val Acc: 66.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 Train: 101it [00:11,  8.64it/s, acc=94.9, loss=0.526]                                                                                          \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 51.71it/s, acc=65.1, loss=0.608]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100]\n",
      "  Train Loss: 0.0187 | Train Acc: 94.89%\n",
      "  Val Loss: 0.0510 | Val Acc: 65.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 Train: 101it [00:11,  8.62it/s, acc=96.4, loss=0.535]                                                                                          \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 50.52it/s, acc=65.4, loss=0.573]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100]\n",
      "  Train Loss: 0.0166 | Train Acc: 96.39%\n",
      "  Val Loss: 0.0508 | Val Acc: 65.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100 Train: 101it [00:11,  8.61it/s, acc=97.3, loss=0.432]                                                                                          \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 50.45it/s, acc=65.1, loss=0.619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100]\n",
      "  Train Loss: 0.0147 | Train Acc: 97.26%\n",
      "  Val Loss: 0.0511 | Val Acc: 65.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100 Train: 101it [00:11,  8.65it/s, acc=97.1, loss=0.398]                                                                                          \n",
      "Validation:  22%|██████████████████▉                                                                   | 22/100 [00:00<00:01, 51.60it/s, acc=66, loss=0.558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100]\n",
      "  Train Loss: 0.0140 | Train Acc: 97.13%\n",
      "  Val Loss: 0.0510 | Val Acc: 65.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100 Train: 101it [00:11,  8.63it/s, acc=98.5, loss=0.408]                                                                                          \n",
      "Validation:  22%|██████████████████▉                                                                   | 22/100 [00:00<00:01, 51.56it/s, acc=66, loss=0.569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100]\n",
      "  Train Loss: 0.0124 | Train Acc: 98.50%\n",
      "  Val Loss: 0.0503 | Val Acc: 65.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100 Train: 101it [00:11,  8.64it/s, acc=98.6, loss=0.408]                                                                                          \n",
      "Validation:  22%|███████████████████▏                                                                   | 22/100 [00:00<00:01, 51.38it/s, acc=68, loss=0.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100]\n",
      "  Train Loss: 0.0115 | Train Acc: 98.63%\n",
      "  Val Loss: 0.0503 | Val Acc: 68.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100 Train: 101it [00:11,  8.63it/s, acc=99.2, loss=0.442]                                                                                          \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 52.06it/s, acc=64.5, loss=0.563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100]\n",
      "  Train Loss: 0.0106 | Train Acc: 99.19%\n",
      "  Val Loss: 0.0496 | Val Acc: 64.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100 Train: 101it [00:11,  8.65it/s, acc=98.9, loss=0.401]                                                                                          \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 50.25it/s, acc=66.9, loss=0.544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100]\n",
      "  Train Loss: 0.0100 | Train Acc: 98.94%\n",
      "  Val Loss: 0.0500 | Val Acc: 66.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100 Train: 101it [00:11,  8.64it/s, acc=99.4, loss=0.393]                                                                                          \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 51.03it/s, acc=65.4, loss=0.618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100]\n",
      "  Train Loss: 0.0091 | Train Acc: 99.38%\n",
      "  Val Loss: 0.0512 | Val Acc: 65.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100 Train: 101it [00:11,  8.62it/s, acc=99.2, loss=0.449]                                                                                          \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 50.59it/s, acc=66.6, loss=0.539]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100]\n",
      "  Train Loss: 0.0086 | Train Acc: 99.19%\n",
      "  Val Loss: 0.0511 | Val Acc: 66.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100 Train: 101it [00:11,  8.66it/s, acc=99.3, loss=0.489]                                                                                          \n",
      "Validation:  22%|██████████████████▋                                                                  | 22/100 [00:00<00:01, 51.57it/s, acc=66.9, loss=0.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100]\n",
      "  Train Loss: 0.0082 | Train Acc: 99.25%\n",
      "  Val Loss: 0.0499 | Val Acc: 66.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100 Train: 101it [00:11,  8.62it/s, acc=99.2, loss=0.282]                                                                                          \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 50.76it/s, acc=65.4, loss=0.506]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100]\n",
      "  Train Loss: 0.0078 | Train Acc: 99.19%\n",
      "  Val Loss: 0.0502 | Val Acc: 65.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100 Train: 101it [00:11,  8.67it/s, acc=99.4, loss=0.409]                                                                                          \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 51.70it/s, acc=68.3, loss=0.498]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100]\n",
      "  Train Loss: 0.0072 | Train Acc: 99.44%\n",
      "  Val Loss: 0.0499 | Val Acc: 68.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/100 Train: 101it [00:11,  8.65it/s, acc=99.5, loss=0.261]                                                                                          \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 51.45it/s, acc=66.9, loss=0.498]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/100]\n",
      "  Train Loss: 0.0067 | Train Acc: 99.50%\n",
      "  Val Loss: 0.0503 | Val Acc: 66.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/100 Train: 101it [00:11,  8.62it/s, acc=99.7, loss=0.318]                                                                                          \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 52.59it/s, acc=66.9, loss=0.522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100]\n",
      "  Train Loss: 0.0065 | Train Acc: 99.69%\n",
      "  Val Loss: 0.0503 | Val Acc: 66.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/100 Train: 101it [00:11,  8.65it/s, acc=99.6, loss=0.306]                                                                                          \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 50.28it/s, acc=67.2, loss=0.483]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100]\n",
      "  Train Loss: 0.0059 | Train Acc: 99.63%\n",
      "  Val Loss: 0.0501 | Val Acc: 67.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/100 Train: 101it [00:11,  8.65it/s, acc=99.6, loss=0.334]                                                                                          \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 51.29it/s, acc=67.4, loss=0.476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/100]\n",
      "  Train Loss: 0.0060 | Train Acc: 99.56%\n",
      "  Val Loss: 0.0503 | Val Acc: 67.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/100 Train: 101it [00:11,  8.63it/s, acc=99.9, loss=0.265]                                                                                          \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 50.98it/s, acc=67.2, loss=0.471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100]\n",
      "  Train Loss: 0.0057 | Train Acc: 99.88%\n",
      "  Val Loss: 0.0505 | Val Acc: 67.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/100 Train: 101it [00:11,  8.66it/s, acc=99.8, loss=0.263]                                                                                          \n",
      "Validation:  22%|██████████████████▋                                                                  | 22/100 [00:00<00:01, 51.91it/s, acc=67.7, loss=0.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/100]\n",
      "  Train Loss: 0.0053 | Train Acc: 99.75%\n",
      "  Val Loss: 0.0499 | Val Acc: 67.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/100 Train: 101it [00:11,  8.65it/s, acc=99.9, loss=0.197]                                                                                          \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 50.15it/s, acc=66.9, loss=0.462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/100]\n",
      "  Train Loss: 0.0052 | Train Acc: 99.88%\n",
      "  Val Loss: 0.0502 | Val Acc: 66.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/100 Train: 101it [00:11,  8.64it/s, acc=99.7, loss=0.248]                                                                                          \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 51.19it/s, acc=66.9, loss=0.461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/100]\n",
      "  Train Loss: 0.0049 | Train Acc: 99.69%\n",
      "  Val Loss: 0.0509 | Val Acc: 66.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/100 Train: 101it [00:11,  8.64it/s, acc=99.8, loss=0.239]                                                                                          \n",
      "Validation:  22%|██████████████████▍                                                                 | 22/100 [00:00<00:01, 51.49it/s, acc=67.2, loss=0.455]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 40. No improvement in validation loss for 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm  # Progress tracking\n",
    "\n",
    "# Compute class weights\n",
    "def compute_class_weights(y_train):\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "    return torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "y_train = np.concatenate([y.cpu().numpy() for _, y in load_embeddings_hdf5(\"train_embeddings.h5\", batch_size=32)])\n",
    "class_weights = compute_class_weights(y_train)\n",
    "\n",
    "# Define optimizer, scheduler, and scaler\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "NUM_GPUS = 1\n",
    "num_epochs = 100\n",
    "patience = 15  # Early stopping patience\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    # Training loop\n",
    "    train_loader = load_embeddings_hdf5(\"train_embeddings.h5\", batch_size=16 * NUM_GPUS)  # Adjust batch size\n",
    "    train_bar = tqdm(train_loader, total=len(y_train) // (16 * NUM_GPUS), desc=f\"Epoch {epoch+1}/{num_epochs} Train\")\n",
    "    for X_batch, y_batch in train_bar:\n",
    "        X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():  # Mixed precision\n",
    "            outputs = model(X_batch)\n",
    "            loss = F.cross_entropy(outputs, y_batch, weight=class_weights)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "        \n",
    "        train_bar.set_postfix(loss=loss.item(), acc=100 * correct / total)\n",
    "\n",
    "    train_accuracy = 100 * correct / total\n",
    "    avg_train_loss = train_loss / total\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "    val_loader = load_embeddings_hdf5(\"val_embeddings.h5\", batch_size=16 * NUM_GPUS)\n",
    "    val_bar = tqdm(val_loader, total=len(y_train) // (16 * NUM_GPUS), desc=\"Validation\")\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_bar:\n",
    "            X_val, y_val = X_val.to(DEVICE), y_val.to(DEVICE)\n",
    "            outputs = model(X_val)\n",
    "            loss = F.cross_entropy(outputs, y_val, weight=class_weights)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_correct += predicted.eq(y_val).sum().item()\n",
    "            val_total += y_val.size(0)\n",
    "\n",
    "            val_bar.set_postfix(loss=loss.item(), acc=100 * val_correct / val_total)\n",
    "    \n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    avg_val_loss = val_loss / val_total\n",
    "\n",
    "    # Check for early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_without_improvement = 0  # Reset counter\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")  # Save best model\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    # Stop if no improvement for 10 epochs\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}. No improvement in validation loss for {patience} epochs.\")\n",
    "        break\n",
    "\n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}%\")\n",
    "    print(f\"  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d024b1eb-13bb-41ee-8713-81f9db73ccc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6377\n",
      "Precision: 0.6368\n",
      "Recall: 0.6377\n",
      "F1 Score: 0.6294\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Inference\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in load_embeddings_hdf5(\"test_embeddings.h5\", batch_size=4):\n",
    "        outputs = model(X_batch)\n",
    "        predicted_labels = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        y_true.extend(y_batch.cpu().numpy())\n",
    "        y_pred.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average=\"weighted\")  # \"weighted\" accounts for class imbalance\n",
    "recall = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
