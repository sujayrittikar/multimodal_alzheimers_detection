{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd16bdcb-1310-450c-88dd-eb99b6beb334",
   "metadata": {},
   "source": [
    "# UniModal Images Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94b23760-3ca8-477d-86db-b9c530bea46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of 'flattened_images_2':\n",
      "  84489294400 bytes\n",
      "  80575.27 MB\n",
      "  78.69 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_folder_size(folder):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(folder):\n",
    "        for file in filenames:\n",
    "            file_path = os.path.join(dirpath, file)\n",
    "            \n",
    "            if not os.path.islink(file_path):\n",
    "                total_size += os.path.getsize(file_path)\n",
    "    return total_size\n",
    "\n",
    "folder = \"flattened_images_2\"\n",
    "size_bytes = get_folder_size(folder)\n",
    "size_mb = size_bytes / (1024 * 1024)\n",
    "size_gb = size_bytes / (1024 * 1024 * 1024)\n",
    "\n",
    "print(f\"Total size of '{folder}':\")\n",
    "print(f\"  {size_bytes} bytes\")\n",
    "print(f\"  {size_mb:.2f} MB\")\n",
    "print(f\"  {size_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfb2c5d5-90b2-4f64-b189-a4edb4da53ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 15 14:13:00 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.04             Driver Version: 570.124.04     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A4000               Off |   00000000:01:00.0 Off |                  Off |\n",
      "| 41%   41C    P8             16W /  140W |     431MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A4000               Off |   00000000:21:00.0 Off |                  Off |\n",
      "| 70%   87C    P0            125W /  140W |   11237MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A          143101      C   ...conda3/envs/my_env/bin/python        422MiB |\n",
      "|    1   N/A  N/A          155681      C   ...olia-h/miniconda3/bin/python3      11228MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c8b5986-81f8-40b6-b190-a2acfedc1317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 1.0638, Train Acc: 0.4623 | Val Loss: 1.0175, Val Acc: 0.4971\n",
      "Best model saved.\n",
      "Epoch 2/10 | Train Loss: 0.9520, Train Acc: 0.5439 | Val Loss: 0.8992, Val Acc: 0.6192\n",
      "Best model saved.\n",
      "Epoch 3/10 | Train Loss: 0.4299, Train Acc: 0.8243 | Val Loss: 1.0173, Val Acc: 0.5610\n",
      "Epoch 4/10 | Train Loss: 0.0396, Train Acc: 0.9938 | Val Loss: 1.1181, Val Acc: 0.6337\n",
      "Epoch 5/10 | Train Loss: 0.0086, Train Acc: 0.9988 | Val Loss: 1.1097, Val Acc: 0.6773\n",
      "Early stopping triggered.\n",
      "ðŸ”¥ Test Accuracy: 0.6522\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CN       0.61      0.59      0.60       106\n",
      "         MCI       0.69      0.76      0.72       167\n",
      "          AD       0.62      0.49      0.55        72\n",
      "\n",
      "    accuracy                           0.65       345\n",
      "   macro avg       0.64      0.61      0.62       345\n",
      "weighted avg       0.65      0.65      0.65       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, DeiTForImageClassification\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#  Check CUDA availability\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths to datasets and preprocessed images\n",
    "train_csv = \"train_data_3.csv\"\n",
    "val_csv = \"val_data_3.csv\"\n",
    "test_csv = \"test_data_3.csv\"\n",
    "image_dir = \"preprocessed_images_3\"  \n",
    "\n",
    "# Load Data Splits\n",
    "train_data = pd.read_csv(train_csv)\n",
    "val_data = pd.read_csv(val_csv)\n",
    "test_data = pd.read_csv(test_csv)\n",
    "\n",
    "#  Load DeiT-Base Model and Image Processor with classifier head\n",
    "checkpoint = \"facebook/deit-base-distilled-patch16-224\"  \n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "model = DeiTForImageClassification.from_pretrained(\n",
    "    checkpoint, num_labels=3, ignore_mismatched_sizes=True  \n",
    ").to(device)\n",
    "\n",
    "# Define label mapping \n",
    "label_map = {\"CN\": 0, \"MCI\": 1, \"AD\": 2}\n",
    "\n",
    "class MRIImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        \n",
    "        img_path = os.path.join(self.image_dir, f\"{row['Image Data ID']}.png\")\n",
    "        \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = label_map[row[\"Group\"]]\n",
    "        return image, label\n",
    "\n",
    "#  Define Transformations for DeiT-Base\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "])\n",
    "\n",
    "#  Create PyTorch Dataloaders\n",
    "batch_size = 8\n",
    "train_dataset = MRIImageDataset(train_data, image_dir, transform=transform)\n",
    "val_dataset = MRIImageDataset(val_data, image_dir, transform=transform)\n",
    "test_dataset = MRIImageDataset(test_data, image_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "#  Define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "#  Training Loop with Early Stopping\n",
    "epochs = 10\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_state = None\n",
    "patience = 3\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct_val += predicted.eq(labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    val_loss = running_val_loss / len(val_dataset)\n",
    "    val_acc = correct_val / total_val\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        wait = 0\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "if best_model_state:\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "#  Evaluate Model on Test Set and Compute Classification Report\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images).logits\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = correct / total\n",
    "print(f\"ðŸ”¥ Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Compute and print classification report \n",
    "report = classification_report(all_labels, all_preds, target_names=list(label_map.keys()))\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4bb056c-9471-4874-be26-02de0091ea10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 05:59:31.817615: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-19 05:59:31.856938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-19 05:59:32.474935: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 1.0688, Train Acc: 0.4579 | Val Loss: 1.0299, Val Acc: 0.4855\n",
      "Best model saved.\n",
      "Epoch 2/10 | Train Loss: 0.9702, Train Acc: 0.5445 | Val Loss: 0.9788, Val Acc: 0.5174\n",
      "Best model saved.\n",
      "Epoch 3/10 | Train Loss: 0.4673, Train Acc: 0.8187 | Val Loss: 0.8478, Val Acc: 0.6570\n",
      "Best model saved.\n",
      "Epoch 4/10 | Train Loss: 0.0413, Train Acc: 0.9963 | Val Loss: 0.9522, Val Acc: 0.6453\n",
      "Epoch 5/10 | Train Loss: 0.0045, Train Acc: 1.0000 | Val Loss: 1.0168, Val Acc: 0.6599\n",
      "Epoch 6/10 | Train Loss: 0.0017, Train Acc: 1.0000 | Val Loss: 1.0938, Val Acc: 0.6628\n",
      "Early stopping triggered.\n",
      "ðŸ”¥ Test Accuracy: 0.6435\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CN       0.62      0.52      0.56       106\n",
      "         MCI       0.66      0.80      0.72       167\n",
      "          AD       0.63      0.46      0.53        72\n",
      "\n",
      "    accuracy                           0.64       345\n",
      "   macro avg       0.64      0.59      0.61       345\n",
      "weighted avg       0.64      0.64      0.63       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, DeiTForImageClassification\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#  Check CUDA availability\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths to datasets and preprocessed PNG images\n",
    "train_csv = \"train_data_3.csv\"\n",
    "val_csv = \"val_data_3.csv\"\n",
    "test_csv = \"test_data_3.csv\"\n",
    "image_dir = \"preprocessed_images_3\"  \n",
    "\n",
    "# Load Data Splits\n",
    "train_data = pd.read_csv(train_csv)\n",
    "val_data = pd.read_csv(val_csv)\n",
    "test_data = pd.read_csv(test_csv)\n",
    "\n",
    "#  Load DeiT-Base Model and Image Processor with classifier head\n",
    "checkpoint = \"facebook/deit-base-distilled-patch16-224\"  \n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "model = DeiTForImageClassification.from_pretrained(\n",
    "    checkpoint, num_labels=3, ignore_mismatched_sizes=True  \n",
    ").to(device)\n",
    "\n",
    "# Define label mapping \n",
    "label_map = {\"CN\": 0, \"MCI\": 1, \"AD\": 2}\n",
    "\n",
    "class MRIImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        # Build the file path assuming PNG images\n",
    "        img_path = os.path.join(self.image_dir, f\"{row['Image Data ID']}.png\")\n",
    "        # Open the PNG image using PIL and convert it to RGB\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = label_map[row[\"Group\"]]\n",
    "        return image, label\n",
    "\n",
    "#  Define Transformations for DeiT-Base\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "])\n",
    "\n",
    "#  Create PyTorch Dataloaders\n",
    "batch_size = 8\n",
    "train_dataset = MRIImageDataset(train_data, image_dir, transform=transform)\n",
    "val_dataset = MRIImageDataset(val_data, image_dir, transform=transform)\n",
    "test_dataset = MRIImageDataset(test_data, image_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "#  Define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "#  Training Loop with Early Stopping\n",
    "epochs = 10\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_state = None\n",
    "patience = 3\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct_val += predicted.eq(labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    val_loss = running_val_loss / len(val_dataset)\n",
    "    val_acc = correct_val / total_val\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        wait = 0\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "if best_model_state:\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "#  Evaluate Model on Test Set and Compute Classification Report\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images).logits\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = correct / total\n",
    "print(f\"ðŸ”¥ Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Compute and print classification report using scikit-learn\n",
    "report = classification_report(all_labels, all_preds, target_names=list(label_map.keys()))\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6a2d50-c364-446c-b4d6-5fefae19bc05",
   "metadata": {},
   "source": [
    "## DEIT Model End to end training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fd897b4-9d30-49b7-b048-d9ec408594cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 05:45:45.859845: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-17 05:45:45.897912: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-17 05:45:46.446734: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 1.0628, Train Acc: 0.4654 | Val Loss: 1.0162, Val Acc: 0.5174\n",
      "Best model saved.\n",
      "Epoch 2/10 | Train Loss: 0.9562, Train Acc: 0.5340 | Val Loss: 0.9823, Val Acc: 0.5291\n",
      "Best model saved.\n",
      "Epoch 3/10 | Train Loss: 0.5160, Train Acc: 0.8062 | Val Loss: 0.8068, Val Acc: 0.6512\n",
      "Best model saved.\n",
      "Epoch 4/10 | Train Loss: 0.0677, Train Acc: 0.9931 | Val Loss: 0.9239, Val Acc: 0.6657\n",
      "Epoch 5/10 | Train Loss: 0.0105, Train Acc: 1.0000 | Val Loss: 0.9995, Val Acc: 0.6628\n",
      "Epoch 6/10 | Train Loss: 0.0044, Train Acc: 1.0000 | Val Loss: 1.0697, Val Acc: 0.6541\n",
      "Early stopping triggered.\n",
      "ðŸ”¥ Test Accuracy: 0.6348\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, DeiTForImageClassification\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths to datasets and preprocessed images\n",
    "train_csv = \"train_data_3.csv\"\n",
    "val_csv = \"val_data_3.csv\"\n",
    "test_csv = \"test_data_3.csv\"\n",
    "image_dir = \"preprocessed_images_3\"\n",
    "\n",
    "# Load Data Splits\n",
    "train_data = pd.read_csv(train_csv)\n",
    "val_data = pd.read_csv(val_csv)\n",
    "test_data = pd.read_csv(test_csv)\n",
    "\n",
    "# Load DeiT-Base Model and Image Processor with classifier head\n",
    "checkpoint = \"facebook/deit-base-distilled-patch16-224\"  \n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "model = DeiTForImageClassification.from_pretrained(\n",
    "    checkpoint, num_labels=3, ignore_mismatched_sizes=True  \n",
    ").to(device)\n",
    "\n",
    "# Define label mapping \n",
    "label_map = {\"CN\": 0, \"MCI\": 1, \"AD\": 2}\n",
    "\n",
    "class MRIImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        img_path = os.path.join(self.image_dir, f\"{row['Image Data ID']}.png\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = label_map[row[\"Group\"]]\n",
    "        return image, label\n",
    "\n",
    "# Define Transformations for DeiT-Base\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "])\n",
    "\n",
    "# Create PyTorch Dataloaders\n",
    "batch_size = 8\n",
    "train_dataset = MRIImageDataset(train_data, image_dir, transform=transform)\n",
    "val_dataset = MRIImageDataset(val_data, image_dir, transform=transform)\n",
    "test_dataset = MRIImageDataset(test_data, image_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "# Training Loop with Early Stopping\n",
    "epochs = 10\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_state = None\n",
    "patience = 3\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct_val += predicted.eq(labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    val_loss = running_val_loss / len(val_dataset)\n",
    "    val_acc = correct_val / total_val\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        wait = 0\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "if best_model_state:\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "# Evaluate Model on Test Set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images).logits\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_acc = correct / total\n",
    "print(f\"ðŸ”¥ Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438978d0",
   "metadata": {},
   "source": [
    "## Margin of Errors for deit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4643949-4a56-45cd-a05c-d50d0a5658f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 1.0613, Train Acc: 0.4698 | Val Loss: 1.0142, Val Acc: 0.4884\n",
      "Best model saved.\n",
      "Epoch 2/10 | Train Loss: 0.9641, Train Acc: 0.5240 | Val Loss: 0.9699, Val Acc: 0.5465\n",
      "Best model saved.\n",
      "Epoch 3/10 | Train Loss: 0.6085, Train Acc: 0.7639 | Val Loss: 0.9055, Val Acc: 0.6250\n",
      "Best model saved.\n",
      "Epoch 4/10 | Train Loss: 0.1029, Train Acc: 0.9863 | Val Loss: 0.9356, Val Acc: 0.6744\n",
      "Epoch 5/10 | Train Loss: 0.0150, Train Acc: 1.0000 | Val Loss: 1.0320, Val Acc: 0.6657\n",
      "Epoch 6/10 | Train Loss: 0.0048, Train Acc: 1.0000 | Val Loss: 1.1095, Val Acc: 0.6628\n",
      "Early stopping triggered.\n",
      "ðŸ”¥ Test Accuracy: 0.6551\n",
      "\n",
      "Direct Calculation:\n",
      "Standard Error: 0.0256\n",
      "Margin of Error (95% CI): 0.0502\n",
      "95% Confidence Interval for Accuracy: [0.6049, 0.7052]\n",
      "\n",
      "Bootstrapping:\n",
      "Bootstrapped Mean Accuracy: 0.6551\n",
      "Standard Error (Bootstrap): 0.0269\n",
      "Margin of Error (Bootstrap, 95% CI): 0.0527\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, DeiTForImageClassification\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "#  Check CUDA availability\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths to datasets and preprocessed images\n",
    "train_csv = \"train_data_3.csv\"\n",
    "val_csv = \"val_data_3.csv\"\n",
    "test_csv = \"test_data_3.csv\"\n",
    "image_dir = \"preprocessed_images_3\"\n",
    "\n",
    "# Load Data Splits\n",
    "train_data = pd.read_csv(train_csv)\n",
    "val_data = pd.read_csv(val_csv)\n",
    "test_data = pd.read_csv(test_csv)\n",
    "\n",
    "#  Load DeiT-Base Model and Image Processor with classifier head\n",
    "checkpoint = \"facebook/deit-base-distilled-patch16-224\"  # Using DeiT base distilled model\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "model = DeiTForImageClassification.from_pretrained(\n",
    "    checkpoint, num_labels=3, ignore_mismatched_sizes=True  \n",
    ").to(device)\n",
    "\n",
    "# Define label mapping (example: \"CN\": Cognitively Normal, \"MCI\": Mild Cognitive Impairment, \"AD\": Alzheimerâ€™s Disease)\n",
    "label_map = {\"CN\": 0, \"MCI\": 1, \"AD\": 2}\n",
    "\n",
    "class MRIImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        img_path = os.path.join(self.image_dir, f\"{row['Image Data ID']}.png\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = label_map[row[\"Group\"]]\n",
    "        return image, label\n",
    "\n",
    "#  Define Transformations for DeiT-Base\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "])\n",
    "\n",
    "#  Create PyTorch Dataloaders\n",
    "batch_size = 8\n",
    "train_dataset = MRIImageDataset(train_data, image_dir, transform=transform)\n",
    "val_dataset = MRIImageDataset(val_data, image_dir, transform=transform)\n",
    "test_dataset = MRIImageDataset(test_data, image_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "#  Define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "#  Training Loop with Early Stopping\n",
    "epochs = 10\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_state = None\n",
    "patience = 3\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct_val += predicted.eq(labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    val_loss = running_val_loss / len(val_dataset)\n",
    "    val_acc = correct_val / total_val\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        wait = 0\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "if best_model_state:\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "#  Evaluate Model on Test Set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images).logits\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = correct / total\n",
    "print(f\"ðŸ”¥ Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Approach 1: Direct Calculation using the Binomial Formula.\n",
    "n_test = len(test_dataset)\n",
    "se_direct = math.sqrt(test_acc * (1 - test_acc) / n_test)\n",
    "moe_direct = 1.96 * se_direct\n",
    "print(\"\\nDirect Calculation:\")\n",
    "print(\"Standard Error: {:.4f}\".format(se_direct))\n",
    "print(\"Margin of Error (95% CI): {:.4f}\".format(moe_direct))\n",
    "print(\"95% Confidence Interval for Accuracy: [{:.4f}, {:.4f}]\".format(test_acc - moe_direct, test_acc + moe_direct))\n",
    "\n",
    "# Approach 2: Bootstrapping.\n",
    "n_bootstrap = 1000\n",
    "boot_accs = []\n",
    "all_preds = np.array(all_preds)\n",
    "all_targets = np.array(all_targets)\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_test), n_test, replace=True)\n",
    "    acc_bs = accuracy_score(all_targets[indices], all_preds[indices])\n",
    "    boot_accs.append(acc_bs)\n",
    "boot_accs = np.array(boot_accs)\n",
    "se_bootstrap = np.std(boot_accs)\n",
    "moe_bootstrap = 1.96 * se_bootstrap\n",
    "print(\"\\nBootstrapping:\")\n",
    "print(\"Bootstrapped Mean Accuracy: {:.4f}\".format(np.mean(boot_accs)))\n",
    "print(\"Standard Error (Bootstrap): {:.4f}\".format(se_bootstrap))\n",
    "print(\"Margin of Error (Bootstrap, 95% CI): {:.4f}\".format(moe_bootstrap))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "674b5b3b-bee3-4ef0-bbea-4de9d4565a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.6492\n",
      "Bootstrapped Weighted F1 Mean: 0.6498\n",
      "Standard Error (Bootstrap) for Weighted F1: 0.0269\n",
      "Margin of Error (Bootstrap, 95% CI) for Weighted F1: 0.0528\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Compute the weighted F1 score on the test set.\n",
    "weighted_f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "print(\"Weighted F1 Score: {:.4f}\".format(weighted_f1))\n",
    "\n",
    "# Bootstrapping to estimate the margin of error for the weighted F1 score.\n",
    "n_bootstrap = 1000\n",
    "boot_f1s = []\n",
    "n_samples = len(all_targets)\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_samples), n_samples, replace=True)\n",
    "    f1_bs = f1_score(all_targets[indices], all_preds[indices], average='weighted')\n",
    "    boot_f1s.append(f1_bs)\n",
    "boot_f1s = np.array(boot_f1s)\n",
    "\n",
    "std_f1 = np.std(boot_f1s)\n",
    "moe_f1 = 1.96 * std_f1\n",
    "\n",
    "print(\"Bootstrapped Weighted F1 Mean: {:.4f}\".format(np.mean(boot_f1s)))\n",
    "print(\"Standard Error (Bootstrap) for Weighted F1: {:.4f}\".format(std_f1))\n",
    "print(\"Margin of Error (Bootstrap, 95% CI) for Weighted F1: {:.4f}\".format(moe_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aac7ec4b-c2e7-466a-b317-1884c7c75c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 1.0635, Train Acc: 0.4741 | Val Loss: 1.0187, Val Acc: 0.4855\n",
      "Best model saved.\n",
      "Epoch 2/10 | Train Loss: 0.9260, Train Acc: 0.5657 | Val Loss: 0.9279, Val Acc: 0.5233\n",
      "Best model saved.\n",
      "Epoch 3/10 | Train Loss: 0.3414, Train Acc: 0.8729 | Val Loss: 0.9068, Val Acc: 0.5930\n",
      "Best model saved.\n",
      "Epoch 4/10 | Train Loss: 0.0400, Train Acc: 0.9907 | Val Loss: 0.9134, Val Acc: 0.6541\n",
      "Epoch 5/10 | Train Loss: 0.0072, Train Acc: 0.9994 | Val Loss: 0.9495, Val Acc: 0.7122\n",
      "Epoch 6/10 | Train Loss: 0.0014, Train Acc: 1.0000 | Val Loss: 1.0481, Val Acc: 0.6977\n",
      "Early stopping triggered.\n",
      "ðŸ”¥ Test Accuracy: 0.6638\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CN       0.70      0.57      0.62       106\n",
      "         MCI       0.66      0.83      0.73       167\n",
      "          AD       0.62      0.43      0.51        72\n",
      "\n",
      "    accuracy                           0.66       345\n",
      "   macro avg       0.66      0.61      0.62       345\n",
      "weighted avg       0.66      0.66      0.65       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, DeiTForImageClassification\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths to datasets and preprocessed PNG images\n",
    "train_csv = \"train_data_3.csv\"\n",
    "val_csv = \"val_data_3.csv\"\n",
    "test_csv = \"test_data_3.csv\"\n",
    "image_dir = \"preprocessed_images_3\"  \n",
    "\n",
    "# Load Data Splits\n",
    "train_data = pd.read_csv(train_csv)\n",
    "val_data = pd.read_csv(val_csv)\n",
    "test_data = pd.read_csv(test_csv)\n",
    "\n",
    "# Load DeiT-Base Model and Image Processor with classifier head\n",
    "checkpoint = \"facebook/deit-base-distilled-patch16-224\"  \n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "model = DeiTForImageClassification.from_pretrained(\n",
    "    checkpoint, num_labels=3, ignore_mismatched_sizes=True  \n",
    ").to(device)\n",
    "\n",
    "# Define label mapping \n",
    "label_map = {\"CN\": 0, \"MCI\": 1, \"AD\": 2}\n",
    "\n",
    "class MRIImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        # Build the file path assuming PNG images\n",
    "        img_path = os.path.join(self.image_dir, f\"{row['Image Data ID']}.png\")\n",
    "        # Open the PNG image using PIL and convert it to RGB\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = label_map[row[\"Group\"]]\n",
    "        return image, label\n",
    "\n",
    "# Define Transformations for DeiT-Base\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "])\n",
    "\n",
    "# Create PyTorch Dataloaders\n",
    "batch_size = 8\n",
    "train_dataset = MRIImageDataset(train_data, image_dir, transform=transform)\n",
    "val_dataset = MRIImageDataset(val_data, image_dir, transform=transform)\n",
    "test_dataset = MRIImageDataset(test_data, image_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "#  Define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "#  Training Loop with Early Stopping\n",
    "epochs = 10\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_state = None\n",
    "patience = 3\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct_val += predicted.eq(labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    val_loss = running_val_loss / len(val_dataset)\n",
    "    val_acc = correct_val / total_val\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        wait = 0\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "if best_model_state:\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "# Evaluate Model on Test Set and Compute Classification Report\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images).logits\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = correct / total\n",
    "print(f\"ðŸ”¥ Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Compute and print classification report using scikit-learn\n",
    "report = classification_report(all_labels, all_preds, target_names=list(label_map.keys()))\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b6ca9c-2b6e-4574-bb1d-59d78981777a",
   "metadata": {},
   "source": [
    "## Using MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e774a54-1f2f-44cf-a42f-b9ea14752cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape: (1605, 768)\n",
      "Val features shape: (344, 768)\n",
      "Test features shape: (345, 768)\n",
      "Epoch 1/200: Train Loss = 1.0683, Val Loss = 1.0389, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 2/200: Train Loss = 1.0490, Val Loss = 1.0353, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 3/200: Train Loss = 1.0480, Val Loss = 1.0324, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 4/200: Train Loss = 1.0396, Val Loss = 1.0306, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 5/200: Train Loss = 1.0326, Val Loss = 1.0289, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 6/200: Train Loss = 1.0371, Val Loss = 1.0265, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 7/200: Train Loss = 1.0277, Val Loss = 1.0248, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 8/200: Train Loss = 1.0270, Val Loss = 1.0230, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 9/200: Train Loss = 1.0305, Val Loss = 1.0210, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 10/200: Train Loss = 1.0224, Val Loss = 1.0199, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 11/200: Train Loss = 1.0237, Val Loss = 1.0176, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 12/200: Train Loss = 1.0174, Val Loss = 1.0162, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 13/200: Train Loss = 1.0205, Val Loss = 1.0139, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 14/200: Train Loss = 1.0142, Val Loss = 1.0125, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 15/200: Train Loss = 1.0134, Val Loss = 1.0103, Val Acc = 0.4913\n",
      "  -> New best model saved.\n",
      "Epoch 16/200: Train Loss = 1.0059, Val Loss = 1.0081, Val Acc = 0.4942\n",
      "  -> New best model saved.\n",
      "Epoch 17/200: Train Loss = 1.0050, Val Loss = 1.0081, Val Acc = 0.4913\n",
      "  -> New best model saved.\n",
      "Epoch 18/200: Train Loss = 1.0022, Val Loss = 1.0048, Val Acc = 0.4913\n",
      "  -> New best model saved.\n",
      "Epoch 19/200: Train Loss = 0.9976, Val Loss = 1.0057, Val Acc = 0.4884\n",
      "Epoch 20/200: Train Loss = 0.9949, Val Loss = 1.0010, Val Acc = 0.5174\n",
      "  -> New best model saved.\n",
      "Epoch 21/200: Train Loss = 0.9899, Val Loss = 1.0046, Val Acc = 0.4884\n",
      "Epoch 22/200: Train Loss = 0.9879, Val Loss = 0.9992, Val Acc = 0.5000\n",
      "  -> New best model saved.\n",
      "Epoch 23/200: Train Loss = 0.9913, Val Loss = 0.9976, Val Acc = 0.5174\n",
      "  -> New best model saved.\n",
      "Epoch 24/200: Train Loss = 0.9835, Val Loss = 0.9955, Val Acc = 0.5349\n",
      "  -> New best model saved.\n",
      "Epoch 25/200: Train Loss = 0.9803, Val Loss = 0.9930, Val Acc = 0.5291\n",
      "  -> New best model saved.\n",
      "Epoch 26/200: Train Loss = 0.9700, Val Loss = 0.9969, Val Acc = 0.5058\n",
      "Epoch 27/200: Train Loss = 0.9696, Val Loss = 0.9883, Val Acc = 0.5378\n",
      "  -> New best model saved.\n",
      "Epoch 28/200: Train Loss = 0.9749, Val Loss = 0.9860, Val Acc = 0.5349\n",
      "  -> New best model saved.\n",
      "Epoch 29/200: Train Loss = 0.9589, Val Loss = 0.9845, Val Acc = 0.5291\n",
      "  -> New best model saved.\n",
      "Epoch 30/200: Train Loss = 0.9592, Val Loss = 0.9831, Val Acc = 0.5291\n",
      "  -> New best model saved.\n",
      "Epoch 31/200: Train Loss = 0.9577, Val Loss = 0.9805, Val Acc = 0.5320\n",
      "  -> New best model saved.\n",
      "Epoch 32/200: Train Loss = 0.9514, Val Loss = 0.9798, Val Acc = 0.5320\n",
      "  -> New best model saved.\n",
      "Epoch 33/200: Train Loss = 0.9535, Val Loss = 0.9772, Val Acc = 0.5378\n",
      "  -> New best model saved.\n",
      "Epoch 34/200: Train Loss = 0.9406, Val Loss = 0.9782, Val Acc = 0.5320\n",
      "Epoch 35/200: Train Loss = 0.9349, Val Loss = 0.9739, Val Acc = 0.5465\n",
      "  -> New best model saved.\n",
      "Epoch 36/200: Train Loss = 0.9283, Val Loss = 0.9749, Val Acc = 0.5494\n",
      "Epoch 37/200: Train Loss = 0.9324, Val Loss = 0.9717, Val Acc = 0.5552\n",
      "  -> New best model saved.\n",
      "Epoch 38/200: Train Loss = 0.9316, Val Loss = 0.9696, Val Acc = 0.5552\n",
      "  -> New best model saved.\n",
      "Epoch 39/200: Train Loss = 0.9209, Val Loss = 0.9684, Val Acc = 0.5552\n",
      "  -> New best model saved.\n",
      "Epoch 40/200: Train Loss = 0.9166, Val Loss = 0.9670, Val Acc = 0.5610\n",
      "  -> New best model saved.\n",
      "Epoch 41/200: Train Loss = 0.9132, Val Loss = 0.9661, Val Acc = 0.5465\n",
      "  -> New best model saved.\n",
      "Epoch 42/200: Train Loss = 0.9061, Val Loss = 0.9657, Val Acc = 0.5581\n",
      "  -> New best model saved.\n",
      "Epoch 43/200: Train Loss = 0.9011, Val Loss = 0.9663, Val Acc = 0.5669\n",
      "Epoch 44/200: Train Loss = 0.8980, Val Loss = 0.9583, Val Acc = 0.5581\n",
      "  -> New best model saved.\n",
      "Epoch 45/200: Train Loss = 0.8946, Val Loss = 0.9587, Val Acc = 0.5552\n",
      "Epoch 46/200: Train Loss = 0.8891, Val Loss = 0.9564, Val Acc = 0.5581\n",
      "  -> New best model saved.\n",
      "Epoch 47/200: Train Loss = 0.8865, Val Loss = 0.9542, Val Acc = 0.5669\n",
      "  -> New best model saved.\n",
      "Epoch 48/200: Train Loss = 0.8822, Val Loss = 0.9552, Val Acc = 0.5610\n",
      "Epoch 49/200: Train Loss = 0.8739, Val Loss = 0.9509, Val Acc = 0.5669\n",
      "  -> New best model saved.\n",
      "Epoch 50/200: Train Loss = 0.8669, Val Loss = 0.9520, Val Acc = 0.5610\n",
      "Epoch 51/200: Train Loss = 0.8564, Val Loss = 0.9513, Val Acc = 0.5465\n",
      "Epoch 52/200: Train Loss = 0.8575, Val Loss = 0.9475, Val Acc = 0.5698\n",
      "  -> New best model saved.\n",
      "Epoch 53/200: Train Loss = 0.8445, Val Loss = 0.9468, Val Acc = 0.5669\n",
      "  -> New best model saved.\n",
      "Epoch 54/200: Train Loss = 0.8402, Val Loss = 0.9492, Val Acc = 0.5698\n",
      "Epoch 55/200: Train Loss = 0.8433, Val Loss = 0.9455, Val Acc = 0.5669\n",
      "  -> New best model saved.\n",
      "Epoch 56/200: Train Loss = 0.8263, Val Loss = 0.9467, Val Acc = 0.5669\n",
      "Epoch 57/200: Train Loss = 0.8297, Val Loss = 0.9469, Val Acc = 0.5610\n",
      "Epoch 58/200: Train Loss = 0.8273, Val Loss = 0.9400, Val Acc = 0.5640\n",
      "  -> New best model saved.\n",
      "Epoch 59/200: Train Loss = 0.8195, Val Loss = 0.9390, Val Acc = 0.5610\n",
      "  -> New best model saved.\n",
      "Epoch 60/200: Train Loss = 0.8121, Val Loss = 0.9427, Val Acc = 0.5640\n",
      "Epoch 61/200: Train Loss = 0.8083, Val Loss = 0.9374, Val Acc = 0.5756\n",
      "  -> New best model saved.\n",
      "Epoch 62/200: Train Loss = 0.7967, Val Loss = 0.9370, Val Acc = 0.5756\n",
      "  -> New best model saved.\n",
      "Epoch 63/200: Train Loss = 0.7917, Val Loss = 0.9303, Val Acc = 0.5785\n",
      "  -> New best model saved.\n",
      "Epoch 64/200: Train Loss = 0.7862, Val Loss = 0.9309, Val Acc = 0.5756\n",
      "Epoch 65/200: Train Loss = 0.7885, Val Loss = 0.9266, Val Acc = 0.5843\n",
      "  -> New best model saved.\n",
      "Epoch 66/200: Train Loss = 0.7737, Val Loss = 0.9258, Val Acc = 0.5872\n",
      "  -> New best model saved.\n",
      "Epoch 67/200: Train Loss = 0.7599, Val Loss = 0.9415, Val Acc = 0.5698\n",
      "Epoch 68/200: Train Loss = 0.7576, Val Loss = 0.9231, Val Acc = 0.5843\n",
      "  -> New best model saved.\n",
      "Epoch 69/200: Train Loss = 0.7500, Val Loss = 0.9235, Val Acc = 0.5814\n",
      "Epoch 70/200: Train Loss = 0.7521, Val Loss = 0.9298, Val Acc = 0.5814\n",
      "Epoch 71/200: Train Loss = 0.7429, Val Loss = 0.9185, Val Acc = 0.5959\n",
      "  -> New best model saved.\n",
      "Epoch 72/200: Train Loss = 0.7301, Val Loss = 0.9150, Val Acc = 0.5988\n",
      "  -> New best model saved.\n",
      "Epoch 73/200: Train Loss = 0.7255, Val Loss = 0.9133, Val Acc = 0.6076\n",
      "  -> New best model saved.\n",
      "Epoch 74/200: Train Loss = 0.7233, Val Loss = 0.9134, Val Acc = 0.6105\n",
      "Epoch 75/200: Train Loss = 0.7128, Val Loss = 0.9118, Val Acc = 0.6047\n",
      "  -> New best model saved.\n",
      "Epoch 76/200: Train Loss = 0.7055, Val Loss = 0.9189, Val Acc = 0.6017\n",
      "Epoch 77/200: Train Loss = 0.7066, Val Loss = 0.9147, Val Acc = 0.5988\n",
      "Epoch 78/200: Train Loss = 0.6934, Val Loss = 0.9084, Val Acc = 0.5959\n",
      "  -> New best model saved.\n",
      "Epoch 79/200: Train Loss = 0.6820, Val Loss = 0.9062, Val Acc = 0.5959\n",
      "  -> New best model saved.\n",
      "Epoch 80/200: Train Loss = 0.6859, Val Loss = 0.9019, Val Acc = 0.6105\n",
      "  -> New best model saved.\n",
      "Epoch 81/200: Train Loss = 0.6691, Val Loss = 0.9105, Val Acc = 0.6017\n",
      "Epoch 82/200: Train Loss = 0.6637, Val Loss = 0.9033, Val Acc = 0.6047\n",
      "Epoch 83/200: Train Loss = 0.6663, Val Loss = 0.8982, Val Acc = 0.6076\n",
      "  -> New best model saved.\n",
      "Epoch 84/200: Train Loss = 0.6586, Val Loss = 0.8964, Val Acc = 0.6192\n",
      "  -> New best model saved.\n",
      "Epoch 85/200: Train Loss = 0.6408, Val Loss = 0.8956, Val Acc = 0.6221\n",
      "  -> New best model saved.\n",
      "Epoch 86/200: Train Loss = 0.6391, Val Loss = 0.9010, Val Acc = 0.6134\n",
      "Epoch 87/200: Train Loss = 0.6203, Val Loss = 0.8953, Val Acc = 0.6134\n",
      "  -> New best model saved.\n",
      "Epoch 88/200: Train Loss = 0.6169, Val Loss = 0.8944, Val Acc = 0.6134\n",
      "  -> New best model saved.\n",
      "Epoch 89/200: Train Loss = 0.6139, Val Loss = 0.8916, Val Acc = 0.6279\n",
      "  -> New best model saved.\n",
      "Epoch 90/200: Train Loss = 0.6098, Val Loss = 0.9009, Val Acc = 0.6105\n",
      "Epoch 91/200: Train Loss = 0.6077, Val Loss = 0.8932, Val Acc = 0.6105\n",
      "Epoch 92/200: Train Loss = 0.5996, Val Loss = 0.8860, Val Acc = 0.6250\n",
      "  -> New best model saved.\n",
      "Epoch 93/200: Train Loss = 0.5882, Val Loss = 0.8940, Val Acc = 0.6279\n",
      "Epoch 94/200: Train Loss = 0.5878, Val Loss = 0.8802, Val Acc = 0.6308\n",
      "  -> New best model saved.\n",
      "Epoch 95/200: Train Loss = 0.5811, Val Loss = 0.8817, Val Acc = 0.6250\n",
      "Epoch 96/200: Train Loss = 0.5644, Val Loss = 0.8861, Val Acc = 0.6250\n",
      "Epoch 97/200: Train Loss = 0.5640, Val Loss = 0.8861, Val Acc = 0.6221\n",
      "Epoch 98/200: Train Loss = 0.5554, Val Loss = 0.8921, Val Acc = 0.6308\n",
      "Epoch 99/200: Train Loss = 0.5377, Val Loss = 0.8801, Val Acc = 0.6250\n",
      "  -> New best model saved.\n",
      "Epoch 100/200: Train Loss = 0.5409, Val Loss = 0.8775, Val Acc = 0.6337\n",
      "  -> New best model saved.\n",
      "Epoch 101/200: Train Loss = 0.5351, Val Loss = 0.8732, Val Acc = 0.6366\n",
      "  -> New best model saved.\n",
      "Epoch 102/200: Train Loss = 0.5234, Val Loss = 0.8774, Val Acc = 0.6250\n",
      "Epoch 103/200: Train Loss = 0.5195, Val Loss = 0.8690, Val Acc = 0.6308\n",
      "  -> New best model saved.\n",
      "Epoch 104/200: Train Loss = 0.5169, Val Loss = 0.8704, Val Acc = 0.6453\n",
      "Epoch 105/200: Train Loss = 0.5094, Val Loss = 0.8887, Val Acc = 0.6424\n",
      "Epoch 106/200: Train Loss = 0.4932, Val Loss = 0.8635, Val Acc = 0.6424\n",
      "  -> New best model saved.\n",
      "Epoch 107/200: Train Loss = 0.4913, Val Loss = 0.8668, Val Acc = 0.6395\n",
      "Epoch 108/200: Train Loss = 0.4861, Val Loss = 0.8771, Val Acc = 0.6512\n",
      "Epoch 109/200: Train Loss = 0.4847, Val Loss = 0.8621, Val Acc = 0.6512\n",
      "  -> New best model saved.\n",
      "Epoch 110/200: Train Loss = 0.4900, Val Loss = 0.8680, Val Acc = 0.6424\n",
      "Epoch 111/200: Train Loss = 0.4612, Val Loss = 0.8719, Val Acc = 0.6424\n",
      "Epoch 112/200: Train Loss = 0.4570, Val Loss = 0.8673, Val Acc = 0.6512\n",
      "Epoch 113/200: Train Loss = 0.4460, Val Loss = 0.8625, Val Acc = 0.6512\n",
      "Epoch 114/200: Train Loss = 0.4408, Val Loss = 0.8570, Val Acc = 0.6424\n",
      "  -> New best model saved.\n",
      "Epoch 115/200: Train Loss = 0.4411, Val Loss = 0.8614, Val Acc = 0.6541\n",
      "Epoch 116/200: Train Loss = 0.4308, Val Loss = 0.8591, Val Acc = 0.6715\n",
      "Epoch 117/200: Train Loss = 0.4192, Val Loss = 0.8711, Val Acc = 0.6483\n",
      "Epoch 118/200: Train Loss = 0.4168, Val Loss = 0.8719, Val Acc = 0.6657\n",
      "Epoch 119/200: Train Loss = 0.4199, Val Loss = 0.8620, Val Acc = 0.6570\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 0.6144927536231884\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.59      0.46      0.52        72\n",
      "          CN       0.58      0.62      0.60       106\n",
      "         MCI       0.64      0.68      0.66       167\n",
      "\n",
      "    accuracy                           0.61       345\n",
      "   macro avg       0.61      0.59      0.59       345\n",
      "weighted avg       0.61      0.61      0.61       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Load DeiT features.\n",
    "train_features = np.load(\"Deit_train_features_1.npy\")  \n",
    "val_features   = np.load(\"Deit_val_features_1.npy\")      \n",
    "test_features  = np.load(\"Deit_test_features_1.npy\")     \n",
    "\n",
    "print(\"Train features shape:\", train_features.shape)\n",
    "print(\"Val features shape:\", val_features.shape)\n",
    "print(\"Test features shape:\", test_features.shape)\n",
    "\n",
    "\n",
    "# Read the CSV files \n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "\n",
    "label_col = \"Group\"\n",
    "\n",
    "# Encode labels using LabelEncoder.\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "\n",
    "# Convert features and labels to tensors.\n",
    "train_features = torch.tensor(train_features, dtype=torch.float32)\n",
    "val_features   = torch.tensor(val_features, dtype=torch.float32)\n",
    "test_features  = torch.tensor(test_features, dtype=torch.float32)\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels   = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_labels  = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets.\n",
    "train_dataset = TensorDataset(train_features, train_labels)\n",
    "val_dataset   = TensorDataset(val_features, val_labels)\n",
    "test_dataset  = TensorDataset(test_features, test_labels)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),  \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),        \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes) \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Example usage:\n",
    "input_dim = train_features.shape[1]  \n",
    "model_mlp = MLPClassifier(input_dim, num_classes)\n",
    "model_mlp.to(device)\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_mlp.parameters(), lr=2e-5)\n",
    "num_epochs = 200\n",
    "patience = 5  \n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_mlp.train()\n",
    "    train_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_mlp(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Evaluate on the validation set.\n",
    "    model_mlp.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model_mlp(x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    val_acc = accuracy_score(all_targets, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping check.\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model_mlp.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model state.\n",
    "model_mlp.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model_mlp.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model_mlp(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585529c9-7c4d-467a-b492-13b482db26c4",
   "metadata": {},
   "source": [
    "# Uni Modal Tabular Data performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4969a38e-8b4c-408c-a036-526503a0f2a3",
   "metadata": {},
   "source": [
    "## Ft Transformer end to end training and evalaution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfba5e74-f629-48f5-8f6d-8619b84edfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  \n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = 42  # choose your seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e66305b4-d952-47ac-aca0-b30d0147cfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.0557 | Val Loss: 1.0274\n",
      "Epoch 2: Train Loss: 1.0233 | Val Loss: 0.9525\n",
      "Epoch 3: Train Loss: 0.7375 | Val Loss: 0.4822\n",
      "Epoch 4: Train Loss: 0.4742 | Val Loss: 0.3990\n",
      "Epoch 5: Train Loss: 0.4257 | Val Loss: 0.3785\n",
      "Epoch 6: Train Loss: 0.3891 | Val Loss: 0.3898\n",
      "Epoch 7: Train Loss: 0.3681 | Val Loss: 0.3462\n",
      "Epoch 8: Train Loss: 0.3403 | Val Loss: 0.3471\n",
      "Epoch 9: Train Loss: 0.3667 | Val Loss: 0.3960\n",
      "Epoch 10: Train Loss: 0.3602 | Val Loss: 0.3369\n",
      "Epoch 11: Train Loss: 0.3360 | Val Loss: 0.3430\n",
      "Epoch 12: Train Loss: 0.3377 | Val Loss: 0.3402\n",
      "Epoch 13: Train Loss: 0.3413 | Val Loss: 0.3201\n",
      "Epoch 14: Train Loss: 0.3366 | Val Loss: 0.3385\n",
      "Epoch 15: Train Loss: 0.3308 | Val Loss: 0.3481\n",
      "Epoch 16: Train Loss: 0.3233 | Val Loss: 0.3290\n",
      "Epoch 17: Train Loss: 0.3308 | Val Loss: 0.3324\n",
      "Epoch 18: Train Loss: 0.3194 | Val Loss: 0.3325\n",
      "Early stopping triggered.\n",
      "Trained model saved to best_ft_transformer_classification.pt\n",
      "Test Accuracy: 0.9043478260869565\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.83      0.81      0.82        72\n",
      "          CN       0.98      0.95      0.97       106\n",
      "         MCI       0.89      0.92      0.90       167\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.90      0.89      0.90       345\n",
      "weighted avg       0.90      0.90      0.90       345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_789196/921975057.py:191: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(save_model_path))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from rtdl_revisiting_models import FTTransformer\n",
    "\n",
    "\n",
    "# Load CSV files for train, validation, and test splits.\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "# Define feature columns and target label.\n",
    "numerical_features = [\"Age\", \"CDGLOBAL\", \"CDRSB\", \"MMSCORE\", \"HMSCORE\", \"NPISCORE\", \"GDTOTAL\"]\n",
    "categorical_features = [\"GENOTYPE\"]\n",
    "label = \"Group\"\n",
    "\n",
    "# Subset dataframes to desired columns.\n",
    "cols = numerical_features + categorical_features + [label]\n",
    "train_data = train_data[cols]\n",
    "val_data   = val_data[cols]\n",
    "test_data  = test_data[cols]\n",
    "\n",
    "# Handle missingness for numerical features.\n",
    "cols_with_missing = [\"CDRSB\", \"MMSCORE\", \"HMSCORE\", \"NPISCORE\", \"GDTOTAL\"]\n",
    "for col in cols_with_missing:\n",
    "    for df in [train_data, val_data, test_data]:\n",
    "        df[col + \"_is_missing\"] = df[col].isnull().astype(int)\n",
    "        df[col] = df[col].fillna(-999)\n",
    "\n",
    "# Extend continuous features to include missing indicators.\n",
    "numerical_features_extended = numerical_features + [col + \"_is_missing\" for col in cols_with_missing]\n",
    "\n",
    "# Encode categorical features using LabelEncoder.\n",
    "cat_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    train_data[col] = le.fit_transform(train_data[col].astype(str))\n",
    "    val_data[col]   = le.transform(val_data[col].astype(str))\n",
    "    test_data[col]  = le.transform(test_data[col].astype(str))\n",
    "    cat_encoders[col] = le\n",
    "\n",
    "# Encode the target.\n",
    "label_encoder = LabelEncoder()\n",
    "train_data[label] = label_encoder.fit_transform(train_data[label])\n",
    "val_data[label]   = label_encoder.transform(val_data[label])\n",
    "test_data[label]  = label_encoder.transform(test_data[label])\n",
    "num_classes = len(label_encoder.classes_)  \n",
    "\n",
    "\n",
    "# Continuous features.\n",
    "X_train_cont = train_data[numerical_features_extended].values.astype(np.float32)\n",
    "X_val_cont   = val_data[numerical_features_extended].values.astype(np.float32)\n",
    "X_test_cont  = test_data[numerical_features_extended].values.astype(np.float32)\n",
    "\n",
    "# Categorical features.\n",
    "X_train_cat = train_data[categorical_features].values.astype(np.int64)\n",
    "X_val_cat   = val_data[categorical_features].values.astype(np.int64)\n",
    "X_test_cat  = test_data[categorical_features].values.astype(np.int64)\n",
    "\n",
    "# Labels.\n",
    "y_train = train_data[label].values.astype(np.int64)\n",
    "y_val   = val_data[label].values.astype(np.int64)\n",
    "y_test  = test_data[label].values.astype(np.int64)\n",
    "\n",
    "# Create a simple PyTorch Dataset.\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, cont, cat, labels):\n",
    "        self.cont = cont\n",
    "        self.cat = cat\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"cont\": torch.tensor(self.cont[idx], dtype=torch.float32),\n",
    "            \"cat\": torch.tensor(self.cat[idx], dtype=torch.long),\n",
    "            \"target\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = TabularDataset(X_train_cont, X_train_cat, y_train)\n",
    "val_dataset   = TabularDataset(X_val_cont, X_val_cat, y_val)\n",
    "test_dataset  = TabularDataset(X_test_cont, X_test_cat, y_test)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Get the number of continuous features.\n",
    "n_cont_features = X_train_cont.shape[1]\n",
    "cat_cardinalities = [int(train_data[col].nunique()) for col in categorical_features]\n",
    "\n",
    "# For classification, set d_out = number of classes.\n",
    "d_out = num_classes\n",
    "\n",
    "# Instantiate the FTTransformer.\n",
    "model = FTTransformer(\n",
    "    n_cont_features=n_cont_features,\n",
    "    cat_cardinalities=cat_cardinalities,\n",
    "    d_out=d_out,\n",
    "    n_blocks=3,\n",
    "    d_block=192,                \n",
    "    attention_n_heads=8,\n",
    "    attention_dropout=0.2,\n",
    "    ffn_d_hidden=None,          \n",
    "    ffn_d_hidden_multiplier=4/3,\n",
    "    ffn_dropout=0.1,\n",
    "    residual_dropout=0.0\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set up optimizer, loss function, and scheduler.\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# Training loop with early stopping.\n",
    "max_epochs = 100\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        cont = batch[\"cont\"].to(device)\n",
    "        cat = batch[\"cat\"].to(device)\n",
    "        targets = batch[\"target\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(cont, cat) \n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * cont.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            cont = batch[\"cont\"].to(device)\n",
    "            cat = batch[\"cat\"].to(device)\n",
    "            targets = batch[\"target\"].to(device)\n",
    "            \n",
    "            logits = model(cont, cat)\n",
    "            loss = criterion(logits, targets)\n",
    "            val_loss += loss.item() * cont.size(0)\n",
    "    val_loss /= len(val_dataset)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Save the best model.\n",
    "save_model_path = \"best_ft_transformer_classification.pt\"\n",
    "torch.save(best_model_state, save_model_path)\n",
    "print(\"Trained model saved to\", save_model_path)\n",
    "\n",
    "# Load the best model.\n",
    "model.load_state_dict(torch.load(save_model_path))\n",
    "\n",
    "# Evaluate classification performance on the test set.\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        cont = batch[\"cont\"].to(device)\n",
    "        cat = batch[\"cat\"].to(device)\n",
    "        targets = batch[\"target\"].to(device)\n",
    "        \n",
    "        logits = model(cont, cat)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Classification Report (Test):\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ac3d0d",
   "metadata": {},
   "source": [
    "## Margin of errors for the ft-transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d06fc855-97f0-48b0-8df1-352106a5f32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.0941 | Val Loss: 1.0624\n",
      "Epoch 2: Train Loss: 1.0491 | Val Loss: 1.0481\n",
      "Epoch 3: Train Loss: 1.0447 | Val Loss: 1.0407\n",
      "Epoch 4: Train Loss: 1.0392 | Val Loss: 1.0390\n",
      "Epoch 5: Train Loss: 1.0342 | Val Loss: 1.0369\n",
      "Epoch 6: Train Loss: 1.0341 | Val Loss: 1.0315\n",
      "Epoch 7: Train Loss: 1.0240 | Val Loss: 1.0204\n",
      "Epoch 8: Train Loss: 1.0204 | Val Loss: 1.0066\n",
      "Epoch 9: Train Loss: 1.0051 | Val Loss: 0.9841\n",
      "Epoch 10: Train Loss: 0.9869 | Val Loss: 0.9368\n",
      "Epoch 11: Train Loss: 0.9405 | Val Loss: 0.8596\n",
      "Epoch 12: Train Loss: 0.8710 | Val Loss: 0.7485\n",
      "Epoch 13: Train Loss: 0.7629 | Val Loss: 0.6240\n",
      "Epoch 14: Train Loss: 0.6741 | Val Loss: 0.5347\n",
      "Epoch 15: Train Loss: 0.6215 | Val Loss: 0.4861\n",
      "Epoch 16: Train Loss: 0.5702 | Val Loss: 0.4439\n",
      "Epoch 17: Train Loss: 0.5395 | Val Loss: 0.4257\n",
      "Epoch 18: Train Loss: 0.4969 | Val Loss: 0.4071\n",
      "Epoch 19: Train Loss: 0.4943 | Val Loss: 0.3988\n",
      "Epoch 20: Train Loss: 0.4719 | Val Loss: 0.3869\n",
      "Epoch 21: Train Loss: 0.4724 | Val Loss: 0.3813\n",
      "Epoch 22: Train Loss: 0.4293 | Val Loss: 0.3817\n",
      "Epoch 23: Train Loss: 0.4283 | Val Loss: 0.3675\n",
      "Epoch 24: Train Loss: 0.4261 | Val Loss: 0.3597\n",
      "Epoch 25: Train Loss: 0.4251 | Val Loss: 0.3637\n",
      "Epoch 26: Train Loss: 0.4212 | Val Loss: 0.3555\n",
      "Epoch 27: Train Loss: 0.4070 | Val Loss: 0.3595\n",
      "Epoch 28: Train Loss: 0.4079 | Val Loss: 0.3557\n",
      "Epoch 29: Train Loss: 0.3834 | Val Loss: 0.3619\n",
      "Early stopping triggered.\n",
      "Trained model saved to best_ft_transformer_classification.pt\n",
      "Test Accuracy: 0.9043478260869565\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.84      0.79      0.81        72\n",
      "          CN       0.98      0.95      0.97       106\n",
      "         MCI       0.89      0.92      0.90       167\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.90      0.89      0.89       345\n",
      "weighted avg       0.90      0.90      0.90       345\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 57   0  15]\n",
      " [  0 101   5]\n",
      " [ 11   2 154]]\n",
      "\n",
      "Direct Calculation for Accuracy:\n",
      "Standard Error: 0.0158\n",
      "Margin of Error (95% CI): 0.0310\n",
      "95% Confidence Interval for Accuracy: [0.8733, 0.9354]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_160858/3551740628.py:192: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(save_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bootstrapping for Accuracy:\n",
      "Bootstrapped Mean Accuracy: 0.9036\n",
      "Standard Error (Bootstrap): 0.0160\n",
      "Margin of Error (Bootstrap, 95% CI): 0.0314\n",
      "\n",
      "Weighted F1 Score: 0.9041073677435971\n",
      "\n",
      "Bootstrapping for Weighted F1 Score:\n",
      "Bootstrapped Mean Weighted F1: 0.9038\n",
      "Standard Error (Bootstrap) for Weighted F1: 0.0158\n",
      "Margin of Error (Bootstrap, 95% CI) for Weighted F1: 0.0311\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from rtdl_revisiting_models import FTTransformer  \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "import math\n",
    "\n",
    "\n",
    "# Load CSV files for train, validation, and test splits.\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "# Define feature columns and target label.\n",
    "numerical_features = [\"Age\", \"CDGLOBAL\", \"CDRSB\", \"MMSCORE\", \"HMSCORE\", \"NPISCORE\", \"GDTOTAL\"]\n",
    "categorical_features = [\"GENOTYPE\"]\n",
    "label = \"Group\"\n",
    "\n",
    "# Subset dataframes to desired columns.\n",
    "cols = numerical_features + categorical_features + [label]\n",
    "train_data = train_data[cols]\n",
    "val_data   = val_data[cols]\n",
    "test_data  = test_data[cols]\n",
    "\n",
    "# Handle missingness for numerical features.\n",
    "cols_with_missing = [\"CDRSB\", \"MMSCORE\", \"HMSCORE\", \"NPISCORE\", \"GDTOTAL\"]\n",
    "for col in cols_with_missing:\n",
    "    for df in [train_data, val_data, test_data]:\n",
    "        # Create a missing indicator\n",
    "        df[col + \"_is_missing\"] = df[col].isnull().astype(int)\n",
    "        # Use a sentinel value (-999) for missing data\n",
    "        df[col] = df[col].fillna(-999)\n",
    "\n",
    "# Extend the list of continuous features to include missing indicators.\n",
    "numerical_features_extended = numerical_features + [col + \"_is_missing\" for col in cols_with_missing]\n",
    "\n",
    "# Encode categorical features.\n",
    "cat_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    train_data[col] = le.fit_transform(train_data[col].astype(str))\n",
    "    val_data[col]   = le.transform(val_data[col].astype(str))\n",
    "    test_data[col]  = le.transform(test_data[col].astype(str))\n",
    "    cat_encoders[col] = le\n",
    "\n",
    "# Encode the target labels.\n",
    "label_encoder = LabelEncoder()\n",
    "train_data[label] = label_encoder.fit_transform(train_data[label])\n",
    "val_data[label]   = label_encoder.transform(val_data[label])\n",
    "test_data[label]  = label_encoder.transform(test_data[label])\n",
    "num_classes = len(label_encoder.classes_)  \n",
    "\n",
    "\n",
    "# Continuous features.\n",
    "X_train_cont = train_data[numerical_features_extended].values.astype(np.float32)\n",
    "X_val_cont   = val_data[numerical_features_extended].values.astype(np.float32)\n",
    "X_test_cont  = test_data[numerical_features_extended].values.astype(np.float32)\n",
    "\n",
    "# Categorical features.\n",
    "X_train_cat = train_data[categorical_features].values.astype(np.int64)\n",
    "X_val_cat   = val_data[categorical_features].values.astype(np.int64)\n",
    "X_test_cat  = test_data[categorical_features].values.astype(np.int64)\n",
    "\n",
    "# Labels.\n",
    "y_train = train_data[label].values.astype(np.int64)\n",
    "y_val   = val_data[label].values.astype(np.int64)\n",
    "y_test  = test_data[label].values.astype(np.int64)\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, cont, cat, labels):\n",
    "        self.cont = cont\n",
    "        self.cat = cat\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"cont\": torch.tensor(self.cont[idx], dtype=torch.float32),\n",
    "            \"cat\": torch.tensor(self.cat[idx], dtype=torch.long),\n",
    "            \"target\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = TabularDataset(X_train_cont, X_train_cat, y_train)\n",
    "val_dataset   = TabularDataset(X_val_cont, X_val_cat, y_val)\n",
    "test_dataset  = TabularDataset(X_test_cont, X_test_cat, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "n_cont_features = X_train_cont.shape[1]\n",
    "cat_cardinalities = [int(train_data[col].nunique()) for col in categorical_features]\n",
    "d_out = num_classes  # Number of classes for classification\n",
    "\n",
    "model = FTTransformer(\n",
    "    n_cont_features=n_cont_features,\n",
    "    cat_cardinalities=cat_cardinalities,\n",
    "    d_out=d_out,\n",
    "    n_blocks=3,\n",
    "    d_block=192,                \n",
    "    attention_n_heads=8,\n",
    "    attention_dropout=0.2,\n",
    "    ffn_d_hidden=None,\n",
    "    ffn_d_hidden_multiplier=4/3,\n",
    "    ffn_dropout=0.1,\n",
    "    residual_dropout=0.0\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# 5. Set Up Optimizer, Loss, and Scheduler\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "\n",
    "# 6. Training Loop with Early Stopping\n",
    "\n",
    "max_epochs = 100\n",
    "patience = 3\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        cont = batch[\"cont\"].to(device)\n",
    "        cat = batch[\"cat\"].to(device)\n",
    "        targets = batch[\"target\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(cont, cat)\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * cont.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            cont = batch[\"cont\"].to(device)\n",
    "            cat = batch[\"cat\"].to(device)\n",
    "            targets = batch[\"target\"].to(device)\n",
    "            \n",
    "            logits = model(cont, cat)\n",
    "            loss = criterion(logits, targets)\n",
    "            val_loss += loss.item() * cont.size(0)\n",
    "    val_loss /= len(val_dataset)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Save the best model.\n",
    "save_model_path = \"best_ft_transformer_classification.pt\"\n",
    "torch.save(best_model_state, save_model_path)\n",
    "print(\"Trained model saved to\", save_model_path)\n",
    "\n",
    "# Load the best model.\n",
    "model.load_state_dict(torch.load(save_model_path))\n",
    "\n",
    "\n",
    "# 7. Evaluate on the Test Set\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        cont = batch[\"cont\"].to(device)\n",
    "        cat = batch[\"cat\"].to(device)\n",
    "        targets = batch[\"target\"].to(device)\n",
    "        \n",
    "        logits = model(cont, cat)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Classification Report (Test):\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))\n",
    "\n",
    "conf_matrix = confusion_matrix(all_targets, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "\n",
    "# 8. Compute Margin of Error for Test Accuracy and Weighted F1 Score\n",
    "\n",
    "\n",
    "# -- Accuracy Margin of Error --\n",
    "\n",
    "# Approach 1: Direct Calculation (Binomial Formula)\n",
    "n_test = len(test_dataset)\n",
    "se_direct = math.sqrt(test_acc * (1 - test_acc) / n_test)\n",
    "moe_direct = 1.96 * se_direct\n",
    "print(\"\\nDirect Calculation for Accuracy:\")\n",
    "print(\"Standard Error: {:.4f}\".format(se_direct))\n",
    "print(\"Margin of Error (95% CI): {:.4f}\".format(moe_direct))\n",
    "print(\"95% Confidence Interval for Accuracy: [{:.4f}, {:.4f}]\".format(test_acc - moe_direct, test_acc + moe_direct))\n",
    "\n",
    "# Approach 2: Bootstrapping for Accuracy\n",
    "n_bootstrap = 1000\n",
    "boot_accs = []\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_test), n_test, replace=True)\n",
    "    acc_bs = accuracy_score(all_targets[indices], all_preds[indices])\n",
    "    boot_accs.append(acc_bs)\n",
    "boot_accs = np.array(boot_accs)\n",
    "se_bootstrap = np.std(boot_accs)\n",
    "moe_bootstrap = 1.96 * se_bootstrap\n",
    "print(\"\\nBootstrapping for Accuracy:\")\n",
    "print(\"Bootstrapped Mean Accuracy: {:.4f}\".format(np.mean(boot_accs)))\n",
    "print(\"Standard Error (Bootstrap): {:.4f}\".format(se_bootstrap))\n",
    "print(\"Margin of Error (Bootstrap, 95% CI): {:.4f}\".format(moe_bootstrap))\n",
    "\n",
    "# -- Weighted F1 Score Margin of Error --\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "weighted_f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "print(\"\\nWeighted F1 Score:\", weighted_f1)\n",
    "\n",
    "# Bootstrapping for Weighted F1 Score\n",
    "boot_f1s = []\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_test), n_test, replace=True)\n",
    "    f1_bs = f1_score(all_targets[indices], all_preds[indices], average='weighted')\n",
    "    boot_f1s.append(f1_bs)\n",
    "boot_f1s = np.array(boot_f1s)\n",
    "se_boot_f1 = np.std(boot_f1s)\n",
    "moe_boot_f1 = 1.96 * se_boot_f1\n",
    "print(\"\\nBootstrapping for Weighted F1 Score:\")\n",
    "print(\"Bootstrapped Mean Weighted F1: {:.4f}\".format(np.mean(boot_f1s)))\n",
    "print(\"Standard Error (Bootstrap) for Weighted F1: {:.4f}\".format(se_boot_f1))\n",
    "print(\"Margin of Error (Bootstrap, 95% CI) for Weighted F1: {:.4f}\".format(moe_boot_f1))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
