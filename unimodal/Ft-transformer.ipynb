{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d544b4d-5758-4882-8d6c-aeb1cc85b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Import the FTTransformer model from rtdl_revisiting_models.\n",
    "from rtdl_revisiting_models import FTTransformer\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Preprocess the Data\n",
    "##########################################\n",
    "# Load CSV files for train, validation, and test splits.\n",
    "train_data = pd.read_csv(\"train_data.csv\")\n",
    "val_data   = pd.read_csv(\"val_data.csv\")\n",
    "test_data  = pd.read_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18ec215f-bc00-443d-bfb6-d855ee732758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns and target label.\n",
    "numerical_features = [\"Age\", \"CDGLOBAL\", \"CDRSB\", \"MMSCORE\", \"HMSCORE\", \"NPISCORE\", \"GDTOTAL\"]\n",
    "categorical_features = [\"GENOTYPE\"]\n",
    "label = \"Group\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e419fc3c-4d64-4ba8-b66a-3558f4ab9afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset dataframes to desired columns.\n",
    "cols = numerical_features + categorical_features + [label]\n",
    "train_data = train_data[cols]\n",
    "val_data   = val_data[cols]\n",
    "test_data  = test_data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71de3af1-7f9d-41fc-8559-a4ecb230ffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missingness for numerical features.\n",
    "cols_with_missing = [\"CDRSB\", \"MMSCORE\", \"HMSCORE\", \"NPISCORE\", \"GDTOTAL\"]\n",
    "for col in cols_with_missing:\n",
    "    for df in [train_data, val_data, test_data]:\n",
    "        df[col + \"_is_missing\"] = df[col].isnull().astype(int)\n",
    "        df[col] = df[col].fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "448a83bd-bd95-4544-9949-076110f6e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend continuous features to include missing indicators.\n",
    "numerical_features_extended = numerical_features + [col + \"_is_missing\" for col in cols_with_missing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e6210ec-02f3-483d-b1e1-03c5df13a100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features using LabelEncoder.\n",
    "cat_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    train_data[col] = le.fit_transform(train_data[col].astype(str))\n",
    "    val_data[col]   = le.transform(val_data[col].astype(str))\n",
    "    test_data[col]  = le.transform(test_data[col].astype(str))\n",
    "    cat_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f17a012-b669-411e-b64a-33ab71bead97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target.\n",
    "label_encoder = LabelEncoder()\n",
    "train_data[label] = label_encoder.fit_transform(train_data[label])\n",
    "val_data[label]   = label_encoder.transform(val_data[label])\n",
    "test_data[label]  = label_encoder.transform(test_data[label])\n",
    "num_classes = len(label_encoder.classes_)  # e.g., 3 for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15abcc9f-e7d5-4704-8dd6-f6e05cfb76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# 2. Prepare NumPy Arrays and Create Dataset\n",
    "##########################################\n",
    "# Continuous features (including missing indicators).\n",
    "X_train_cont = train_data[numerical_features_extended].values.astype(np.float32)\n",
    "X_val_cont   = val_data[numerical_features_extended].values.astype(np.float32)\n",
    "X_test_cont  = test_data[numerical_features_extended].values.astype(np.float32)\n",
    "\n",
    "# Categorical features.\n",
    "X_train_cat = train_data[categorical_features].values.astype(np.int64)\n",
    "X_val_cat   = val_data[categorical_features].values.astype(np.int64)\n",
    "X_test_cat  = test_data[categorical_features].values.astype(np.int64)\n",
    "\n",
    "# Labels.\n",
    "y_train = train_data[label].values.astype(np.int64)\n",
    "y_val   = val_data[label].values.astype(np.int64)\n",
    "y_test  = test_data[label].values.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9578f9d3-8a03-4592-ae22-6d943501e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple PyTorch Dataset.\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, cont, cat, labels):\n",
    "        self.cont = cont\n",
    "        self.cat = cat\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"cont\": torch.tensor(self.cont[idx], dtype=torch.float32),\n",
    "            \"cat\": torch.tensor(self.cat[idx], dtype=torch.long),\n",
    "            \"target\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac788daf-81c4-4f04-9642-ef0f0750b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TabularDataset(X_train_cont, X_train_cat, y_train)\n",
    "val_dataset   = TabularDataset(X_val_cont, X_val_cat, y_val)\n",
    "test_dataset  = TabularDataset(X_test_cont, X_test_cat, y_test)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8de9eadf-da25-4e07-988d-6c2027aa5ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FTTransformer(\n",
       "  (cls_embedding): _CLSEmbedding()\n",
       "  (cont_embeddings): LinearEmbeddings()\n",
       "  (cat_embeddings): CategoricalEmbeddings(\n",
       "    (embeddings): ModuleList(\n",
       "      (0): Embedding(6, 192)\n",
       "    )\n",
       "  )\n",
       "  (backbone): FTTransformerBackbone(\n",
       "    (blocks): ModuleList(\n",
       "      (0): ModuleDict(\n",
       "        (attention): MultiheadAttention(\n",
       "          (W_q): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (W_k): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (W_v): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (W_out): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (attention_residual_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ffn_normalization): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (linear1): Linear(in_features=192, out_features=512, bias=True)\n",
       "          (activation): _ReGLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=192, bias=True)\n",
       "        )\n",
       "        (ffn_residual_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (output): Identity()\n",
       "      )\n",
       "      (1-2): 2 x ModuleDict(\n",
       "        (attention): MultiheadAttention(\n",
       "          (W_q): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (W_k): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (W_v): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (W_out): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (attention_residual_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ffn_normalization): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (linear1): Linear(in_features=192, out_features=512, bias=True)\n",
       "          (activation): _ReGLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=192, bias=True)\n",
       "        )\n",
       "        (ffn_residual_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (output): Identity()\n",
       "        (attention_normalization): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (output): Sequential(\n",
       "      (normalization): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (activation): ReLU()\n",
       "      (linear): Linear(in_features=192, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################\n",
    "# 3. Initialize and Train the FTTransformer Classifier\n",
    "##########################################\n",
    "# Get the number of continuous features.\n",
    "n_cont_features = X_train_cont.shape[1]\n",
    "# Determine the cardinalities for each categorical feature.\n",
    "cat_cardinalities = [int(train_data[col].nunique()) for col in categorical_features]\n",
    "\n",
    "# For classification, set d_out = number of classes.\n",
    "d_out = num_classes\n",
    "\n",
    "# Instantiate the FTTransformer.\n",
    "model = FTTransformer(\n",
    "    n_cont_features=n_cont_features,\n",
    "    cat_cardinalities=cat_cardinalities,\n",
    "    d_out=d_out,\n",
    "    n_blocks=3,\n",
    "    d_block=192,                # Backbone (hidden) dimension\n",
    "    attention_n_heads=8,\n",
    "    attention_dropout=0.2,\n",
    "    ffn_d_hidden=None,          # Defaults internally if None.\n",
    "    ffn_d_hidden_multiplier=4/3,\n",
    "    ffn_dropout=0.1,\n",
    "    residual_dropout=0.0\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eb4c5f8-2bf3-4200-a1f2-9c7ef5e5188e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rittikar-s/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.0578 | Val Loss: 1.0382\n",
      "Epoch 2: Train Loss: 1.0128 | Val Loss: 0.8282\n",
      "Epoch 3: Train Loss: 0.7741 | Val Loss: 0.5106\n",
      "Epoch 4: Train Loss: 0.5536 | Val Loss: 0.4235\n",
      "Epoch 5: Train Loss: 0.4884 | Val Loss: 0.3976\n",
      "Epoch 6: Train Loss: 0.4308 | Val Loss: 0.3562\n",
      "Epoch 7: Train Loss: 0.3916 | Val Loss: 0.4066\n",
      "Epoch 8: Train Loss: 0.3812 | Val Loss: 0.3571\n",
      "Epoch 9: Train Loss: 0.3757 | Val Loss: 0.3487\n",
      "Epoch 10: Train Loss: 0.3501 | Val Loss: 0.3483\n",
      "Epoch 11: Train Loss: 0.3555 | Val Loss: 0.3395\n",
      "Epoch 12: Train Loss: 0.3441 | Val Loss: 0.3403\n",
      "Epoch 13: Train Loss: 0.3334 | Val Loss: 0.3384\n",
      "Epoch 14: Train Loss: 0.3370 | Val Loss: 0.3344\n",
      "Epoch 15: Train Loss: 0.3442 | Val Loss: 0.3401\n",
      "Epoch 16: Train Loss: 0.3461 | Val Loss: 0.3897\n",
      "Epoch 17: Train Loss: 0.3406 | Val Loss: 0.3461\n",
      "Epoch 18: Train Loss: 0.3236 | Val Loss: 0.3335\n",
      "Epoch 19: Train Loss: 0.3139 | Val Loss: 0.3337\n",
      "Epoch 20: Train Loss: 0.3174 | Val Loss: 0.3450\n",
      "Epoch 21: Train Loss: 0.3264 | Val Loss: 0.3954\n",
      "Epoch 22: Train Loss: 0.3232 | Val Loss: 0.3419\n",
      "Epoch 23: Train Loss: 0.3078 | Val Loss: 0.3262\n",
      "Epoch 24: Train Loss: 0.3062 | Val Loss: 0.3405\n",
      "Epoch 25: Train Loss: 0.2933 | Val Loss: 0.3208\n",
      "Epoch 26: Train Loss: 0.3014 | Val Loss: 0.3418\n",
      "Epoch 27: Train Loss: 0.2956 | Val Loss: 0.3316\n",
      "Epoch 28: Train Loss: 0.2974 | Val Loss: 0.3406\n",
      "Epoch 29: Train Loss: 0.2957 | Val Loss: 0.3434\n",
      "Epoch 30: Train Loss: 0.2930 | Val Loss: 0.3281\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "# Set up optimizer, loss function, and scheduler.\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# Training loop with early stopping.\n",
    "max_epochs = 100\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        cont = batch[\"cont\"].to(device)\n",
    "        cat = batch[\"cat\"].to(device)\n",
    "        targets = batch[\"target\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(cont, cat)  # Forward pass returns logits (shape: [batch_size, d_out])\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * cont.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            cont = batch[\"cont\"].to(device)\n",
    "            cat = batch[\"cat\"].to(device)\n",
    "            targets = batch[\"target\"].to(device)\n",
    "            \n",
    "            logits = model(cont, cat)\n",
    "            loss = criterion(logits, targets)\n",
    "            val_loss += loss.item() * cont.size(0)\n",
    "    val_loss /= len(val_dataset)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39d7ffb8-316a-4684-9021-1d10a1fc26d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model saved to best_ft_transformer_classification.pt\n"
     ]
    }
   ],
   "source": [
    "# Save the best model.\n",
    "save_model_path = \"best_ft_transformer_classification.pt\"\n",
    "torch.save(best_model_state, save_model_path)\n",
    "print(\"Trained model saved to\", save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6c7b4e2-ede4-4aac-b351-56111dc53822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best model (optional).\n",
    "model.load_state_dict(torch.load(save_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e381fec-1bc0-4e98-a35a-48fe7aafc0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9043478260869565\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.83      0.81      0.82        72\n",
      "          CN       0.98      0.95      0.97       106\n",
      "         MCI       0.89      0.92      0.90       167\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.90      0.89      0.90       345\n",
      "weighted avg       0.90      0.90      0.90       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate classification performance on the test set.\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        cont = batch[\"cont\"].to(device)\n",
    "        cat = batch[\"cat\"].to(device)\n",
    "        targets = batch[\"target\"].to(device)\n",
    "        \n",
    "        logits = model(cont, cat)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Classification Report (Test):\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb532d63-b4a9-4114-8d72-91358a97f986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted feature shapes:\n",
      "Train features: (1605, 3)\n",
      "Validation features: (344, 3)\n",
      "Test features: (345, 3)\n",
      "Extracted features saved as 'ft_train_features.npy', 'ft_val_features.npy', and 'ft_test_features.npy'.\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "# 4. Extract 192-Dimensional Embeddings (Before the Final Classification)\n",
    "##########################################\n",
    "# The classifier was trained with d_out = num_classes.\n",
    "# To obtain 192-dim embeddings (the backbone outputs), we replace the final linear layer with an identity.\n",
    "# Here we assume the final projection is stored in the attribute 'fc'.\n",
    "model.fc = nn.Identity()  # Now model(cont, cat) returns the backbone features of shape (batch_size, 192).\n",
    "\n",
    "# Function to extract features from a DataLoader.\n",
    "def extract_features(loader, model, device):\n",
    "    model.eval()\n",
    "    features_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            cont = batch[\"cont\"].to(device)\n",
    "            cat = batch[\"cat\"].to(device)\n",
    "            feats = model(cont, cat)  # Should now return features of shape (batch_size, 192)\n",
    "            features_list.append(feats.cpu().numpy())\n",
    "    return np.concatenate(features_list)\n",
    "\n",
    "# Extract features from each set.\n",
    "features_train = extract_features(train_loader, model, device)\n",
    "features_val   = extract_features(val_loader, model, device)\n",
    "features_test  = extract_features(test_loader, model, device)\n",
    "\n",
    "print(\"Extracted feature shapes:\")\n",
    "print(\"Train features:\", features_train.shape)\n",
    "print(\"Validation features:\", features_val.shape)\n",
    "print(\"Test features:\", features_test.shape)\n",
    "\n",
    "# Save the extracted features.\n",
    "np.save(\"ft_train_features.npy\", features_train)\n",
    "np.save(\"ft_val_features.npy\", features_val)\n",
    "np.save(\"ft_test_features.npy\", features_test)\n",
    "print(\"Extracted features saved as 'ft_train_features.npy', 'ft_val_features.npy', and 'ft_test_features.npy'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
