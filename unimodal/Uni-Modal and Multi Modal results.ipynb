{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4517caa0-5e33-421b-a2b5-246f43a26310",
   "metadata": {},
   "source": [
    "# Uni-Modal Image Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cca931-ed4c-4f9c-9530-23ff8722eb18",
   "metadata": {},
   "source": [
    "## Logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7da525f8-80a2-4409-b36a-281409f36078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9601246105919004\n",
      "Validation Accuracy: 0.5872093023255814\n",
      "Test Accuracy: 0.5652173913043478\n",
      "\n",
      "Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.58      0.56       106\n",
      "           1       0.64      0.57      0.61       167\n",
      "           2       0.45      0.51      0.48        72\n",
      "\n",
      "    accuracy                           0.57       345\n",
      "   macro avg       0.55      0.56      0.55       345\n",
      "weighted avg       0.57      0.57      0.57       345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# -----------------------------\n",
    "# Load Feature Files\n",
    "# -----------------------------\n",
    "train_features = np.load(\"Deit_train_features_1.npy\")  # shape (num_train_samples, deit_feature_dim)\n",
    "val_features   = np.load(\"Deit_val_features_1.npy\")      # shape (num_val_samples, deit_feature_dim)\n",
    "test_features  = np.load(\"Deit_test_features_1.npy\")     # shape (num_test_samples, deit_feature_dim)\n",
    "\n",
    "# -----------------------------\n",
    "# Load CSV Files with Labels\n",
    "# -----------------------------\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "# Extract label arrays (assuming the column is named \"Group\")\n",
    "y_train = train_data[\"Group\"].values\n",
    "y_val   = val_data[\"Group\"].values\n",
    "y_test  = test_data[\"Group\"].values\n",
    "\n",
    "# -----------------------------\n",
    "# Encode Labels to Integers\n",
    "# -----------------------------\n",
    "# Assuming the groups are \"CN\", \"MCI\", \"AD\", map them to 0, 1, 2.\n",
    "label_map = {\"CN\": 0, \"MCI\": 1, \"AD\": 2}\n",
    "y_train = np.array([label_map[label] for label in y_train])\n",
    "y_val   = np.array([label_map[label] for label in y_val])\n",
    "y_test  = np.array([label_map[label] for label in y_test])\n",
    "\n",
    "# -----------------------------\n",
    "# Train Logistic Regression Model\n",
    "# -----------------------------\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf.fit(train_features, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate on Training, Validation, and Test Splits\n",
    "# -----------------------------\n",
    "train_preds = clf.predict(train_features)\n",
    "train_acc = accuracy_score(y_train, train_preds)\n",
    "print(\"Train Accuracy:\", train_acc)\n",
    "\n",
    "val_preds = clf.predict(val_features)\n",
    "val_acc = accuracy_score(y_val, val_preds)\n",
    "print(\"Validation Accuracy:\", val_acc)\n",
    "\n",
    "test_preds = clf.predict(test_features)\n",
    "test_acc = accuracy_score(y_test, test_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "print(\"\\nClassification Report on Test Data:\")\n",
    "print(classification_report(y_test, test_preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f1b2f-cc0d-415e-a824-826512ce4d84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b89361c-89f9-4916-955d-9bae81024b89",
   "metadata": {},
   "source": [
    "## MLP + KNN + Randomforest + SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56155f6d-5403-4887-b24e-3492b617c611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training MLP classifier ---\n",
      "MLP Train Accuracy: 0.9988\n",
      "MLP Validation Accuracy: 0.6890\n",
      "MLP Test Accuracy: 0.6087\n",
      "MLP Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.56      0.59       106\n",
      "           1       0.64      0.68      0.66       167\n",
      "           2       0.50      0.51      0.51        72\n",
      "\n",
      "    accuracy                           0.61       345\n",
      "   macro avg       0.59      0.58      0.59       345\n",
      "weighted avg       0.61      0.61      0.61       345\n",
      "\n",
      "\n",
      "--- Training SVM classifier ---\n",
      "SVM Train Accuracy: 0.4872\n",
      "SVM Validation Accuracy: 0.4855\n",
      "SVM Test Accuracy: 0.4841\n",
      "SVM Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       106\n",
      "           1       0.48      1.00      0.65       167\n",
      "           2       0.00      0.00      0.00        72\n",
      "\n",
      "    accuracy                           0.48       345\n",
      "   macro avg       0.16      0.33      0.22       345\n",
      "weighted avg       0.23      0.48      0.32       345\n",
      "\n",
      "\n",
      "--- Training KNN classifier ---\n",
      "KNN Train Accuracy: 0.7601\n",
      "KNN Validation Accuracy: 0.6047\n",
      "KNN Test Accuracy: 0.6058\n",
      "KNN Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.66      0.59       106\n",
      "           1       0.67      0.73      0.70       167\n",
      "           2       0.59      0.24      0.34        72\n",
      "\n",
      "    accuracy                           0.61       345\n",
      "   macro avg       0.59      0.54      0.54       345\n",
      "weighted avg       0.61      0.61      0.59       345\n",
      "\n",
      "\n",
      "--- Training Random Forest classifier ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Train Accuracy: 1.0000\n",
      "Random Forest Validation Accuracy: 0.6279\n",
      "Random Forest Test Accuracy: 0.6377\n",
      "Random Forest Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.39      0.52       106\n",
      "           1       0.59      0.96      0.73       167\n",
      "           2       0.90      0.25      0.39        72\n",
      "\n",
      "    accuracy                           0.64       345\n",
      "   macro avg       0.76      0.53      0.55       345\n",
      "weighted avg       0.72      0.64      0.60       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# -----------------------------\n",
    "# Load DEiT Features from .npy files\n",
    "# -----------------------------\n",
    "# -----------------------------\n",
    "X_train = np.load(\"Deit_train_features_1.npy\")  # shape: (num_train_samples, deit_feature_dim)\n",
    "X_val   = np.load(\"Deit_val_features_1.npy\")      # shape: (num_val_samples, deit_feature_dim)\n",
    "X_test  = np.load(\"Deit_test_features_1.npy\")     # shape: (num_test_samples, deit_feature_dim)\n",
    "\n",
    "# -----------------------------\n",
    "# Load CSV Files with Labels\n",
    "# -----------------------------\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "# Extract label arrays (assuming the column is named \"Group\")\n",
    "y_train = train_data[\"Group\"].values\n",
    "y_val   = val_data[\"Group\"].values\n",
    "y_test  = test_data[\"Group\"].values\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Encode Labels to Integers\n",
    "# -----------------------------\n",
    "# Assuming the groups are \"CN\", \"MCI\", and \"AD\"\n",
    "label_map = {\"CN\": 0, \"MCI\": 1, \"AD\": 2}\n",
    "y_train = np.array([label_map[label] for label in y_train])\n",
    "y_val   = np.array([label_map[label] for label in y_val])\n",
    "y_test  = np.array([label_map[label] for label in y_test])\n",
    "\n",
    "# -----------------------------\n",
    "# Define Classifiers\n",
    "# -----------------------------\n",
    "classifiers = {\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", probability=True, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Train and Evaluate Each Classifier\n",
    "# -----------------------------\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\n--- Training {name} classifier ---\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on Training Set\n",
    "    train_preds = clf.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, train_preds)\n",
    "    print(f\"{name} Train Accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "    # Evaluate on Validation Set\n",
    "    val_preds = clf.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, val_preds)\n",
    "    print(f\"{name} Validation Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    # Evaluate on Test Set\n",
    "    test_preds = clf.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, test_preds)\n",
    "    print(f\"{name} Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Print Detailed Classification Report on Test Set\n",
    "    print(f\"{name} Classification Report on Test Data:\")\n",
    "    print(classification_report(y_test, test_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e635e604-5362-453a-b5b6-34b1397df586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.4-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: scikit-learn in ./multimodal_env/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy in ./multimodal_env/lib/python3.8/site-packages (from xgboost) (1.24.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in ./multimodal_env/lib/python3.8/site-packages (from xgboost) (2.20.5)\n",
      "Requirement already satisfied: scipy in ./multimodal_env/lib/python3.8/site-packages (from xgboost) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./multimodal_env/lib/python3.8/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./multimodal_env/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n",
      "Downloading xgboost-2.1.4-py3-none-manylinux_2_28_x86_64.whl (223.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: xgboost, lightgbm\n",
      "Successfully installed lightgbm-4.6.0 xgboost-2.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost lightgbm scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190294f1-e7d4-4881-beb0-d60b3368970a",
   "metadata": {},
   "source": [
    "## XGBoost + LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f95cf0b2-f2ae-4d95-a6b9-201d30d764e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- XGBoost Classifier ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [22:29:02] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Train Accuracy: 1.0\n",
      "XGBoost Validation Accuracy: 0.6453488372093024\n",
      "XGBoost Test Accuracy: 0.6173913043478261\n",
      "XGBoost Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.49      0.56       106\n",
      "           1       0.61      0.84      0.70       167\n",
      "           2       0.65      0.28      0.39        72\n",
      "\n",
      "    accuracy                           0.62       345\n",
      "   macro avg       0.63      0.54      0.55       345\n",
      "weighted avg       0.62      0.62      0.59       345\n",
      "\n",
      "\n",
      "--- LightGBM Classifier ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036342 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 1605, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.180370\n",
      "[LightGBM] [Info] Start training from score -0.722868\n",
      "[LightGBM] [Info] Start training from score -1.572737\n",
      "LightGBM Train Accuracy: 1.0\n",
      "LightGBM Validation Accuracy: 0.6656976744186046\n",
      "LightGBM Test Accuracy: 0.6405797101449275\n",
      "LightGBM Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.50      0.58       106\n",
      "           1       0.62      0.86      0.72       167\n",
      "           2       0.68      0.35      0.46        72\n",
      "\n",
      "    accuracy                           0.64       345\n",
      "   macro avg       0.66      0.57      0.59       345\n",
      "weighted avg       0.65      0.64      0.62       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# -----------------------------\n",
    "# Load DEiT Features from .npy files\n",
    "# -----------------------------\n",
    "X_train = np.load(\"Deit_train_features_1.npy\")  # shape: (num_train_samples, deit_feature_dim)\n",
    "X_val   = np.load(\"Deit_val_features_1.npy\")      # shape: (num_val_samples, deit_feature_dim)\n",
    "X_test  = np.load(\"Deit_test_features_1.npy\")     # shape: (num_test_samples, deit_feature_dim)\n",
    "\n",
    "# -----------------------------\n",
    "# Load CSV Files with Labels\n",
    "# -----------------------------\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "# Extract label arrays (assuming the column is named \"Group\")\n",
    "y_train = train_data[\"Group\"].values\n",
    "y_val   = val_data[\"Group\"].values\n",
    "y_test  = test_data[\"Group\"].values\n",
    "\n",
    "# -----------------------------\n",
    "# Encode Labels to Integers\n",
    "# -----------------------------\n",
    "# Assuming the groups are \"CN\", \"MCI\", and \"AD\"\n",
    "label_map = {\"CN\": 0, \"MCI\": 1, \"AD\": 2}\n",
    "y_train = np.array([label_map[label] for label in y_train])\n",
    "y_val   = np.array([label_map[label] for label in y_val])\n",
    "y_test  = np.array([label_map[label] for label in y_test])\n",
    "\n",
    "# -----------------------------\n",
    "# Train and Evaluate XGBoost Classifier\n",
    "# -----------------------------\n",
    "print(\"\\n--- XGBoost Classifier ---\")\n",
    "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "xgb_train_preds = xgb_clf.predict(X_train)\n",
    "xgb_val_preds = xgb_clf.predict(X_val)\n",
    "xgb_test_preds = xgb_clf.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"XGBoost Train Accuracy:\", accuracy_score(y_train, xgb_train_preds))\n",
    "print(\"XGBoost Validation Accuracy:\", accuracy_score(y_val, xgb_val_preds))\n",
    "print(\"XGBoost Test Accuracy:\", accuracy_score(y_test, xgb_test_preds))\n",
    "print(\"XGBoost Classification Report on Test Data:\")\n",
    "print(classification_report(y_test, xgb_test_preds))\n",
    "\n",
    "# -----------------------------\n",
    "# Train and Evaluate LightGBM Classifier\n",
    "# -----------------------------\n",
    "print(\"\\n--- LightGBM Classifier ---\")\n",
    "lgbm_clf = LGBMClassifier(random_state=42)\n",
    "lgbm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "lgbm_train_preds = lgbm_clf.predict(X_train)\n",
    "lgbm_val_preds = lgbm_clf.predict(X_val)\n",
    "lgbm_test_preds = lgbm_clf.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"LightGBM Train Accuracy:\", accuracy_score(y_train, lgbm_train_preds))\n",
    "print(\"LightGBM Validation Accuracy:\", accuracy_score(y_val, lgbm_val_preds))\n",
    "print(\"LightGBM Test Accuracy:\", accuracy_score(y_test, lgbm_test_preds))\n",
    "print(\"LightGBM Classification Report on Test Data:\")\n",
    "print(classification_report(y_test, lgbm_test_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bd4dfc5-5837-4036-a498-234e547e0211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Downloading catboost-1.2.7-cp38-cp38-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: scikit-learn in ./multimodal_env/lib/python3.8/site-packages (1.3.2)\n",
      "Collecting graphviz (from catboost)\n",
      "  Downloading graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: matplotlib in ./multimodal_env/lib/python3.8/site-packages (from catboost) (3.7.5)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in ./multimodal_env/lib/python3.8/site-packages (from catboost) (1.24.3)\n",
      "Requirement already satisfied: pandas>=0.24 in ./multimodal_env/lib/python3.8/site-packages (from catboost) (2.0.3)\n",
      "Requirement already satisfied: scipy in ./multimodal_env/lib/python3.8/site-packages (from catboost) (1.10.1)\n",
      "Collecting plotly (from catboost)\n",
      "  Downloading plotly-6.0.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: six in ./multimodal_env/lib/python3.8/site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./multimodal_env/lib/python3.8/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./multimodal_env/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./multimodal_env/lib/python3.8/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./multimodal_env/lib/python3.8/site-packages (from pandas>=0.24->catboost) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./multimodal_env/lib/python3.8/site-packages (from pandas>=0.24->catboost) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./multimodal_env/lib/python3.8/site-packages (from matplotlib->catboost) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./multimodal_env/lib/python3.8/site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./multimodal_env/lib/python3.8/site-packages (from matplotlib->catboost) (4.55.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./multimodal_env/lib/python3.8/site-packages (from matplotlib->catboost) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in ./multimodal_env/lib/python3.8/site-packages (from matplotlib->catboost) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./multimodal_env/lib/python3.8/site-packages (from matplotlib->catboost) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./multimodal_env/lib/python3.8/site-packages (from matplotlib->catboost) (3.1.4)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./multimodal_env/lib/python3.8/site-packages (from matplotlib->catboost) (6.4.5)\n",
      "Collecting narwhals>=1.15.1 (from plotly->catboost)\n",
      "  Downloading narwhals-1.30.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./multimodal_env/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib->catboost) (3.20.2)\n",
      "Downloading catboost-1.2.7-cp38-cp38-manylinux2014_x86_64.whl (98.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "Downloading plotly-6.0.0-py3-none-any.whl (14.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading narwhals-1.30.0-py3-none-any.whl (313 kB)\n",
      "Installing collected packages: narwhals, graphviz, plotly, catboost\n",
      "Successfully installed catboost-1.2.7 graphviz-0.20.3 narwhals-1.30.0 plotly-6.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b35413-c862-4271-918e-f5949e8d71a2",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bb7cb0b6-a1dc-4c26-8405-89111a6d8b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.0838489\ttest: 1.0889739\tbest: 1.0889739 (0)\ttotal: 68.8ms\tremaining: 34.3s\n",
      "100:\tlearn: 0.6003025\ttest: 0.9132813\tbest: 0.9132813 (100)\ttotal: 6.84s\tremaining: 27s\n",
      "200:\tlearn: 0.3425623\ttest: 0.8427394\tbest: 0.8427394 (200)\ttotal: 13.6s\tremaining: 20.2s\n",
      "300:\tlearn: 0.2175492\ttest: 0.8141010\tbest: 0.8133172 (295)\ttotal: 20.3s\tremaining: 13.4s\n",
      "400:\tlearn: 0.1502539\ttest: 0.8022451\tbest: 0.8009836 (387)\ttotal: 27s\tremaining: 6.68s\n",
      "499:\tlearn: 0.1089594\ttest: 0.7832818\tbest: 0.7832818 (499)\ttotal: 33.7s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7832818385\n",
      "bestIteration = 499\n",
      "\n",
      "CatBoost Train Accuracy: 1.0\n",
      "CatBoost Validation Accuracy: 0.627906976744186\n",
      "CatBoost Test Accuracy: 0.6202898550724638\n",
      "\n",
      "CatBoost Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.44      0.53       106\n",
      "           1       0.59      0.89      0.71       167\n",
      "           2       0.76      0.26      0.39        72\n",
      "\n",
      "    accuracy                           0.62       345\n",
      "   macro avg       0.67      0.53      0.55       345\n",
      "weighted avg       0.65      0.62      0.59       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# -----------------------------\n",
    "# Load DEiT Features from .npy files\n",
    "# -----------------------------\n",
    "X_train = np.load(\"Deit_train_features_1.npy\")  # shape (num_train_samples, deit_feature_dim)\n",
    "X_val   = np.load(\"Deit_val_features_1.npy\")      # shape (num_val_samples, deit_feature_dim)\n",
    "X_test  = np.load(\"Deit_test_features_1.npy\")     # shape (num_test_samples, deit_feature_dim)\n",
    "\n",
    "# -----------------------------\n",
    "# Load CSV Files with Labels\n",
    "# -----------------------------\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "# Extract label arrays (assuming the column is named \"Group\")\n",
    "y_train = train_data[\"Group\"].values\n",
    "y_val   = val_data[\"Group\"].values\n",
    "y_test  = test_data[\"Group\"].values\n",
    "\n",
    "# -----------------------------\n",
    "# Encode Labels to Integers\n",
    "# -----------------------------\n",
    "# Assuming the groups are \"CN\", \"MCI\", and \"AD\"\n",
    "label_map = {\"CN\": 0, \"MCI\": 1, \"AD\": 2}\n",
    "y_train = np.array([label_map[label] for label in y_train])\n",
    "y_val   = np.array([label_map[label] for label in y_val])\n",
    "y_test  = np.array([label_map[label] for label in y_test])\n",
    "\n",
    "# -----------------------------\n",
    "# Train and Evaluate CatBoost Classifier\n",
    "# -----------------------------\n",
    "catboost_clf = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    loss_function='MultiClass',\n",
    "    verbose=100,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# Train the classifier, using the validation set for early stopping\n",
    "catboost_clf.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)\n",
    "\n",
    "# Predictions on each split\n",
    "train_preds = catboost_clf.predict(X_train)\n",
    "val_preds = catboost_clf.predict(X_val)\n",
    "test_preds = catboost_clf.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"CatBoost Train Accuracy:\", accuracy_score(y_train, train_preds))\n",
    "print(\"CatBoost Validation Accuracy:\", accuracy_score(y_val, val_preds))\n",
    "print(\"CatBoost Test Accuracy:\", accuracy_score(y_test, test_preds))\n",
    "print(\"\\nCatBoost Classification Report on Test Data:\")\n",
    "print(classification_report(y_test, test_preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bad0bd8-7ab2-47eb-a206-bcc5af61f703",
   "metadata": {},
   "source": [
    "## Ft-Transformer model embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d25ce3-07ee-4bab-8c4e-9b1d3bb826c8",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8861bcd4-0e39-48ef-a6d3-fbe19e98b8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8959501557632399\n",
      "Validation Accuracy: 0.8575581395348837\n",
      "Test Accuracy: 0.9130434782608695\n",
      "\n",
      "Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97       106\n",
      "           1       0.92      0.90      0.91       167\n",
      "           2       0.82      0.88      0.85        72\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.90      0.91      0.91       345\n",
      "weighted avg       0.92      0.91      0.91       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# -----------------------------\n",
    "# Load Feature Files\n",
    "# -----------------------------\n",
    "train_features = np.load(\"ft_train_embeddings.npy\")  # shape (num_train_samples, deit_feature_dim)\n",
    "val_features   = np.load(\"ft_val_embeddings.npy\")      # shape (num_val_samples, deit_feature_dim)\n",
    "test_features  = np.load(\"ft_test_embeddings.npy\")     # shape (num_test_samples, deit_feature_dim)\n",
    "\n",
    "# -----------------------------\n",
    "# Load CSV Files with Labels\n",
    "# -----------------------------\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "# Extract label arrays (assuming the column is named \"Group\")\n",
    "y_train = train_data[\"Group\"].values\n",
    "y_val   = val_data[\"Group\"].values\n",
    "y_test  = test_data[\"Group\"].values\n",
    "\n",
    "# -----------------------------\n",
    "# Encode Labels to Integers\n",
    "# -----------------------------\n",
    "# Assuming the groups are \"CN\", \"MCI\", \"AD\", map them to 0, 1, 2.\n",
    "label_map = {\"CN\": 0, \"MCI\": 1, \"AD\": 2}\n",
    "y_train = np.array([label_map[label] for label in y_train])\n",
    "y_val   = np.array([label_map[label] for label in y_val])\n",
    "y_test  = np.array([label_map[label] for label in y_test])\n",
    "\n",
    "# -----------------------------\n",
    "# Train Logistic Regression Model\n",
    "# -----------------------------\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf.fit(train_features, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate on Training, Validation, and Test Splits\n",
    "# -----------------------------\n",
    "train_preds = clf.predict(train_features)\n",
    "train_acc = accuracy_score(y_train, train_preds)\n",
    "print(\"Train Accuracy:\", train_acc)\n",
    "\n",
    "val_preds = clf.predict(val_features)\n",
    "val_acc = accuracy_score(y_val, val_preds)\n",
    "print(\"Validation Accuracy:\", val_acc)\n",
    "\n",
    "test_preds = clf.predict(test_features)\n",
    "test_acc = accuracy_score(y_test, test_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "print(\"\\nClassification Report on Test Data:\")\n",
    "print(classification_report(y_test, test_preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c9518f-cdc2-4ef4-b465-8f9e149f463d",
   "metadata": {},
   "source": [
    "## MLP + KNN + Random Forest + SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d39555a-e02d-4aa5-a3d8-be59d17aa210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training MLP classifier ---\n",
      "MLP Train Accuracy: 0.8903\n",
      "MLP Validation Accuracy: 0.8547\n",
      "MLP Test Accuracy: 0.9130\n",
      "MLP Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97       106\n",
      "           1       0.91      0.91      0.91       167\n",
      "           2       0.83      0.86      0.84        72\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.91      0.91      0.91       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n",
      "\n",
      "--- Training SVM classifier ---\n",
      "SVM Train Accuracy: 0.8860\n",
      "SVM Validation Accuracy: 0.8605\n",
      "SVM Test Accuracy: 0.9043\n",
      "SVM Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97       106\n",
      "           1       0.89      0.92      0.90       167\n",
      "           2       0.84      0.79      0.81        72\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.90      0.89      0.89       345\n",
      "weighted avg       0.90      0.90      0.90       345\n",
      "\n",
      "\n",
      "--- Training KNN classifier ---\n",
      "KNN Train Accuracy: 0.9121\n",
      "KNN Validation Accuracy: 0.8721\n",
      "KNN Test Accuracy: 0.9014\n",
      "KNN Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96       106\n",
      "           1       0.91      0.88      0.90       167\n",
      "           2       0.79      0.88      0.83        72\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.89      0.90      0.90       345\n",
      "weighted avg       0.90      0.90      0.90       345\n",
      "\n",
      "\n",
      "--- Training Random Forest classifier ---\n",
      "Random Forest Train Accuracy: 0.9994\n",
      "Random Forest Validation Accuracy: 0.9041\n",
      "Random Forest Test Accuracy: 0.9159\n",
      "Random Forest Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       106\n",
      "           1       0.91      0.92      0.91       167\n",
      "           2       0.84      0.85      0.84        72\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.91      0.91      0.91       345\n",
      "weighted avg       0.92      0.92      0.92       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# -----------------------------\n",
    "# Load DEiT Features from .npy files\n",
    "# -----------------------------\n",
    "X_train = np.load(\"ft_train_embeddings.npy\")  # shape (num_train_samples, deit_feature_dim)\n",
    "X_val   = np.load(\"ft_val_embeddings.npy\")      # shape (num_val_samples, deit_feature_dim)\n",
    "X_test  = np.load(\"ft_test_embeddings.npy\")     # shape (num_test_samples, deit_feature_dim)\n",
    "\n",
    "# -----------------------------\n",
    "# Load CSV Files with Labels\n",
    "# -----------------------------\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "# Extract label arrays (assuming the column is named \"Group\")\n",
    "y_train = train_data[\"Group\"].values\n",
    "y_val   = val_data[\"Group\"].values\n",
    "y_test  = test_data[\"Group\"].values\n",
    "\n",
    "# -----------------------------\n",
    "# Encode Labels to Integers\n",
    "# -----------------------------\n",
    "# Assuming the groups are \"CN\", \"MCI\", and \"AD\"\n",
    "label_map = {\"CN\": 0, \"MCI\": 1, \"AD\": 2}\n",
    "y_train = np.array([label_map[label] for label in y_train])\n",
    "y_val   = np.array([label_map[label] for label in y_val])\n",
    "y_test  = np.array([label_map[label] for label in y_test])\n",
    "\n",
    "# -----------------------------\n",
    "# Define Classifiers\n",
    "# -----------------------------\n",
    "classifiers = {\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", probability=True, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Train and Evaluate Each Classifier\n",
    "# -----------------------------\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\n--- Training {name} classifier ---\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on Training Set\n",
    "    train_preds = clf.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, train_preds)\n",
    "    print(f\"{name} Train Accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "    # Evaluate on Validation Set\n",
    "    val_preds = clf.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, val_preds)\n",
    "    print(f\"{name} Validation Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    # Evaluate on Test Set\n",
    "    test_preds = clf.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, test_preds)\n",
    "    print(f\"{name} Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Print Detailed Classification Report on Test Set\n",
    "    print(f\"{name} Classification Report on Test Data:\")\n",
    "    print(classification_report(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226954d1-3595-4d2e-bb83-c722b0445397",
   "metadata": {},
   "source": [
    "## XGBOOST + LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9c3b25c8-72e1-4744-b30d-87ea8bb5ac25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- XGBoost Classifier ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [22:42:23] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Train Accuracy: 0.9993769470404984\n",
      "XGBoost Validation Accuracy: 0.9069767441860465\n",
      "XGBoost Test Accuracy: 0.9130434782608695\n",
      "XGBoost Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97       106\n",
      "           1       0.92      0.90      0.91       167\n",
      "           2       0.82      0.88      0.85        72\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.90      0.91      0.91       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n",
      "\n",
      "--- LightGBM Classifier ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32959\n",
      "[LightGBM] [Info] Number of data points in the train set: 1605, number of used features: 177\n",
      "[LightGBM] [Info] Start training from score -1.180370\n",
      "[LightGBM] [Info] Start training from score -0.722868\n",
      "[LightGBM] [Info] Start training from score -1.572737\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM Train Accuracy: 0.9993769470404984\n",
      "LightGBM Validation Accuracy: 0.9040697674418605\n",
      "LightGBM Test Accuracy: 0.9043478260869565\n",
      "LightGBM Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97       106\n",
      "           1       0.92      0.87      0.90       167\n",
      "           2       0.78      0.89      0.83        72\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.89      0.91      0.90       345\n",
      "weighted avg       0.91      0.90      0.91       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# -----------------------------\n",
    "# Load DEiT Features from .npy files\n",
    "# -----------------------------\n",
    "X_train = np.load(\"ft_train_embeddings.npy\")  # shape: (num_train_samples, deit_feature_dim)\n",
    "X_val   = np.load(\"ft_val_embeddings.npy\")      # shape: (num_val_samples, deit_feature_dim)\n",
    "X_test  = np.load(\"ft_test_embeddings.npy\")     # shape: (num_test_samples, deit_feature_dim)\n",
    "\n",
    "# -----------------------------\n",
    "# Load CSV Files with Labels\n",
    "# -----------------------------\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "# Extract label arrays (assuming the column is named \"Group\")\n",
    "y_train = train_data[\"Group\"].values\n",
    "y_val   = val_data[\"Group\"].values\n",
    "y_test  = test_data[\"Group\"].values\n",
    "\n",
    "# -----------------------------\n",
    "# Encode Labels to Integers\n",
    "# -----------------------------\n",
    "# Assuming the groups are \"CN\", \"MCI\", and \"AD\"\n",
    "label_map = {\"CN\": 0, \"MCI\": 1, \"AD\": 2}\n",
    "y_train = np.array([label_map[label] for label in y_train])\n",
    "y_val   = np.array([label_map[label] for label in y_val])\n",
    "y_test  = np.array([label_map[label] for label in y_test])\n",
    "\n",
    "# -----------------------------\n",
    "# Train and Evaluate XGBoost Classifier\n",
    "# -----------------------------\n",
    "print(\"\\n--- XGBoost Classifier ---\")\n",
    "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "xgb_train_preds = xgb_clf.predict(X_train)\n",
    "xgb_val_preds = xgb_clf.predict(X_val)\n",
    "xgb_test_preds = xgb_clf.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"XGBoost Train Accuracy:\", accuracy_score(y_train, xgb_train_preds))\n",
    "print(\"XGBoost Validation Accuracy:\", accuracy_score(y_val, xgb_val_preds))\n",
    "print(\"XGBoost Test Accuracy:\", accuracy_score(y_test, xgb_test_preds))\n",
    "print(\"XGBoost Classification Report on Test Data:\")\n",
    "print(classification_report(y_test, xgb_test_preds))\n",
    "\n",
    "# -----------------------------\n",
    "# Train and Evaluate LightGBM Classifier\n",
    "# -----------------------------\n",
    "print(\"\\n--- LightGBM Classifier ---\")\n",
    "lgbm_clf = LGBMClassifier(random_state=42)\n",
    "lgbm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "lgbm_train_preds = lgbm_clf.predict(X_train)\n",
    "lgbm_val_preds = lgbm_clf.predict(X_val)\n",
    "lgbm_test_preds = lgbm_clf.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"LightGBM Train Accuracy:\", accuracy_score(y_train, lgbm_train_preds))\n",
    "print(\"LightGBM Validation Accuracy:\", accuracy_score(y_val, lgbm_val_preds))\n",
    "print(\"LightGBM Test Accuracy:\", accuracy_score(y_test, lgbm_test_preds))\n",
    "print(\"LightGBM Classification Report on Test Data:\")\n",
    "print(classification_report(y_test, lgbm_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a31865a-52ae-4aff-8274-2ff98c6827ec",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f92a9a9d-b971-4981-adfc-4ccacfbc96fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.9670822\ttest: 0.9762775\tbest: 0.9762775 (0)\ttotal: 16.7ms\tremaining: 8.35s\n",
      "100:\tlearn: 0.2266326\ttest: 0.3074760\tbest: 0.3074760 (100)\ttotal: 1.54s\tremaining: 6.08s\n",
      "200:\tlearn: 0.1772582\ttest: 0.2902054\tbest: 0.2902054 (200)\ttotal: 3.05s\tremaining: 4.54s\n",
      "300:\tlearn: 0.1487750\ttest: 0.2807472\tbest: 0.2803094 (277)\ttotal: 4.57s\tremaining: 3.02s\n",
      "400:\tlearn: 0.1265543\ttest: 0.2759443\tbest: 0.2759443 (400)\ttotal: 6.08s\tremaining: 1.5s\n",
      "499:\tlearn: 0.1078259\ttest: 0.2712964\tbest: 0.2711617 (497)\ttotal: 7.59s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.2711617449\n",
      "bestIteration = 497\n",
      "\n",
      "Shrink model to first 498 iterations.\n",
      "CatBoost Train Accuracy: 0.967601246105919\n",
      "CatBoost Validation Accuracy: 0.8866279069767442\n",
      "CatBoost Test Accuracy: 0.9159420289855073\n",
      "\n",
      "CatBoost Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97       106\n",
      "           1       0.91      0.92      0.91       167\n",
      "           2       0.84      0.86      0.85        72\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.91      0.91      0.91       345\n",
      "weighted avg       0.92      0.92      0.92       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# -----------------------------\n",
    "# Load DEiT Features from .npy files\n",
    "# -----------------------------\n",
    "X_train = np.load(\"ft_train_embeddings.npy\")  # shape (num_train_samples, deit_feature_dim)\n",
    "X_val   = np.load(\"ft_val_embeddings.npy\")      # shape (num_val_samples, deit_feature_dim)\n",
    "X_test  = np.load(\"ft_test_embeddings.npy\")     # shape (num_test_samples, deit_feature_dim)\n",
    "\n",
    "# -----------------------------\n",
    "# Load CSV Files with Labels\n",
    "# -----------------------------\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "# Extract label arrays (assuming the column is named \"Group\")\n",
    "y_train = train_data[\"Group\"].values\n",
    "y_val   = val_data[\"Group\"].values\n",
    "y_test  = test_data[\"Group\"].values\n",
    "\n",
    "# -----------------------------\n",
    "# Encode Labels to Integers\n",
    "# -----------------------------\n",
    "# Assuming the groups are \"CN\", \"MCI\", and \"AD\"\n",
    "label_map = {\"CN\": 0, \"MCI\": 1, \"AD\": 2}\n",
    "y_train = np.array([label_map[label] for label in y_train])\n",
    "y_val   = np.array([label_map[label] for label in y_val])\n",
    "y_test  = np.array([label_map[label] for label in y_test])\n",
    "\n",
    "# -----------------------------\n",
    "# Train and Evaluate CatBoost Classifier\n",
    "# -----------------------------\n",
    "catboost_clf = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    loss_function='MultiClass',\n",
    "    verbose=100,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# Train the classifier, using the validation set for early stopping\n",
    "catboost_clf.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)\n",
    "\n",
    "# Predictions on each split\n",
    "train_preds = catboost_clf.predict(X_train)\n",
    "val_preds = catboost_clf.predict(X_val)\n",
    "test_preds = catboost_clf.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"CatBoost Train Accuracy:\", accuracy_score(y_train, train_preds))\n",
    "print(\"CatBoost Validation Accuracy:\", accuracy_score(y_val, val_preds))\n",
    "print(\"CatBoost Test Accuracy:\", accuracy_score(y_test, test_preds))\n",
    "print(\"\\nCatBoost Classification Report on Test Data:\")\n",
    "print(classification_report(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113bc88e-9839-41c9-9c9e-1e00500133cf",
   "metadata": {},
   "source": [
    "# Multi Modal Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f780dcae-2b75-4c5d-8d68-0343d9bb3646",
   "metadata": {},
   "source": [
    "## Early Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b2b3bc65-1128-4630-a039-ea402ae83912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused train features shape: (1605, 960)\n",
      "Fused val features shape: (344, 960)\n",
      "Fused test features shape: (345, 960)\n",
      "Epoch 1/50: Train Loss = 1.0472, Val Loss = 0.9846, Val Acc = 0.4855\n",
      "  -> New best model saved.\n",
      "Epoch 2/50: Train Loss = 0.9425, Val Loss = 0.8698, Val Acc = 0.6279\n",
      "  -> New best model saved.\n",
      "Epoch 3/50: Train Loss = 0.8291, Val Loss = 0.7359, Val Acc = 0.7645\n",
      "  -> New best model saved.\n",
      "Epoch 4/50: Train Loss = 0.7063, Val Loss = 0.6146, Val Acc = 0.7733\n",
      "  -> New best model saved.\n",
      "Epoch 5/50: Train Loss = 0.5950, Val Loss = 0.5254, Val Acc = 0.8314\n",
      "  -> New best model saved.\n",
      "Epoch 6/50: Train Loss = 0.5222, Val Loss = 0.4624, Val Acc = 0.8605\n",
      "  -> New best model saved.\n",
      "Epoch 7/50: Train Loss = 0.4604, Val Loss = 0.4165, Val Acc = 0.8663\n",
      "  -> New best model saved.\n",
      "Epoch 8/50: Train Loss = 0.4227, Val Loss = 0.3843, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 9/50: Train Loss = 0.3947, Val Loss = 0.3603, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 10/50: Train Loss = 0.3667, Val Loss = 0.3440, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 11/50: Train Loss = 0.3486, Val Loss = 0.3343, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 12/50: Train Loss = 0.3318, Val Loss = 0.3250, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 13/50: Train Loss = 0.3173, Val Loss = 0.3186, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 14/50: Train Loss = 0.3244, Val Loss = 0.3150, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 15/50: Train Loss = 0.3047, Val Loss = 0.3115, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 16/50: Train Loss = 0.3046, Val Loss = 0.3081, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 17/50: Train Loss = 0.3018, Val Loss = 0.3065, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 18/50: Train Loss = 0.2918, Val Loss = 0.3051, Val Acc = 0.8866\n",
      "  -> New best model saved.\n",
      "Epoch 19/50: Train Loss = 0.2966, Val Loss = 0.3103, Val Acc = 0.8721\n",
      "Epoch 20/50: Train Loss = 0.2973, Val Loss = 0.3047, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 21/50: Train Loss = 0.2902, Val Loss = 0.3023, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 22/50: Train Loss = 0.2717, Val Loss = 0.3026, Val Acc = 0.8808\n",
      "Epoch 23/50: Train Loss = 0.2813, Val Loss = 0.3020, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 24/50: Train Loss = 0.2794, Val Loss = 0.3015, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 25/50: Train Loss = 0.2794, Val Loss = 0.3004, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 26/50: Train Loss = 0.2719, Val Loss = 0.3033, Val Acc = 0.8692\n",
      "Epoch 27/50: Train Loss = 0.2809, Val Loss = 0.2997, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 28/50: Train Loss = 0.2749, Val Loss = 0.3003, Val Acc = 0.8808\n",
      "Epoch 29/50: Train Loss = 0.2688, Val Loss = 0.2999, Val Acc = 0.8808\n",
      "Epoch 30/50: Train Loss = 0.2700, Val Loss = 0.2987, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 31/50: Train Loss = 0.2741, Val Loss = 0.2981, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 32/50: Train Loss = 0.2667, Val Loss = 0.2981, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 33/50: Train Loss = 0.2697, Val Loss = 0.2975, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 34/50: Train Loss = 0.2629, Val Loss = 0.2976, Val Acc = 0.8808\n",
      "Epoch 35/50: Train Loss = 0.2635, Val Loss = 0.2993, Val Acc = 0.8750\n",
      "Epoch 36/50: Train Loss = 0.2630, Val Loss = 0.2965, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 37/50: Train Loss = 0.2561, Val Loss = 0.2960, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 38/50: Train Loss = 0.2679, Val Loss = 0.2966, Val Acc = 0.8779\n",
      "Epoch 39/50: Train Loss = 0.2579, Val Loss = 0.2964, Val Acc = 0.8779\n",
      "Epoch 40/50: Train Loss = 0.2564, Val Loss = 0.2993, Val Acc = 0.8692\n",
      "Epoch 41/50: Train Loss = 0.2514, Val Loss = 0.2969, Val Acc = 0.8750\n",
      "Epoch 42/50: Train Loss = 0.2608, Val Loss = 0.3010, Val Acc = 0.8634\n",
      "Epoch 43/50: Train Loss = 0.2521, Val Loss = 0.2940, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 44/50: Train Loss = 0.2518, Val Loss = 0.2936, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 45/50: Train Loss = 0.2492, Val Loss = 0.2936, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 46/50: Train Loss = 0.2447, Val Loss = 0.2932, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 47/50: Train Loss = 0.2529, Val Loss = 0.2943, Val Acc = 0.8750\n",
      "Epoch 48/50: Train Loss = 0.2456, Val Loss = 0.2945, Val Acc = 0.8721\n",
      "Epoch 49/50: Train Loss = 0.2539, Val Loss = 0.2923, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 50/50: Train Loss = 0.2455, Val Loss = 0.2931, Val Acc = 0.8750\n",
      "Test Accuracy: 0.9188405797101449\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.85      0.86      0.86        72\n",
      "          CN       0.98      0.95      0.97       106\n",
      "         MCI       0.91      0.92      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.91      0.91      0.91       345\n",
      "weighted avg       0.92      0.92      0.92       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Concatenate Features\n",
    "##########################################\n",
    "# Load DeiT features.\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")  # shape: (num_train_samples, deit_feature_dim)\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")      # shape: (num_val_samples,   deit_feature_dim)\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")     # shape: (num_test_samples,  deit_feature_dim)\n",
    "\n",
    "# Load FTTransformer features.\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")        # shape: (num_train_samples, ft_feature_dim)\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")          # shape: (num_val_samples,   ft_feature_dim)\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")         # shape: (num_test_samples,  ft_feature_dim)\n",
    "\n",
    "# Concatenate features along the last axis.\n",
    "train_features = np.concatenate((deit_train, ft_train), axis=1)\n",
    "val_features   = np.concatenate((deit_val, ft_val), axis=1)\n",
    "test_features  = np.concatenate((deit_test, ft_test), axis=1)\n",
    "\n",
    "print(\"Fused train features shape:\", train_features.shape)\n",
    "print(\"Fused val features shape:\", val_features.shape)\n",
    "print(\"Fused test features shape:\", test_features.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "# Read the CSV files (adjust filenames if needed)\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "# We assume the label column is named \"Group\"\n",
    "label_col = \"Group\"\n",
    "\n",
    "# Encode labels using LabelEncoder.\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create PyTorch Datasets and DataLoaders\n",
    "##########################################\n",
    "# Convert features and labels to tensors.\n",
    "train_features = torch.tensor(train_features, dtype=torch.float32)\n",
    "val_features   = torch.tensor(val_features, dtype=torch.float32)\n",
    "test_features  = torch.tensor(test_features, dtype=torch.float32)\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels   = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_labels  = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets.\n",
    "train_dataset = TensorDataset(train_features, train_labels)\n",
    "val_dataset   = TensorDataset(val_features, val_labels)\n",
    "test_dataset  = TensorDataset(test_features, test_labels)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define an MLP Classifier for Fused Features\n",
    "##########################################\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "input_dim = train_features.shape[1]\n",
    "hidden_dim = 128  # Adjust as needed.\n",
    "model_mlp = MLPClassifier(input_dim, hidden_dim, num_classes)\n",
    "model_mlp.to(device)\n",
    "\n",
    "##########################################\n",
    "# 5. Train the MLP Classifier with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_mlp.parameters(), lr=1e-5)\n",
    "num_epochs = 50\n",
    "patience = 10  # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_mlp.train()\n",
    "    train_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_mlp(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Evaluate on the validation set.\n",
    "    model_mlp.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model_mlp(x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    val_acc = accuracy_score(all_targets, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping check.\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model_mlp.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model state.\n",
    "model_mlp.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model_mlp.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model_mlp(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2c0521bb-aa5e-4e4f-8a1a-a2591deb9dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused train features shape: (1605, 960)\n",
      "Fused val features shape: (344, 960)\n",
      "Fused test features shape: (345, 960)\n",
      "Epoch 1/100: Train Loss = 0.9764, Val Loss = 0.8233, Val Acc = 0.7733\n",
      "  -> New best model saved.\n",
      "Epoch 2/100: Train Loss = 0.7357, Val Loss = 0.5864, Val Acc = 0.7733\n",
      "  -> New best model saved.\n",
      "Epoch 3/100: Train Loss = 0.5443, Val Loss = 0.4498, Val Acc = 0.8314\n",
      "  -> New best model saved.\n",
      "Epoch 4/100: Train Loss = 0.4433, Val Loss = 0.3829, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 5/100: Train Loss = 0.3801, Val Loss = 0.3480, Val Acc = 0.8634\n",
      "  -> New best model saved.\n",
      "Epoch 6/100: Train Loss = 0.3470, Val Loss = 0.3316, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 7/100: Train Loss = 0.3223, Val Loss = 0.3188, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 8/100: Train Loss = 0.3078, Val Loss = 0.3144, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 9/100: Train Loss = 0.2966, Val Loss = 0.3118, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 10/100: Train Loss = 0.2957, Val Loss = 0.3105, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 11/100: Train Loss = 0.2929, Val Loss = 0.3130, Val Acc = 0.8605\n",
      "Epoch 12/100: Train Loss = 0.2892, Val Loss = 0.3065, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 13/100: Train Loss = 0.2873, Val Loss = 0.3057, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 14/100: Train Loss = 0.2725, Val Loss = 0.3058, Val Acc = 0.8808\n",
      "Epoch 15/100: Train Loss = 0.2734, Val Loss = 0.3053, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 16/100: Train Loss = 0.2788, Val Loss = 0.3041, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 17/100: Train Loss = 0.2646, Val Loss = 0.3032, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 18/100: Train Loss = 0.2679, Val Loss = 0.3025, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 19/100: Train Loss = 0.2643, Val Loss = 0.3026, Val Acc = 0.8750\n",
      "Epoch 20/100: Train Loss = 0.2671, Val Loss = 0.3066, Val Acc = 0.8750\n",
      "Epoch 21/100: Train Loss = 0.2581, Val Loss = 0.3040, Val Acc = 0.8721\n",
      "Epoch 22/100: Train Loss = 0.2612, Val Loss = 0.3005, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 23/100: Train Loss = 0.2470, Val Loss = 0.3002, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 24/100: Train Loss = 0.2469, Val Loss = 0.3034, Val Acc = 0.8634\n",
      "Epoch 25/100: Train Loss = 0.2516, Val Loss = 0.3020, Val Acc = 0.8663\n",
      "Epoch 26/100: Train Loss = 0.2445, Val Loss = 0.2997, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 27/100: Train Loss = 0.2440, Val Loss = 0.3031, Val Acc = 0.8605\n",
      "Epoch 28/100: Train Loss = 0.2404, Val Loss = 0.2990, Val Acc = 0.8663\n",
      "  -> New best model saved.\n",
      "Epoch 29/100: Train Loss = 0.2369, Val Loss = 0.2961, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 30/100: Train Loss = 0.2419, Val Loss = 0.2970, Val Acc = 0.8750\n",
      "Epoch 31/100: Train Loss = 0.2382, Val Loss = 0.2966, Val Acc = 0.8750\n",
      "Epoch 32/100: Train Loss = 0.2361, Val Loss = 0.2969, Val Acc = 0.8721\n",
      "Epoch 33/100: Train Loss = 0.2333, Val Loss = 0.2943, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 34/100: Train Loss = 0.2304, Val Loss = 0.2949, Val Acc = 0.8721\n",
      "Epoch 35/100: Train Loss = 0.2296, Val Loss = 0.2934, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 36/100: Train Loss = 0.2295, Val Loss = 0.2922, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 37/100: Train Loss = 0.2255, Val Loss = 0.2913, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 38/100: Train Loss = 0.2287, Val Loss = 0.2906, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 39/100: Train Loss = 0.2133, Val Loss = 0.2910, Val Acc = 0.8779\n",
      "Epoch 40/100: Train Loss = 0.2271, Val Loss = 0.2903, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 41/100: Train Loss = 0.2173, Val Loss = 0.2915, Val Acc = 0.8779\n",
      "Epoch 42/100: Train Loss = 0.2163, Val Loss = 0.2977, Val Acc = 0.8779\n",
      "Epoch 43/100: Train Loss = 0.2113, Val Loss = 0.2933, Val Acc = 0.8808\n",
      "Epoch 44/100: Train Loss = 0.2068, Val Loss = 0.2939, Val Acc = 0.8779\n",
      "Epoch 45/100: Train Loss = 0.2134, Val Loss = 0.2896, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 46/100: Train Loss = 0.2017, Val Loss = 0.2917, Val Acc = 0.8837\n",
      "Epoch 47/100: Train Loss = 0.1973, Val Loss = 0.2901, Val Acc = 0.8837\n",
      "Epoch 48/100: Train Loss = 0.2038, Val Loss = 0.2903, Val Acc = 0.8837\n",
      "Epoch 49/100: Train Loss = 0.2023, Val Loss = 0.2899, Val Acc = 0.8866\n",
      "Epoch 50/100: Train Loss = 0.1984, Val Loss = 0.2903, Val Acc = 0.8837\n",
      "Epoch 51/100: Train Loss = 0.2015, Val Loss = 0.2875, Val Acc = 0.8895\n",
      "  -> New best model saved.\n",
      "Epoch 52/100: Train Loss = 0.1920, Val Loss = 0.2903, Val Acc = 0.8837\n",
      "Epoch 53/100: Train Loss = 0.1908, Val Loss = 0.2889, Val Acc = 0.8837\n",
      "Epoch 54/100: Train Loss = 0.1904, Val Loss = 0.2872, Val Acc = 0.8866\n",
      "  -> New best model saved.\n",
      "Epoch 55/100: Train Loss = 0.1860, Val Loss = 0.2877, Val Acc = 0.8866\n",
      "Epoch 56/100: Train Loss = 0.1823, Val Loss = 0.2912, Val Acc = 0.8953\n",
      "Epoch 57/100: Train Loss = 0.1879, Val Loss = 0.2883, Val Acc = 0.8924\n",
      "Epoch 58/100: Train Loss = 0.1816, Val Loss = 0.2858, Val Acc = 0.8895\n",
      "  -> New best model saved.\n",
      "Epoch 59/100: Train Loss = 0.1804, Val Loss = 0.2853, Val Acc = 0.8895\n",
      "  -> New best model saved.\n",
      "Epoch 60/100: Train Loss = 0.1770, Val Loss = 0.2897, Val Acc = 0.8895\n",
      "Epoch 61/100: Train Loss = 0.1739, Val Loss = 0.2839, Val Acc = 0.8895\n",
      "  -> New best model saved.\n",
      "Epoch 62/100: Train Loss = 0.1694, Val Loss = 0.2872, Val Acc = 0.8895\n",
      "Epoch 63/100: Train Loss = 0.1735, Val Loss = 0.2858, Val Acc = 0.8895\n",
      "Epoch 64/100: Train Loss = 0.1687, Val Loss = 0.2875, Val Acc = 0.8924\n",
      "Epoch 65/100: Train Loss = 0.1621, Val Loss = 0.2841, Val Acc = 0.8895\n",
      "Epoch 66/100: Train Loss = 0.1599, Val Loss = 0.2831, Val Acc = 0.8924\n",
      "  -> New best model saved.\n",
      "Epoch 67/100: Train Loss = 0.1589, Val Loss = 0.2839, Val Acc = 0.8895\n",
      "Epoch 68/100: Train Loss = 0.1611, Val Loss = 0.2844, Val Acc = 0.8924\n",
      "Epoch 69/100: Train Loss = 0.1577, Val Loss = 0.2882, Val Acc = 0.8924\n",
      "Epoch 70/100: Train Loss = 0.1538, Val Loss = 0.3060, Val Acc = 0.8953\n",
      "Epoch 71/100: Train Loss = 0.1490, Val Loss = 0.2852, Val Acc = 0.8924\n",
      "Epoch 72/100: Train Loss = 0.1547, Val Loss = 0.2851, Val Acc = 0.8924\n",
      "Epoch 73/100: Train Loss = 0.1476, Val Loss = 0.2851, Val Acc = 0.8895\n",
      "Epoch 74/100: Train Loss = 0.1528, Val Loss = 0.2879, Val Acc = 0.8895\n",
      "Epoch 75/100: Train Loss = 0.1417, Val Loss = 0.2854, Val Acc = 0.8924\n",
      "Epoch 76/100: Train Loss = 0.1427, Val Loss = 0.2816, Val Acc = 0.8924\n",
      "  -> New best model saved.\n",
      "Epoch 77/100: Train Loss = 0.1432, Val Loss = 0.2839, Val Acc = 0.8953\n",
      "Epoch 78/100: Train Loss = 0.1360, Val Loss = 0.2825, Val Acc = 0.8924\n",
      "Epoch 79/100: Train Loss = 0.1309, Val Loss = 0.2827, Val Acc = 0.8953\n",
      "Epoch 80/100: Train Loss = 0.1360, Val Loss = 0.2860, Val Acc = 0.9012\n",
      "Epoch 81/100: Train Loss = 0.1281, Val Loss = 0.2825, Val Acc = 0.8924\n",
      "Epoch 82/100: Train Loss = 0.1205, Val Loss = 0.2818, Val Acc = 0.9070\n",
      "Epoch 83/100: Train Loss = 0.1282, Val Loss = 0.2829, Val Acc = 0.9070\n",
      "Epoch 84/100: Train Loss = 0.1229, Val Loss = 0.2814, Val Acc = 0.8953\n",
      "  -> New best model saved.\n",
      "Epoch 85/100: Train Loss = 0.1184, Val Loss = 0.2845, Val Acc = 0.8983\n",
      "Epoch 86/100: Train Loss = 0.1062, Val Loss = 0.2900, Val Acc = 0.8983\n",
      "Epoch 87/100: Train Loss = 0.1156, Val Loss = 0.2834, Val Acc = 0.9041\n",
      "Epoch 88/100: Train Loss = 0.1101, Val Loss = 0.2853, Val Acc = 0.9012\n",
      "Epoch 89/100: Train Loss = 0.1143, Val Loss = 0.2855, Val Acc = 0.9012\n",
      "Epoch 90/100: Train Loss = 0.1040, Val Loss = 0.2860, Val Acc = 0.8983\n",
      "Epoch 91/100: Train Loss = 0.1063, Val Loss = 0.2869, Val Acc = 0.9012\n",
      "Epoch 92/100: Train Loss = 0.1044, Val Loss = 0.2867, Val Acc = 0.9070\n",
      "Epoch 93/100: Train Loss = 0.0999, Val Loss = 0.2904, Val Acc = 0.9041\n",
      "Epoch 94/100: Train Loss = 0.0988, Val Loss = 0.2825, Val Acc = 0.9070\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 0.9072463768115943\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.82      0.82      0.82        72\n",
      "          CN       0.99      0.95      0.97       106\n",
      "         MCI       0.89      0.92      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.90      0.90      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Concatenate Features\n",
    "##########################################\n",
    "# Load DeiT features.\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")  # shape: (num_train_samples, deit_feature_dim)\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")      # shape: (num_val_samples,   deit_feature_dim)\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")     # shape: (num_test_samples,  deit_feature_dim)\n",
    "\n",
    "# Load FTTransformer features.\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")        # shape: (num_train_samples, ft_feature_dim)\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")          # shape: (num_val_samples,   ft_feature_dim)\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")         # shape: (num_test_samples,  ft_feature_dim)\n",
    "\n",
    "# Concatenate features along the last axis.\n",
    "train_features = np.concatenate((deit_train, ft_train), axis=1)\n",
    "val_features   = np.concatenate((deit_val, ft_val), axis=1)\n",
    "test_features  = np.concatenate((deit_test, ft_test), axis=1)\n",
    "\n",
    "print(\"Fused train features shape:\", train_features.shape)\n",
    "print(\"Fused val features shape:\", val_features.shape)\n",
    "print(\"Fused test features shape:\", test_features.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "# Read the CSV files (adjust filenames if needed)\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "# We assume the label column is named \"Group\"\n",
    "label_col = \"Group\"\n",
    "\n",
    "# Encode labels using LabelEncoder.\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create PyTorch Datasets and DataLoaders\n",
    "##########################################\n",
    "# Convert features and labels to tensors.\n",
    "train_features = torch.tensor(train_features, dtype=torch.float32)\n",
    "val_features   = torch.tensor(val_features, dtype=torch.float32)\n",
    "test_features  = torch.tensor(test_features, dtype=torch.float32)\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels   = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_labels  = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets.\n",
    "train_dataset = TensorDataset(train_features, train_labels)\n",
    "val_dataset   = TensorDataset(val_features, val_labels)\n",
    "test_dataset  = TensorDataset(test_features, test_labels)\n",
    "\n",
    "# Create DataLoaders.\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define an MLP Classifier for Fused Features\n",
    "##########################################\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "input_dim = train_features.shape[1]\n",
    "hidden_dim = 128  # Adjust as needed.\n",
    "model_mlp = MLPClassifier(input_dim, hidden_dim, num_classes)\n",
    "model_mlp.to(device)\n",
    "\n",
    "##########################################\n",
    "# 5. Train the MLP Classifier with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_mlp.parameters(), lr=2e-5)\n",
    "num_epochs = 100\n",
    "patience = 10  # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_mlp.train()\n",
    "    train_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_mlp(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Evaluate on the validation set.\n",
    "    model_mlp.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model_mlp(x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    val_acc = accuracy_score(all_targets, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping check.\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model_mlp.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model state.\n",
    "model_mlp.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model_mlp.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model_mlp(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "48cd4acc-6c36-480b-af91-9ddc02e9a88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused train features shape: (1605, 960)\n",
      "Fused val features shape: (344, 960)\n",
      "Fused test features shape: (345, 960)\n",
      "Classes: ['AD' 'CN' 'MCI']\n",
      "\n",
      "--- Training MLP classifier ---\n",
      "MLP Train Accuracy: 0.9558\n",
      "MLP Validation Accuracy: 0.8779\n",
      "MLP Test Accuracy: 0.8841\n",
      "MLP Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.88      0.78        72\n",
      "           1       0.99      0.96      0.98       106\n",
      "           2       0.92      0.84      0.88       167\n",
      "\n",
      "    accuracy                           0.88       345\n",
      "   macro avg       0.87      0.89      0.88       345\n",
      "weighted avg       0.90      0.88      0.89       345\n",
      "\n",
      "\n",
      "--- Training KNN classifier ---\n",
      "KNN Train Accuracy: 0.9439\n",
      "KNN Validation Accuracy: 0.8924\n",
      "KNN Test Accuracy: 0.9188\n",
      "KNN Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.79      0.84        72\n",
      "           1       0.98      0.95      0.97       106\n",
      "           2       0.89      0.95      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.92      0.90      0.91       345\n",
      "weighted avg       0.92      0.92      0.92       345\n",
      "\n",
      "\n",
      "--- Training Logistic Regression classifier ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy: 1.0000\n",
      "Logistic Regression Validation Accuracy: 0.9244\n",
      "Logistic Regression Test Accuracy: 0.8899\n",
      "Logistic Regression Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.79      0.78        72\n",
      "           1       0.99      0.96      0.98       106\n",
      "           2       0.89      0.89      0.89       167\n",
      "\n",
      "    accuracy                           0.89       345\n",
      "   macro avg       0.88      0.88      0.88       345\n",
      "weighted avg       0.89      0.89      0.89       345\n",
      "\n",
      "\n",
      "--- Training SVC classifier ---\n",
      "SVC Train Accuracy: 0.9090\n",
      "SVC Validation Accuracy: 0.8750\n",
      "SVC Test Accuracy: 0.9072\n",
      "SVC Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.83        72\n",
      "           1       0.98      0.95      0.97       106\n",
      "           2       0.89      0.92      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.90      0.90      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n",
      "\n",
      "--- Training Random Forest classifier ---\n",
      "Random Forest Train Accuracy: 1.0000\n",
      "Random Forest Validation Accuracy: 0.8983\n",
      "Random Forest Test Accuracy: 0.9217\n",
      "Random Forest Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.93      0.90      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.91      0.92      0.92       345\n",
      "weighted avg       0.92      0.92      0.92       345\n",
      "\n",
      "\n",
      "--- Training LightGBM classifier ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041228 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 228799\n",
      "[LightGBM] [Info] Number of data points in the train set: 1605, number of used features: 945\n",
      "[LightGBM] [Info] Start training from score -1.572737\n",
      "[LightGBM] [Info] Start training from score -1.180370\n",
      "[LightGBM] [Info] Start training from score -0.722868\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM Train Accuracy: 1.0000\n",
      "LightGBM Validation Accuracy: 0.8983\n",
      "LightGBM Test Accuracy: 0.9304\n",
      "LightGBM Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.88        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.93      0.93      0.93       167\n",
      "\n",
      "    accuracy                           0.93       345\n",
      "   macro avg       0.92      0.93      0.93       345\n",
      "weighted avg       0.93      0.93      0.93       345\n",
      "\n",
      "\n",
      "--- Training XGBoost classifier ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [23:07:37] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Train Accuracy: 1.0000\n",
      "XGBoost Validation Accuracy: 0.8983\n",
      "XGBoost Test Accuracy: 0.9188\n",
      "XGBoost Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.85        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.92      0.91      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.91      0.92      0.91       345\n",
      "weighted avg       0.92      0.92      0.92       345\n",
      "\n",
      "\n",
      "--- Training CatBoost classifier ---\n",
      "0:\tlearn: 0.9834117\ttest: 0.9914141\tbest: 0.9914141 (0)\ttotal: 85.6ms\tremaining: 42.7s\n",
      "100:\tlearn: 0.1710003\ttest: 0.2934273\tbest: 0.2934273 (100)\ttotal: 8.47s\tremaining: 33.5s\n",
      "200:\tlearn: 0.1073446\ttest: 0.2781848\tbest: 0.2779590 (196)\ttotal: 16.8s\tremaining: 25s\n",
      "300:\tlearn: 0.0747061\ttest: 0.2731891\tbest: 0.2724735 (277)\ttotal: 25.1s\tremaining: 16.6s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.2722486211\n",
      "bestIteration = 316\n",
      "\n",
      "Shrink model to first 317 iterations.\n",
      "CatBoost Train Accuracy: 0.9944\n",
      "CatBoost Validation Accuracy: 0.9041\n",
      "CatBoost Test Accuracy: 0.9188\n",
      "CatBoost Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.86        72\n",
      "           1       0.98      0.95      0.97       106\n",
      "           2       0.91      0.92      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.91      0.91      0.91       345\n",
      "weighted avg       0.92      0.92      0.92       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Sklearn classifiers.\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# XGBoost, LightGBM, and CatBoost.\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "##########################################\n",
    "# 1. Load and Fuse Features\n",
    "##########################################\n",
    "# Load DeiT features.\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")  # shape: (num_train_samples, deit_feature_dim)\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")      # shape: (num_val_samples,   deit_feature_dim)\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")     # shape: (num_test_samples,  deit_feature_dim)\n",
    "\n",
    "# Load FTTransformer features.\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")        # shape: (num_train_samples, ft_feature_dim)\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")          # shape: (num_val_samples,   ft_feature_dim)\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")         # shape: (num_test_samples,  ft_feature_dim)\n",
    "\n",
    "# Concatenate features along the last axis.\n",
    "train_features = np.concatenate((deit_train, ft_train), axis=1)\n",
    "val_features   = np.concatenate((deit_val, ft_val), axis=1)\n",
    "test_features  = np.concatenate((deit_test, ft_test), axis=1)\n",
    "\n",
    "print(\"Fused train features shape:\", train_features.shape)\n",
    "print(\"Fused val features shape:\", val_features.shape)\n",
    "print(\"Fused test features shape:\", test_features.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load and Encode Labels\n",
    "##########################################\n",
    "# Read the CSV files (adjust filenames if needed)\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "# We assume the label column is named \"Group\"\n",
    "label_col = \"Group\"\n",
    "\n",
    "# Encode labels using LabelEncoder.\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Define and Train Multiple Classifiers\n",
    "##########################################\n",
    "# Define a dictionary of classifiers.\n",
    "classifiers = {\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"SVC\": SVC(kernel=\"rbf\", probability=True, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(\n",
    "                    iterations=500,\n",
    "                    learning_rate=0.1,\n",
    "                    depth=6,\n",
    "                    loss_function='MultiClass',\n",
    "                    verbose=100,\n",
    "                    random_seed=42\n",
    "                )\n",
    "}\n",
    "\n",
    "# Loop over each classifier, train and evaluate.\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\n--- Training {name} classifier ---\")\n",
    "    # For CatBoost, use early stopping with the validation set.\n",
    "    if name == \"CatBoost\":\n",
    "        clf.fit(train_features, train_labels, eval_set=(val_features, val_labels), early_stopping_rounds=50)\n",
    "    else:\n",
    "        clf.fit(train_features, train_labels)\n",
    "    \n",
    "    # Make predictions on each split.\n",
    "    train_preds = clf.predict(train_features)\n",
    "    val_preds   = clf.predict(val_features)\n",
    "    test_preds  = clf.predict(test_features)\n",
    "    \n",
    "    # Compute accuracy.\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    test_acc = accuracy_score(test_labels, test_preds)\n",
    "    \n",
    "    print(f\"{name} Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"{name} Validation Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"{name} Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"{name} Classification Report on Test Data:\")\n",
    "    print(classification_report(test_labels, test_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c81452-cd10-4619-ad54-01fefb10637d",
   "metadata": {},
   "source": [
    "## Mid Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "447ce1da-a43d-4e66-81d0-213c388257e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Epoch 1/100: Train Loss = 0.4006, Val Loss = 0.3472, Val Acc = 0.8430\n",
      "  -> New best model saved.\n",
      "Epoch 2/100: Train Loss = 0.3441, Val Loss = 0.3209, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 3/100: Train Loss = 0.3207, Val Loss = 0.3355, Val Acc = 0.8430\n",
      "Epoch 4/100: Train Loss = 0.2984, Val Loss = 0.3158, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 5/100: Train Loss = 0.2852, Val Loss = 0.3048, Val Acc = 0.8576\n",
      "  -> New best model saved.\n",
      "Epoch 6/100: Train Loss = 0.2799, Val Loss = 0.3960, Val Acc = 0.8169\n",
      "Epoch 7/100: Train Loss = 0.2834, Val Loss = 0.3204, Val Acc = 0.8634\n",
      "Epoch 8/100: Train Loss = 0.2637, Val Loss = 0.4094, Val Acc = 0.8634\n",
      "Epoch 9/100: Train Loss = 0.2523, Val Loss = 0.3368, Val Acc = 0.8779\n",
      "Epoch 10/100: Train Loss = 0.2394, Val Loss = 0.3078, Val Acc = 0.8634\n",
      "Epoch 11/100: Train Loss = 0.2314, Val Loss = 0.3332, Val Acc = 0.8779\n",
      "Epoch 12/100: Train Loss = 0.2239, Val Loss = 0.3180, Val Acc = 0.8895\n",
      "Epoch 13/100: Train Loss = 0.2258, Val Loss = 0.3203, Val Acc = 0.8895\n",
      "Epoch 14/100: Train Loss = 0.2015, Val Loss = 0.3758, Val Acc = 0.8547\n",
      "Epoch 15/100: Train Loss = 0.1840, Val Loss = 0.3631, Val Acc = 0.8895\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 0.9043478260869565\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.84      0.78      0.81        72\n",
      "          CN       0.98      0.96      0.97       106\n",
      "         MCI       0.89      0.92      0.90       167\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.90      0.89      0.89       345\n",
      "weighted avg       0.90      0.90      0.90       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load Features (Uni-modal for each modality)\n",
    "##########################################\n",
    "\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")  # shape: (num_train_samples, deit_feature_dim)\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")      # shape: (num_val_samples,   deit_feature_dim)\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")     \n",
    "\n",
    "# Load FTTransformer features.\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")        # shape: (num_train_samples, 192)\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")          # shape: (num_val_samples, 192)\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")         # shape: (num_test_samples, 192)\n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "# Read the CSV files (adjust filenames if needed)\n",
    "train_data = pd.read_csv(\"train_data_4.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_4.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_4.csv\")\n",
    "\n",
    "# We assume the label column is named \"Group\"\n",
    "label_col = \"Group\"\n",
    "\n",
    "# Encode labels using LabelEncoder.\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create a Custom Dataset for Multi-Modal Data\n",
    "##########################################\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, deit_features, ft_features, labels):\n",
    "        self.deit_features = torch.tensor(deit_features, dtype=torch.float32)\n",
    "        self.ft_features = torch.tensor(ft_features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.deit_features[idx], self.ft_features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = MultiModalDataset(deit_train, ft_train, train_labels)\n",
    "val_dataset   = MultiModalDataset(deit_val, ft_val, val_labels)\n",
    "test_dataset  = MultiModalDataset(deit_test, ft_test, test_labels)\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define the Mid-Fusion Classifier\n",
    "##########################################\n",
    "# Set projection dimensions.\n",
    "proj_dim_deit = 128  # Project DeiT features from 768 to 384.\n",
    "proj_dim_ft = 192    # Project FTTransformer features from 192 to 192.\n",
    "hidden_dim = 128     # Hidden dimension for the classifier.\n",
    "\n",
    "class MidFusionClassifier(nn.Module):\n",
    "    def __init__(self, deit_dim, ft_dim, proj_dim_deit, proj_dim_ft, hidden_dim, num_classes):\n",
    "        super(MidFusionClassifier, self).__init__()\n",
    "        # Project DeiT features from deit_dim (768) to proj_dim_deit (384).\n",
    "        self.deit_proj = nn.Linear(deit_dim, proj_dim_deit)\n",
    "        # Project FTTransformer features from ft_dim (192) to proj_dim_ft (192).\n",
    "        self.ft_proj   = nn.Linear(ft_dim, proj_dim_ft)\n",
    "        # Fuse by concatenation. Fused dimension = proj_dim_deit + proj_dim_ft = 384 + 192 = 576.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(proj_dim_deit + proj_dim_ft, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    def forward(self, deit, ft):\n",
    "        deit_proj = self.deit_proj(deit)  # (batch_size, proj_dim_deit)\n",
    "        ft_proj = self.ft_proj(ft)          # (batch_size, proj_dim_ft)\n",
    "        fused = torch.cat((deit_proj, ft_proj), dim=1)  # (batch_size, proj_dim_deit + proj_dim_ft)\n",
    "        out = self.classifier(fused)\n",
    "        return out\n",
    "\n",
    "# Hyperparameters for input dimensions.\n",
    "deit_dim = deit_train.shape[1]  # expected 768\n",
    "ft_dim = ft_train.shape[1]      # expected 192\n",
    "\n",
    "model = MidFusionClassifier(deit_dim, ft_dim, proj_dim_deit, proj_dim_ft, hidden_dim, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "##########################################\n",
    "# 5. Train the Mid-Fusion Classifier with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 100\n",
    "patience = 10  # early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for deit_feat, ft_feat, labels in train_loader:\n",
    "        deit_feat, ft_feat, labels = deit_feat.to(device), ft_feat.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(deit_feat, ft_feat)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * labels.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Evaluate on validation set.\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for deit_feat, ft_feat, labels in val_loader:\n",
    "            deit_feat, ft_feat, labels = deit_feat.to(device), ft_feat.to(device), labels.to(device)\n",
    "            logits = model(deit_feat, ft_feat)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item() * labels.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(labels.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    val_acc = accuracy_score(all_targets, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model state.\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for deit_feat, ft_feat, labels in test_loader:\n",
    "        deit_feat, ft_feat, labels = deit_feat.to(device), ft_feat.to(device), labels.to(device)\n",
    "        logits = model(deit_feat, ft_feat)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(labels.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3dcab52-de39-46b7-81d1-0e6f09eb4864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Classes: ['AD' 'CN' 'MCI']\n",
      "Epoch 1/100: Train Loss = 0.4694, Val Loss = 0.3851, Val Acc = 0.8401\n",
      "  -> New best model saved.\n",
      "Epoch 2/100: Train Loss = 0.4004, Val Loss = 0.3380, Val Acc = 0.8663\n",
      "  -> New best model saved.\n",
      "Epoch 3/100: Train Loss = 0.3940, Val Loss = 0.3018, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 4/100: Train Loss = 0.3384, Val Loss = 0.3262, Val Acc = 0.8750\n",
      "Epoch 5/100: Train Loss = 0.3535, Val Loss = 0.3508, Val Acc = 0.8692\n",
      "Epoch 6/100: Train Loss = 0.2880, Val Loss = 0.3011, Val Acc = 0.9070\n",
      "  -> New best model saved.\n",
      "Epoch 7/100: Train Loss = 0.2687, Val Loss = 0.2866, Val Acc = 0.9070\n",
      "  -> New best model saved.\n",
      "Epoch 8/100: Train Loss = 0.2553, Val Loss = 0.3058, Val Acc = 0.8983\n",
      "Epoch 9/100: Train Loss = 0.2274, Val Loss = 0.3462, Val Acc = 0.8837\n",
      "Epoch 10/100: Train Loss = 0.2236, Val Loss = 0.3464, Val Acc = 0.9012\n",
      "Epoch 11/100: Train Loss = 0.1986, Val Loss = 0.2646, Val Acc = 0.9041\n",
      "  -> New best model saved.\n",
      "Epoch 12/100: Train Loss = 0.1614, Val Loss = 0.3886, Val Acc = 0.8547\n",
      "Epoch 13/100: Train Loss = 0.1717, Val Loss = 0.3684, Val Acc = 0.8924\n",
      "Epoch 14/100: Train Loss = 0.1668, Val Loss = 0.4393, Val Acc = 0.8924\n",
      "Epoch 15/100: Train Loss = 0.2021, Val Loss = 0.4081, Val Acc = 0.8779\n",
      "Epoch 16/100: Train Loss = 0.1679, Val Loss = 0.3320, Val Acc = 0.8895\n",
      "Epoch 17/100: Train Loss = 0.1277, Val Loss = 0.3617, Val Acc = 0.8895\n",
      "Epoch 18/100: Train Loss = 0.1295, Val Loss = 0.3657, Val Acc = 0.9099\n",
      "Epoch 19/100: Train Loss = 0.1297, Val Loss = 0.3372, Val Acc = 0.9041\n",
      "Epoch 20/100: Train Loss = 0.1085, Val Loss = 0.6046, Val Acc = 0.8866\n",
      "Epoch 21/100: Train Loss = 0.1364, Val Loss = 0.3896, Val Acc = 0.8983\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 0.9130434782608695\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.86      0.78      0.82        72\n",
      "          CN       0.99      0.96      0.98       106\n",
      "         MCI       0.89      0.94      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.91      0.89      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load Features (Uni-modal for each modality)\n",
    "##########################################\n",
    "# Load DeiT features.\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")  # shape: (num_train_samples, 768)\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")      # shape: (num_val_samples,   768)\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")     # shape: (num_test_samples,  768)\n",
    "\n",
    "# Load FTTransformer features.\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")        # shape: (num_train_samples, 192)\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")          # shape: (num_val_samples,   192)\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")         # shape: (num_test_samples,  192)\n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "train_data = pd.read_csv(\"train_data_4.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_4.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_4.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "\n",
    "# Encode labels using LabelEncoder.\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create a Custom Dataset for Multi-Modal Data\n",
    "##########################################\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, deit_features, ft_features, labels):\n",
    "        self.deit_features = torch.tensor(deit_features, dtype=torch.float32)\n",
    "        self.ft_features = torch.tensor(ft_features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.deit_features[idx], self.ft_features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = MultiModalDataset(deit_train, ft_train, train_labels)\n",
    "val_dataset   = MultiModalDataset(deit_val, ft_val, val_labels)\n",
    "test_dataset  = MultiModalDataset(deit_test, ft_test, test_labels)\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define the Modified Fusion Model\n",
    "##########################################\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModifiedFusionModel(nn.Module):\n",
    "    def __init__(self, ft_input_size=192, deit_input_size=768, num_classes=3):\n",
    "        super(ModifiedFusionModel, self).__init__()\n",
    "        \n",
    "        # FTTransformer branch: reduce from 192 to 170.\n",
    "        self.ft_fc = nn.Sequential(\n",
    "            nn.Linear(ft_input_size, 170),\n",
    "            nn.BatchNorm1d(170),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # DeiT branch: reduce from 768 to 460.\n",
    "        self.deit_fc = nn.Sequential(\n",
    "            nn.Linear(deit_input_size, 460),\n",
    "            nn.BatchNorm1d(460),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Fusion classifier: Concatenated dimension is 170 + 460 = 630.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(630, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, deit_features, ft_features):\n",
    "        # Process each modality.\n",
    "        deit_proj = self.deit_fc(deit_features)  # (batch_size, 460)\n",
    "        ft_proj = self.ft_fc(ft_features)          # (batch_size, 170)\n",
    "        \n",
    "        # Concatenate along the feature dimension.\n",
    "        fused_features = torch.cat([deit_proj, ft_proj], dim=1)  # (batch_size, 630)\n",
    "        \n",
    "        # Classify.\n",
    "        logits = self.classifier(fused_features)  # (batch_size, num_classes)\n",
    "        return logits\n",
    "\n",
    "# Initialize the model.\n",
    "# Note: deit_train.shape[1] should be 768 and ft_train.shape[1] should be 192.\n",
    "deit_dim = deit_train.shape[1]\n",
    "ft_dim = ft_train.shape[1]\n",
    "model = ModifiedFusionModel(ft_input_size=ft_dim, deit_input_size=deit_dim, num_classes=num_classes)\n",
    "model.to(device)\n",
    "\n",
    "##########################################\n",
    "# 5. Train the Modified Fusion Model with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-3)\n",
    "num_epochs = 100\n",
    "patience = 10  # Early stopping patience.\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for deit_feat, ft_feat, labels in train_loader:\n",
    "        deit_feat, ft_feat, labels = deit_feat.to(device), ft_feat.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(deit_feat, ft_feat)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * labels.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Evaluate on validation set.\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for deit_feat, ft_feat, labels in val_loader:\n",
    "            deit_feat, ft_feat, labels = deit_feat.to(device), ft_feat.to(device), labels.to(device)\n",
    "            logits = model(deit_feat, ft_feat)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item() * labels.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(labels.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    val_acc = accuracy_score(all_targets, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model state.\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for deit_feat, ft_feat, labels in test_loader:\n",
    "        deit_feat, ft_feat, labels = deit_feat.to(device), ft_feat.to(device), labels.to(device)\n",
    "        logits = model(deit_feat, ft_feat)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(labels.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2506c86b-525d-426e-b703-4052c902c9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Epoch 1/100: Train Loss = 1.0031, Val Loss = 0.9318, Val Acc = 0.6308\n",
      "  -> New best model saved.\n",
      "Epoch 2/100: Train Loss = 0.8732, Val Loss = 0.7910, Val Acc = 0.7703\n",
      "  -> New best model saved.\n",
      "Epoch 3/100: Train Loss = 0.7286, Val Loss = 0.6322, Val Acc = 0.7907\n",
      "  -> New best model saved.\n",
      "Epoch 4/100: Train Loss = 0.5894, Val Loss = 0.5010, Val Acc = 0.8343\n",
      "  -> New best model saved.\n",
      "Epoch 5/100: Train Loss = 0.4866, Val Loss = 0.4193, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 6/100: Train Loss = 0.4145, Val Loss = 0.3711, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 7/100: Train Loss = 0.3714, Val Loss = 0.3443, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 8/100: Train Loss = 0.3477, Val Loss = 0.3303, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 9/100: Train Loss = 0.3269, Val Loss = 0.3273, Val Acc = 0.8605\n",
      "  -> New best model saved.\n",
      "Epoch 10/100: Train Loss = 0.3169, Val Loss = 0.3198, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 11/100: Train Loss = 0.3073, Val Loss = 0.3183, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 12/100: Train Loss = 0.3067, Val Loss = 0.3210, Val Acc = 0.8605\n",
      "Epoch 13/100: Train Loss = 0.3031, Val Loss = 0.3153, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 14/100: Train Loss = 0.2973, Val Loss = 0.3150, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 15/100: Train Loss = 0.2943, Val Loss = 0.3171, Val Acc = 0.8692\n",
      "Epoch 16/100: Train Loss = 0.2890, Val Loss = 0.3147, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 17/100: Train Loss = 0.2878, Val Loss = 0.3119, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 18/100: Train Loss = 0.2874, Val Loss = 0.3149, Val Acc = 0.8663\n",
      "Epoch 19/100: Train Loss = 0.2888, Val Loss = 0.3133, Val Acc = 0.8721\n",
      "Epoch 20/100: Train Loss = 0.2838, Val Loss = 0.3116, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 21/100: Train Loss = 0.2859, Val Loss = 0.3100, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 22/100: Train Loss = 0.2788, Val Loss = 0.3122, Val Acc = 0.8634\n",
      "Epoch 23/100: Train Loss = 0.2811, Val Loss = 0.3091, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 24/100: Train Loss = 0.2760, Val Loss = 0.3070, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 25/100: Train Loss = 0.2770, Val Loss = 0.3057, Val Acc = 0.8808\n",
      "  -> New best model saved.\n",
      "Epoch 26/100: Train Loss = 0.2790, Val Loss = 0.3065, Val Acc = 0.8779\n",
      "Epoch 27/100: Train Loss = 0.2689, Val Loss = 0.3046, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 28/100: Train Loss = 0.2676, Val Loss = 0.3087, Val Acc = 0.8634\n",
      "Epoch 29/100: Train Loss = 0.2607, Val Loss = 0.3030, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 30/100: Train Loss = 0.2657, Val Loss = 0.3056, Val Acc = 0.8721\n",
      "Epoch 31/100: Train Loss = 0.2624, Val Loss = 0.3044, Val Acc = 0.8692\n",
      "Epoch 32/100: Train Loss = 0.2613, Val Loss = 0.3015, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 33/100: Train Loss = 0.2666, Val Loss = 0.3003, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 34/100: Train Loss = 0.2617, Val Loss = 0.3005, Val Acc = 0.8779\n",
      "Epoch 35/100: Train Loss = 0.2589, Val Loss = 0.3018, Val Acc = 0.8721\n",
      "Epoch 36/100: Train Loss = 0.2601, Val Loss = 0.2980, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 37/100: Train Loss = 0.2518, Val Loss = 0.2985, Val Acc = 0.8750\n",
      "Epoch 38/100: Train Loss = 0.2515, Val Loss = 0.2970, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 39/100: Train Loss = 0.2516, Val Loss = 0.3007, Val Acc = 0.8692\n",
      "Epoch 40/100: Train Loss = 0.2525, Val Loss = 0.2964, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 41/100: Train Loss = 0.2463, Val Loss = 0.2955, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 42/100: Train Loss = 0.2488, Val Loss = 0.2989, Val Acc = 0.8721\n",
      "Epoch 43/100: Train Loss = 0.2425, Val Loss = 0.2957, Val Acc = 0.8750\n",
      "Epoch 44/100: Train Loss = 0.2438, Val Loss = 0.2972, Val Acc = 0.8750\n",
      "Epoch 45/100: Train Loss = 0.2470, Val Loss = 0.2940, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 46/100: Train Loss = 0.2399, Val Loss = 0.2988, Val Acc = 0.8721\n",
      "Epoch 47/100: Train Loss = 0.2405, Val Loss = 0.2937, Val Acc = 0.8750\n",
      "  -> New best model saved.\n",
      "Epoch 48/100: Train Loss = 0.2374, Val Loss = 0.2934, Val Acc = 0.8779\n",
      "  -> New best model saved.\n",
      "Epoch 49/100: Train Loss = 0.2290, Val Loss = 0.2944, Val Acc = 0.8750\n",
      "Epoch 50/100: Train Loss = 0.2287, Val Loss = 0.2954, Val Acc = 0.8779\n",
      "Epoch 51/100: Train Loss = 0.2294, Val Loss = 0.2950, Val Acc = 0.8779\n",
      "Epoch 52/100: Train Loss = 0.2353, Val Loss = 0.2960, Val Acc = 0.8779\n",
      "Epoch 53/100: Train Loss = 0.2267, Val Loss = 0.2939, Val Acc = 0.8837\n",
      "Epoch 54/100: Train Loss = 0.2256, Val Loss = 0.2912, Val Acc = 0.8837\n",
      "  -> New best model saved.\n",
      "Epoch 55/100: Train Loss = 0.2259, Val Loss = 0.2952, Val Acc = 0.8837\n",
      "Epoch 56/100: Train Loss = 0.2288, Val Loss = 0.2916, Val Acc = 0.8895\n",
      "Epoch 57/100: Train Loss = 0.2242, Val Loss = 0.2924, Val Acc = 0.8808\n",
      "Epoch 58/100: Train Loss = 0.2214, Val Loss = 0.2939, Val Acc = 0.8837\n",
      "Epoch 59/100: Train Loss = 0.2202, Val Loss = 0.3006, Val Acc = 0.8779\n",
      "Epoch 60/100: Train Loss = 0.2179, Val Loss = 0.2918, Val Acc = 0.8924\n",
      "Epoch 61/100: Train Loss = 0.2167, Val Loss = 0.2949, Val Acc = 0.8837\n",
      "Epoch 62/100: Train Loss = 0.2113, Val Loss = 0.2945, Val Acc = 0.8837\n",
      "Epoch 63/100: Train Loss = 0.2154, Val Loss = 0.2907, Val Acc = 0.8866\n",
      "  -> New best model saved.\n",
      "Epoch 64/100: Train Loss = 0.2148, Val Loss = 0.2905, Val Acc = 0.8866\n",
      "  -> New best model saved.\n",
      "Epoch 65/100: Train Loss = 0.2078, Val Loss = 0.2914, Val Acc = 0.8866\n",
      "Epoch 66/100: Train Loss = 0.2146, Val Loss = 0.2920, Val Acc = 0.8866\n",
      "Epoch 67/100: Train Loss = 0.2067, Val Loss = 0.2908, Val Acc = 0.8866\n",
      "Epoch 68/100: Train Loss = 0.2036, Val Loss = 0.2936, Val Acc = 0.8924\n",
      "Epoch 69/100: Train Loss = 0.2080, Val Loss = 0.2898, Val Acc = 0.8866\n",
      "  -> New best model saved.\n",
      "Epoch 70/100: Train Loss = 0.2060, Val Loss = 0.2892, Val Acc = 0.8866\n",
      "  -> New best model saved.\n",
      "Epoch 71/100: Train Loss = 0.1998, Val Loss = 0.2881, Val Acc = 0.8866\n",
      "  -> New best model saved.\n",
      "Epoch 72/100: Train Loss = 0.1967, Val Loss = 0.2920, Val Acc = 0.8924\n",
      "Epoch 73/100: Train Loss = 0.1964, Val Loss = 0.2922, Val Acc = 0.8953\n",
      "Epoch 74/100: Train Loss = 0.2003, Val Loss = 0.2885, Val Acc = 0.8866\n",
      "Epoch 75/100: Train Loss = 0.1978, Val Loss = 0.2896, Val Acc = 0.8924\n",
      "Epoch 76/100: Train Loss = 0.1875, Val Loss = 0.2900, Val Acc = 0.8866\n",
      "Epoch 77/100: Train Loss = 0.1925, Val Loss = 0.3001, Val Acc = 0.8808\n",
      "Epoch 78/100: Train Loss = 0.1972, Val Loss = 0.2894, Val Acc = 0.8895\n",
      "Epoch 79/100: Train Loss = 0.1933, Val Loss = 0.3051, Val Acc = 0.8721\n",
      "Epoch 80/100: Train Loss = 0.1957, Val Loss = 0.2888, Val Acc = 0.8866\n",
      "Epoch 81/100: Train Loss = 0.1854, Val Loss = 0.2929, Val Acc = 0.8953\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 0.8956521739130435\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.77      0.85      0.81        72\n",
      "          CN       0.98      0.95      0.97       106\n",
      "         MCI       0.90      0.88      0.89       167\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.88      0.89      0.89       345\n",
      "weighted avg       0.90      0.90      0.90       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load Features (Uni-modal for each modality)\n",
    "##########################################\n",
    "\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")  # shape: (num_train_samples, deit_feature_dim)\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")      # shape: (num_val_samples,   deit_feature_dim)\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")     \n",
    "\n",
    "# Load FTTransformer features.\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")        # shape: (num_train_samples, 192)\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")          # shape: (num_val_samples, 192)\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")         # shape: (num_test_samples, 192)\n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "# Read the CSV files (adjust filenames if needed)\n",
    "train_data = pd.read_csv(\"train_data_4.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_4.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_4.csv\")\n",
    "\n",
    "# We assume the label column is named \"Group\"\n",
    "label_col = \"Group\"\n",
    "\n",
    "# Encode labels using LabelEncoder.\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create a Custom Dataset for Multi-Modal Data\n",
    "##########################################\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, deit_features, ft_features, labels):\n",
    "        self.deit_features = torch.tensor(deit_features, dtype=torch.float32)\n",
    "        self.ft_features = torch.tensor(ft_features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.deit_features[idx], self.ft_features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = MultiModalDataset(deit_train, ft_train, train_labels)\n",
    "val_dataset   = MultiModalDataset(deit_val, ft_val, val_labels)\n",
    "test_dataset  = MultiModalDataset(deit_test, ft_test, test_labels)\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define the Mid-Fusion Classifier\n",
    "##########################################\n",
    "# Set projection dimensions.\n",
    "proj_dim_deit = 370  # Project DeiT features from 768 to 384.\n",
    "proj_dim_ft = 170    # Project FTTransformer features from 192 to 192.\n",
    "hidden_dim = 128     # Hidden dimension for the classifier.\n",
    "\n",
    "class MidFusionClassifier(nn.Module):\n",
    "    def __init__(self, deit_dim, ft_dim, proj_dim_deit, proj_dim_ft, hidden_dim, num_classes):\n",
    "        super(MidFusionClassifier, self).__init__()\n",
    "        # Project DeiT features from deit_dim (768) to proj_dim_deit (384).\n",
    "        self.deit_proj = nn.Linear(deit_dim, proj_dim_deit)\n",
    "        # Project FTTransformer features from ft_dim (192) to proj_dim_ft (192).\n",
    "        self.ft_proj   = nn.Linear(ft_dim, proj_dim_ft)\n",
    "        # Fuse by concatenation. Fused dimension = proj_dim_deit + proj_dim_ft = 384 + 192 = 576.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(proj_dim_deit + proj_dim_ft, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    def forward(self, deit, ft):\n",
    "        deit_proj = self.deit_proj(deit)  # (batch_size, proj_dim_deit)\n",
    "        ft_proj = self.ft_proj(ft)          # (batch_size, proj_dim_ft)\n",
    "        fused = torch.cat((deit_proj, ft_proj), dim=1)  # (batch_size, proj_dim_deit + proj_dim_ft)\n",
    "        out = self.classifier(fused)\n",
    "        return out\n",
    "\n",
    "# Hyperparameters for input dimensions.\n",
    "deit_dim = deit_train.shape[1]  # expected 768\n",
    "ft_dim = ft_train.shape[1]      # expected 192\n",
    "\n",
    "model = MidFusionClassifier(deit_dim, ft_dim, proj_dim_deit, proj_dim_ft, hidden_dim, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "##########################################\n",
    "# 5. Train the Mid-Fusion Classifier with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "num_epochs = 100\n",
    "patience = 10  # early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for deit_feat, ft_feat, labels in train_loader:\n",
    "        deit_feat, ft_feat, labels = deit_feat.to(device), ft_feat.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(deit_feat, ft_feat)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * labels.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Evaluate on validation set.\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for deit_feat, ft_feat, labels in val_loader:\n",
    "            deit_feat, ft_feat, labels = deit_feat.to(device), ft_feat.to(device), labels.to(device)\n",
    "            logits = model(deit_feat, ft_feat)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item() * labels.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(labels.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    val_acc = accuracy_score(all_targets, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model state.\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Evaluate on the Test Set\n",
    "##########################################\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for deit_feat, ft_feat, labels in test_loader:\n",
    "        deit_feat, ft_feat, labels = deit_feat.to(device), ft_feat.to(device), labels.to(device)\n",
    "        logits = model(deit_feat, ft_feat)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(labels.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[str(c) for c in label_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a98a5955-9b46-48e8-bf04-31fcf4506ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused train features shape: torch.Size([1605, 540])\n",
      "Fused val features shape: torch.Size([344, 540])\n",
      "Fused test features shape: torch.Size([345, 540])\n",
      "\n",
      "--- Training MLP classifier ---\n",
      "MLP Train Accuracy: 0.9526\n",
      "MLP Validation Accuracy: 0.8983\n",
      "MLP Test Accuracy: 0.8986\n",
      "MLP Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.86      0.81        72\n",
      "           1       0.99      0.96      0.98       106\n",
      "           2       0.91      0.87      0.89       167\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.89      0.90      0.89       345\n",
      "weighted avg       0.90      0.90      0.90       345\n",
      "\n",
      "\n",
      "--- Training KNN classifier ---\n",
      "KNN Train Accuracy: 0.9427\n",
      "KNN Validation Accuracy: 0.9041\n",
      "KNN Test Accuracy: 0.9188\n",
      "KNN Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.85        72\n",
      "           1       0.98      0.95      0.97       106\n",
      "           2       0.91      0.93      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.92      0.91      0.91       345\n",
      "weighted avg       0.92      0.92      0.92       345\n",
      "\n",
      "\n",
      "--- Training Logistic Regression classifier ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy: 0.9925\n",
      "Logistic Regression Validation Accuracy: 0.9244\n",
      "Logistic Regression Test Accuracy: 0.9014\n",
      "Logistic Regression Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.85      0.81        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.91      0.89      0.90       167\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.89      0.90      0.89       345\n",
      "weighted avg       0.90      0.90      0.90       345\n",
      "\n",
      "\n",
      "--- Training SVC classifier ---\n",
      "SVC Train Accuracy: 0.9321\n",
      "SVC Validation Accuracy: 0.8895\n",
      "SVC Test Accuracy: 0.9014\n",
      "SVC Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.83      0.82        72\n",
      "           1       0.98      0.95      0.97       106\n",
      "           2       0.90      0.90      0.90       167\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.89      0.89      0.89       345\n",
      "weighted avg       0.90      0.90      0.90       345\n",
      "\n",
      "\n",
      "--- Training Random Forest classifier ---\n",
      "Random Forest Train Accuracy: 1.0000\n",
      "Random Forest Validation Accuracy: 0.9070\n",
      "Random Forest Test Accuracy: 0.9304\n",
      "Random Forest Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.92      0.93      0.93       167\n",
      "\n",
      "    accuracy                           0.93       345\n",
      "   macro avg       0.93      0.92      0.92       345\n",
      "weighted avg       0.93      0.93      0.93       345\n",
      "\n",
      "\n",
      "--- Training LightGBM classifier ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023790 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 137700\n",
      "[LightGBM] [Info] Number of data points in the train set: 1605, number of used features: 540\n",
      "[LightGBM] [Info] Start training from score -1.572737\n",
      "[LightGBM] [Info] Start training from score -1.180370\n",
      "[LightGBM] [Info] Start training from score -0.722868\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM Train Accuracy: 1.0000\n",
      "LightGBM Validation Accuracy: 0.9070\n",
      "LightGBM Test Accuracy: 0.9043\n",
      "LightGBM Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81        72\n",
      "           1       0.99      0.96      0.98       106\n",
      "           2       0.90      0.90      0.90       167\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.89      0.90      0.90       345\n",
      "weighted avg       0.91      0.90      0.91       345\n",
      "\n",
      "\n",
      "--- Training XGBoost classifier ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [23:19:50] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Train Accuracy: 1.0000\n",
      "XGBoost Validation Accuracy: 0.9099\n",
      "XGBoost Test Accuracy: 0.9043\n",
      "XGBoost Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81        72\n",
      "           1       0.99      0.96      0.98       106\n",
      "           2       0.89      0.91      0.90       167\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.90      0.89      0.89       345\n",
      "weighted avg       0.91      0.90      0.90       345\n",
      "\n",
      "\n",
      "--- Training CatBoost classifier ---\n",
      "0:\tlearn: 0.9723272\ttest: 0.9814717\tbest: 0.9814717 (0)\ttotal: 47.1ms\tremaining: 23.5s\n",
      "100:\tlearn: 0.1279626\ttest: 0.2789423\tbest: 0.2783845 (93)\ttotal: 4.66s\tremaining: 18.4s\n",
      "200:\tlearn: 0.0794834\ttest: 0.2713609\tbest: 0.2697940 (171)\ttotal: 9.27s\tremaining: 13.8s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.2696699329\n",
      "bestIteration = 234\n",
      "\n",
      "Shrink model to first 235 iterations.\n",
      "CatBoost Train Accuracy: 0.9900\n",
      "CatBoost Validation Accuracy: 0.9157\n",
      "CatBoost Test Accuracy: 0.9130\n",
      "CatBoost Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.84        72\n",
      "           1       0.98      0.95      0.97       106\n",
      "           2       0.91      0.92      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.91      0.91      0.91       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming the device and your MultiModalDataset, DataLoader, and MidFusionClassifier are already defined.\n",
    "# If not, please refer to your previous code snippet.\n",
    "\n",
    "##########################################\n",
    "# 1. Extract Fused Features from the MidFusionClassifier\n",
    "##########################################\n",
    "def extract_fused_features(dataloader, model, device):\n",
    "    \"\"\"\n",
    "    Extract fused features from the mid-fusion model.\n",
    "    The fused features are the concatenation of the projected DeiT and FTTransformer features.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for deit_feat, ft_feat, labels in dataloader:\n",
    "            deit_feat = deit_feat.to(device)\n",
    "            ft_feat = ft_feat.to(device)\n",
    "            # Project each modality using the model's projection layers.\n",
    "            deit_proj = model.deit_proj(deit_feat)\n",
    "            ft_proj = model.ft_proj(ft_feat)\n",
    "            # Concatenate projections along the feature dimension.\n",
    "            fused = torch.cat((deit_proj, ft_proj), dim=1)  # shape: (batch_size, proj_dim_deit + proj_dim_ft)\n",
    "            all_features.append(fused.cpu())\n",
    "            all_labels.append(labels)\n",
    "    features = torch.cat(all_features, dim=0)\n",
    "    labels = torch.cat(all_labels, dim=0)\n",
    "    return features, labels\n",
    "\n",
    "# Create DataLoaders (assuming batch_size and datasets already defined)\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Extract fused features for each split.\n",
    "fused_train, fused_train_labels = extract_fused_features(train_loader, model, device)\n",
    "fused_val, fused_val_labels = extract_fused_features(val_loader, model, device)\n",
    "fused_test, fused_test_labels = extract_fused_features(test_loader, model, device)\n",
    "\n",
    "print(\"Fused train features shape:\", fused_train.shape)\n",
    "print(\"Fused val features shape:\", fused_val.shape)\n",
    "print(\"Fused test features shape:\", fused_test.shape)\n",
    "\n",
    "# Convert fused features and labels to NumPy arrays.\n",
    "X_train = fused_train.numpy()\n",
    "X_val   = fused_val.numpy()\n",
    "X_test  = fused_test.numpy()\n",
    "\n",
    "y_train = fused_train_labels.numpy()\n",
    "y_val   = fused_val_labels.numpy()\n",
    "y_test  = fused_test_labels.numpy()\n",
    "\n",
    "##########################################\n",
    "# 2. (Optional) Check/Encode Labels\n",
    "##########################################\n",
    "# If you haven't already encoded your labels in your CSV files, you can do so here.\n",
    "# In our MultiModalDataset, labels were already converted to torch.long.\n",
    "# For demonstration, let's assume they are already in integer form (e.g., 0, 1, 2).\n",
    "\n",
    "##########################################\n",
    "# 3. Define and Evaluate Multiple Classifiers\n",
    "##########################################\n",
    "classifiers = {\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"SVC\": SVC(kernel=\"rbf\", probability=True, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(\n",
    "                    iterations=500,\n",
    "                    learning_rate=0.1,\n",
    "                    depth=6,\n",
    "                    loss_function='MultiClass',\n",
    "                    verbose=100,\n",
    "                    random_seed=42\n",
    "                )\n",
    "}\n",
    "\n",
    "# Loop over each classifier, train and evaluate.\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\n--- Training {name} classifier ---\")\n",
    "    if name == \"CatBoost\":\n",
    "        # CatBoost requires early stopping with eval_set.\n",
    "        clf.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)\n",
    "    else:\n",
    "        clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions on each split.\n",
    "    train_preds = clf.predict(X_train)\n",
    "    val_preds   = clf.predict(X_val)\n",
    "    test_preds  = clf.predict(X_test)\n",
    "    \n",
    "    # Compute accuracy.\n",
    "    train_acc = accuracy_score(y_train, train_preds)\n",
    "    val_acc   = accuracy_score(y_val, val_preds)\n",
    "    test_acc  = accuracy_score(y_test, test_preds)\n",
    "    \n",
    "    print(f\"{name} Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"{name} Validation Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"{name} Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"{name} Classification Report on Test Data:\")\n",
    "    print(classification_report(y_test, test_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d86a342-c08f-47c7-91c8-4ffc8579df1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77a50bb1-a15e-4221-b99d-d360a04cef49",
   "metadata": {},
   "source": [
    "## Dimentioanlty reduction suing FC layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47186ace-2993-4f52-a827-bec9b4eebfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Classes: ['AD' 'CN' 'MCI']\n",
      "Fused train features shape: torch.Size([1605, 448])\n",
      "Fused val features shape: torch.Size([344, 448])\n",
      "Fused test features shape: torch.Size([345, 448])\n",
      "\n",
      "--- Training MLP classifier ---\n",
      "MLP Train Accuracy: 0.9813\n",
      "MLP Validation Accuracy: 0.8808\n",
      "MLP Test Accuracy: 0.8986\n",
      "MLP Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81        72\n",
      "           1       0.98      0.95      0.97       106\n",
      "           2       0.89      0.90      0.90       167\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.89      0.89      0.89       345\n",
      "weighted avg       0.90      0.90      0.90       345\n",
      "\n",
      "\n",
      "--- Training KNN classifier ---\n",
      "KNN Train Accuracy: 0.9333\n",
      "KNN Validation Accuracy: 0.8924\n",
      "KNN Test Accuracy: 0.9072\n",
      "KNN Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.82        72\n",
      "           1       0.98      0.95      0.97       106\n",
      "           2       0.89      0.93      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.91      0.89      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n",
      "\n",
      "--- Training Logistic Regression classifier ---\n",
      "Logistic Regression Train Accuracy: 0.9396\n",
      "Logistic Regression Validation Accuracy: 0.8692\n",
      "Logistic Regression Test Accuracy: 0.8870\n",
      "Logistic Regression Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.81      0.78        72\n",
      "           1       0.98      0.95      0.97       106\n",
      "           2       0.89      0.88      0.88       167\n",
      "\n",
      "    accuracy                           0.89       345\n",
      "   macro avg       0.88      0.88      0.88       345\n",
      "weighted avg       0.89      0.89      0.89       345\n",
      "\n",
      "\n",
      "--- Training SVC classifier ---\n",
      "SVC Train Accuracy: 0.8984\n",
      "SVC Validation Accuracy: 0.8779\n",
      "SVC Test Accuracy: 0.9159\n",
      "SVC Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85        72\n",
      "           1       0.98      0.95      0.97       106\n",
      "           2       0.91      0.92      0.91       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.91      0.91      0.91       345\n",
      "weighted avg       0.92      0.92      0.92       345\n",
      "\n",
      "\n",
      "--- Training Random Forest classifier ---\n",
      "Random Forest Train Accuracy: 1.0000\n",
      "Random Forest Validation Accuracy: 0.9070\n",
      "Random Forest Test Accuracy: 0.9217\n",
      "Random Forest Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.86        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.92      0.92      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.92      0.92      0.92       345\n",
      "weighted avg       0.92      0.92      0.92       345\n",
      "\n",
      "\n",
      "--- Training LightGBM classifier ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 78838\n",
      "[LightGBM] [Info] Number of data points in the train set: 1605, number of used features: 392\n",
      "[LightGBM] [Info] Start training from score -1.572737\n",
      "[LightGBM] [Info] Start training from score -1.180370\n",
      "[LightGBM] [Info] Start training from score -0.722868\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM Train Accuracy: 1.0000\n",
      "LightGBM Validation Accuracy: 0.9128\n",
      "LightGBM Test Accuracy: 0.9246\n",
      "LightGBM Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.93      0.92      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.92      0.92      0.92       345\n",
      "weighted avg       0.93      0.92      0.93       345\n",
      "\n",
      "\n",
      "--- Training XGBoost classifier ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [00:48:25] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Train Accuracy: 1.0000\n",
      "XGBoost Validation Accuracy: 0.9099\n",
      "XGBoost Test Accuracy: 0.9333\n",
      "XGBoost Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.88        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.92      0.94      0.93       167\n",
      "\n",
      "    accuracy                           0.93       345\n",
      "   macro avg       0.93      0.93      0.93       345\n",
      "weighted avg       0.93      0.93      0.93       345\n",
      "\n",
      "\n",
      "--- Training CatBoost classifier ---\n",
      "0:\tlearn: 0.9777039\ttest: 0.9863885\tbest: 0.9863885 (0)\ttotal: 29.7ms\tremaining: 14.8s\n",
      "100:\tlearn: 0.1817948\ttest: 0.2963788\tbest: 0.2963788 (100)\ttotal: 3.03s\tremaining: 12s\n",
      "200:\tlearn: 0.1193927\ttest: 0.2721782\tbest: 0.2721782 (200)\ttotal: 6.03s\tremaining: 8.97s\n",
      "300:\tlearn: 0.0857174\ttest: 0.2643756\tbest: 0.2642774 (292)\ttotal: 9.02s\tremaining: 5.96s\n",
      "400:\tlearn: 0.0644922\ttest: 0.2615112\tbest: 0.2610671 (388)\ttotal: 12s\tremaining: 2.97s\n",
      "499:\tlearn: 0.0492366\ttest: 0.2581222\tbest: 0.2581222 (499)\ttotal: 15s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.2581221934\n",
      "bestIteration = 499\n",
      "\n",
      "CatBoost Train Accuracy: 0.9994\n",
      "CatBoost Validation Accuracy: 0.9099\n",
      "CatBoost Test Accuracy: 0.9246\n",
      "CatBoost Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.93      0.92      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.92      0.92      0.92       345\n",
      "weighted avg       0.93      0.92      0.93       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load Features (for each modality)\n",
    "##########################################\n",
    "# Load DeiT features (expected shape: (num_samples, 768))\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")\n",
    "\n",
    "# Load FTTransformer features (expected shape: (num_samples, 192))\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")\n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create a Custom Dataset for Multi-Modal Data\n",
    "##########################################\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, deit_features, ft_features, labels):\n",
    "        self.deit_features = torch.tensor(deit_features, dtype=torch.float32)\n",
    "        self.ft_features = torch.tensor(ft_features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.deit_features[idx], self.ft_features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = MultiModalDataset(deit_train, ft_train, train_labels)\n",
    "val_dataset   = MultiModalDataset(deit_val, ft_val, val_labels)\n",
    "test_dataset  = MultiModalDataset(deit_test, ft_test, test_labels)\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)  # set shuffle=False for feature extraction\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define the Modified Fusion Model\n",
    "##########################################\n",
    "class ModifiedFusionModel(nn.Module):\n",
    "    def __init__(self, ft_input_size=192, deit_input_size=768, num_classes=3):\n",
    "        super(ModifiedFusionModel, self).__init__()\n",
    "        \n",
    "        # FTTransformer branch: reduce from 192 to 170.\n",
    "        self.ft_fc = nn.Sequential(\n",
    "            nn.Linear(ft_input_size, 192),\n",
    "            nn.BatchNorm1d(192),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # DeiT branch: reduce from 768 to 460.\n",
    "        self.deit_fc = nn.Sequential(\n",
    "            nn.Linear(deit_input_size, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Classifier: Note, for feature extraction we will take the concatenated fused features.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(192 + 256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, deit_features, ft_features):\n",
    "        # Project each modality.\n",
    "        deit_proj = self.deit_fc(deit_features)  # shape: (batch_size, 460)\n",
    "        ft_proj = self.ft_fc(ft_features)          # shape: (batch_size, 170)\n",
    "        # Concatenate the projections.\n",
    "        fused_features = torch.cat([ft_proj, deit_proj], dim=1)  # shape: (batch_size, 630)\n",
    "        # Final classification (not used for feature extraction below).\n",
    "        logits = self.classifier(fused_features)\n",
    "        return logits, fused_features\n",
    "\n",
    "# Initialize the model.\n",
    "deit_dim = deit_train.shape[1]  # expected 768\n",
    "ft_dim = ft_train.shape[1]      # expected 192\n",
    "fusion_model = ModifiedFusionModel(ft_input_size=ft_dim, deit_input_size=deit_dim, num_classes=num_classes)\n",
    "fusion_model.to(device)\n",
    "\n",
    "##########################################\n",
    "# 5. Extract Fused Features Using the Fusion Model\n",
    "##########################################\n",
    "def extract_fused_features(dataloader, model, device):\n",
    "    \"\"\"\n",
    "    Pass data through the model to extract fused features.\n",
    "    We ignore the final classifier outputs and take the fused_features from the concatenation.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for deit_feat, ft_feat, labels in dataloader:\n",
    "            deit_feat = deit_feat.to(device)\n",
    "            ft_feat = ft_feat.to(device)\n",
    "            # Forward pass returns (logits, fused_features)\n",
    "            _, fused = model(deit_feat, ft_feat)\n",
    "            all_features.append(fused.cpu())\n",
    "            all_labels.append(labels)\n",
    "    features = torch.cat(all_features, dim=0)\n",
    "    labels = torch.cat(all_labels, dim=0)\n",
    "    return features, labels\n",
    "\n",
    "# Extract fused features for train, validation, and test sets.\n",
    "fused_train, fused_train_labels = extract_fused_features(train_loader, fusion_model, device)\n",
    "fused_val, fused_val_labels = extract_fused_features(val_loader, fusion_model, device)\n",
    "fused_test, fused_test_labels = extract_fused_features(test_loader, fusion_model, device)\n",
    "\n",
    "print(\"Fused train features shape:\", fused_train.shape)  # Expected: (num_train_samples, 630)\n",
    "print(\"Fused val features shape:\", fused_val.shape)\n",
    "print(\"Fused test features shape:\", fused_test.shape)\n",
    "\n",
    "# Convert to NumPy arrays.\n",
    "X_train = fused_train.numpy()\n",
    "X_val   = fused_val.numpy()\n",
    "X_test  = fused_test.numpy()\n",
    "y_train = fused_train_labels.numpy()\n",
    "y_val   = fused_val_labels.numpy()\n",
    "y_test  = fused_test_labels.numpy()\n",
    "\n",
    "##########################################\n",
    "# 6. Define and Evaluate Multiple Classifiers on Fused Features\n",
    "##########################################\n",
    "classifiers = {\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"SVC\": SVC(kernel=\"rbf\", probability=True, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(\n",
    "                    iterations=500,\n",
    "                    learning_rate=0.1,\n",
    "                    depth=6,\n",
    "                    loss_function='MultiClass',\n",
    "                    verbose=100,\n",
    "                    random_seed=42\n",
    "                )\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\n--- Training {name} classifier ---\")\n",
    "    if name == \"CatBoost\":\n",
    "        clf.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)\n",
    "    else:\n",
    "        clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions on each split.\n",
    "    train_preds = clf.predict(X_train)\n",
    "    val_preds = clf.predict(X_val)\n",
    "    test_preds = clf.predict(X_test)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, train_preds)\n",
    "    val_acc = accuracy_score(y_val, val_preds)\n",
    "    test_acc = accuracy_score(y_test, test_preds)\n",
    "    \n",
    "    print(f\"{name} Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"{name} Validation Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"{name} Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"{name} Classification Report on Test Data:\")\n",
    "    print(classification_report(y_test, test_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c52f7f-3da8-41a4-a78f-de2b75929a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd4aaf-6497-4ce0-8e99-5d79eaf63dab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b9ac9-56ba-4866-9ad0-7085e5d9b575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "48d60cc8-86c2-461e-ae2b-de6ec8466d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Classes: ['AD' 'CN' 'MCI']\n",
      "IntermediateFusionModel(\n",
      "  (ft_branch): Sequential(\n",
      "    (0): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (deit_branch): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=460, bias=True)\n",
      "    (1): BatchNorm1d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (fusion): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.3, inplace=False)\n",
      "    (9): Linear(in_features=64, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1/50: Train Loss = 0.2091, Val Loss = 0.2592, Val Acc = 0.9041\n",
      "  -> New best model saved.\n",
      "Epoch 2/50: Train Loss = 0.1843, Val Loss = 0.2537, Val Acc = 0.9070\n",
      "  -> New best model saved.\n",
      "Epoch 3/50: Train Loss = 0.2032, Val Loss = 0.2603, Val Acc = 0.9099\n",
      "Epoch 4/50: Train Loss = 0.1859, Val Loss = 0.2560, Val Acc = 0.9099\n",
      "Epoch 5/50: Train Loss = 0.1941, Val Loss = 0.2623, Val Acc = 0.9041\n",
      "Epoch 6/50: Train Loss = 0.1868, Val Loss = 0.2490, Val Acc = 0.9070\n",
      "  -> New best model saved.\n",
      "Epoch 7/50: Train Loss = 0.1934, Val Loss = 0.2526, Val Acc = 0.9157\n",
      "Epoch 8/50: Train Loss = 0.1789, Val Loss = 0.2471, Val Acc = 0.8983\n",
      "  -> New best model saved.\n",
      "Epoch 9/50: Train Loss = 0.1790, Val Loss = 0.2674, Val Acc = 0.9070\n",
      "Epoch 10/50: Train Loss = 0.1637, Val Loss = 0.2838, Val Acc = 0.8983\n",
      "Epoch 11/50: Train Loss = 0.1765, Val Loss = 0.2706, Val Acc = 0.8983\n",
      "Epoch 12/50: Train Loss = 0.1712, Val Loss = 0.2578, Val Acc = 0.9128\n",
      "Epoch 13/50: Train Loss = 0.1730, Val Loss = 0.2758, Val Acc = 0.9041\n",
      "Epoch 14/50: Train Loss = 0.1697, Val Loss = 0.2685, Val Acc = 0.9099\n",
      "Epoch 15/50: Train Loss = 0.1547, Val Loss = 0.2601, Val Acc = 0.9099\n",
      "Epoch 16/50: Train Loss = 0.1469, Val Loss = 0.2743, Val Acc = 0.9099\n",
      "Epoch 17/50: Train Loss = 0.1480, Val Loss = 0.2556, Val Acc = 0.9099\n",
      "Epoch 18/50: Train Loss = 0.1472, Val Loss = 0.2749, Val Acc = 0.9099\n",
      "Early stopping triggered.\n",
      "Fused train features shape: torch.Size([1605, 576])\n",
      "Fused val features shape: torch.Size([344, 576])\n",
      "Fused test features shape: torch.Size([345, 576])\n",
      "\n",
      "--- Training MLP classifier ---\n",
      "MLP Train Accuracy: 1.0000\n",
      "MLP Validation Accuracy: 0.9157\n",
      "MLP Test Accuracy: 0.9072\n",
      "MLP Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.81      0.81        72\n",
      "           1       0.99      0.96      0.98       106\n",
      "           2       0.89      0.92      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.90      0.89      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n",
      "\n",
      "--- Training KNN classifier ---\n",
      "KNN Train Accuracy: 0.9726\n",
      "KNN Validation Accuracy: 0.8866\n",
      "KNN Test Accuracy: 0.9217\n",
      "KNN Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.85        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.93      0.91      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.91      0.92      0.92       345\n",
      "weighted avg       0.92      0.92      0.92       345\n",
      "\n",
      "\n",
      "--- Training Logistic Regression classifier ---\n",
      "Logistic Regression Train Accuracy: 1.0000\n",
      "Logistic Regression Validation Accuracy: 0.9186\n",
      "Logistic Regression Test Accuracy: 0.9101\n",
      "Logistic Regression Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82        72\n",
      "           1       0.99      0.96      0.98       106\n",
      "           2       0.90      0.92      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.90      0.90      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n",
      "\n",
      "--- Training SVC classifier ---\n",
      "SVC Train Accuracy: 0.9969\n",
      "SVC Validation Accuracy: 0.9157\n",
      "SVC Test Accuracy: 0.9130\n",
      "SVC Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83        72\n",
      "           1       0.99      0.95      0.97       106\n",
      "           2       0.90      0.93      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.91      0.90      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n",
      "\n",
      "--- Training Random Forest classifier ---\n",
      "Random Forest Train Accuracy: 1.0000\n",
      "Random Forest Validation Accuracy: 0.9099\n",
      "Random Forest Test Accuracy: 0.9217\n",
      "Random Forest Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.92      0.92      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.91      0.92      0.92       345\n",
      "weighted avg       0.92      0.92      0.92       345\n",
      "\n",
      "\n",
      "--- Training LightGBM classifier ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028173 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 130714\n",
      "[LightGBM] [Info] Number of data points in the train set: 1605, number of used features: 561\n",
      "[LightGBM] [Info] Start training from score -1.572737\n",
      "[LightGBM] [Info] Start training from score -1.180370\n",
      "[LightGBM] [Info] Start training from score -0.722868\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM Train Accuracy: 1.0000\n",
      "LightGBM Validation Accuracy: 0.9099\n",
      "LightGBM Test Accuracy: 0.9217\n",
      "LightGBM Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.86        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.92      0.92      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.92      0.92      0.92       345\n",
      "weighted avg       0.92      0.92      0.92       345\n",
      "\n",
      "\n",
      "--- Training XGBoost classifier ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [04:02:57] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Train Accuracy: 1.0000\n",
      "XGBoost Validation Accuracy: 0.9157\n",
      "XGBoost Test Accuracy: 0.9217\n",
      "XGBoost Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.86        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.92      0.92      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.92      0.92      0.92       345\n",
      "weighted avg       0.92      0.92      0.92       345\n",
      "\n",
      "\n",
      "--- Training CatBoost classifier ---\n",
      "0:\tlearn: 0.9846169\ttest: 0.9989000\tbest: 0.9989000 (0)\ttotal: 48.7ms\tremaining: 24.3s\n",
      "100:\tlearn: 0.0892945\ttest: 0.2573692\tbest: 0.2573692 (100)\ttotal: 4.77s\tremaining: 18.9s\n",
      "200:\tlearn: 0.0479322\ttest: 0.2482266\tbest: 0.2482266 (200)\ttotal: 9.5s\tremaining: 14.1s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.2474455292\n",
      "bestIteration = 243\n",
      "\n",
      "Shrink model to first 244 iterations.\n",
      "CatBoost Train Accuracy: 0.9975\n",
      "CatBoost Validation Accuracy: 0.9157\n",
      "CatBoost Test Accuracy: 0.9130\n",
      "CatBoost Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84        72\n",
      "           1       0.98      0.95      0.97       106\n",
      "           2       0.90      0.92      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.91      0.90      0.91       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load Features (for each modality)\n",
    "##########################################\n",
    "# Load DeiT features (expected shape: (num_samples, 768))\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")\n",
    "\n",
    "# Load FTTransformer features (expected shape: (num_samples, 192))\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")\n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create a Custom Dataset for Multi-Modal Data\n",
    "##########################################\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, deit_features, ft_features, labels):\n",
    "        # Convert features to torch tensors.\n",
    "        self.deit_features = torch.tensor(deit_features, dtype=torch.float32)\n",
    "        self.ft_features = torch.tensor(ft_features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.deit_features[idx], self.ft_features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = MultiModalDataset(deit_train, ft_train, train_labels)\n",
    "val_dataset   = MultiModalDataset(deit_val, ft_val, val_labels)\n",
    "test_dataset  = MultiModalDataset(deit_test, ft_test, test_labels)\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define the Intermediate Fusion Model\n",
    "##########################################\n",
    "\n",
    "class IntermediateFusionModel(nn.Module):\n",
    "    def __init__(self, ft_input_size=192, deit_input_size=768, num_classes=3):\n",
    "        super(IntermediateFusionModel, self).__init__()\n",
    "        # FTTransformer branch: keep the original 192 dimensions (apply normalization only)\n",
    "        self.ft_branch = nn.Sequential(\n",
    "            nn.BatchNorm1d(ft_input_size)\n",
    "        )\n",
    "        # DeiT branch: reduce from 768 to 320.\n",
    "        self.deit_branch = nn.Sequential(\n",
    "            nn.Linear(deit_input_size, 460),\n",
    "            nn.BatchNorm1d(460),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        # Fusion classifier:\n",
    "        # The fused vector has 192 (FT) + 320 (DeiT) = 512 dimensions.\n",
    "        # We add two extra layers before the final classification.\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),   # Extra layer 1.\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),    # Extra layer 2.\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, ft_features, deit_features):\n",
    "        # Process FTTransformer features: shape remains (batch_size, 192)\n",
    "        ft_out = self.ft_branch(ft_features)\n",
    "        # Process DeiT features: reduced to (batch_size, 320)\n",
    "        deit_out = self.deit_branch(deit_features)\n",
    "        # Concatenate along the feature dimension: result shape (batch_size, 512)\n",
    "        fused_features = torch.cat([ft_out, deit_out], dim=1)\n",
    "        # Pass through the final classifier.\n",
    "        logits = self.fusion(fused_features)\n",
    "        return logits, fused_features\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have your FTTransformer features (shape: [batch_size, 192])\n",
    "# and DeiT features (shape: [batch_size, 768]), and num_classes is defined.\n",
    "num_classes = 3  # For example, classes: CN, MCI, AD.\n",
    "model = IntermediateFusionModel(ft_input_size=192, deit_input_size=768, num_classes=num_classes)\n",
    "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "print(model)\n",
    "\n",
    "##########################################\n",
    "# 5. Train the Fusion Model with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(fusion_model.parameters(), lr=1e-5)\n",
    "num_epochs = 50\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    fusion_model.train()\n",
    "    train_loss = 0.0\n",
    "    for deit_feat, ft_feat, labels in train_loader:\n",
    "        deit_feat, ft_feat, labels = deit_feat.to(device), ft_feat.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = fusion_model(deit_feat, ft_feat)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * labels.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    fusion_model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for deit_feat, ft_feat, labels in val_loader:\n",
    "            deit_feat, ft_feat, labels = deit_feat.to(device), ft_feat.to(device), labels.to(device)\n",
    "            logits, _ = fusion_model(deit_feat, ft_feat)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item() * labels.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(labels.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    val_acc = accuracy_score(all_targets, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = fusion_model.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "if best_model_state is not None:\n",
    "    fusion_model.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Extract Fused Features Using the Fusion Model\n",
    "##########################################\n",
    "def extract_fused_features(dataloader, model, device):\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for deit_feat, ft_feat, labels in dataloader:\n",
    "            deit_feat, ft_feat = deit_feat.to(device), ft_feat.to(device)\n",
    "            # Forward pass returns (logits, fused_features)\n",
    "            _, fused = fusion_model(deit_feat, ft_feat)\n",
    "            all_features.append(fused.cpu())\n",
    "            all_labels.append(labels)\n",
    "    features = torch.cat(all_features, dim=0)\n",
    "    labels = torch.cat(all_labels, dim=0)\n",
    "    return features, labels\n",
    "\n",
    "fused_train, fused_train_labels = extract_fused_features(train_loader, fusion_model, device)\n",
    "fused_val, fused_val_labels = extract_fused_features(val_loader, fusion_model, device)\n",
    "fused_test, fused_test_labels = extract_fused_features(test_loader, fusion_model, device)\n",
    "\n",
    "print(\"Fused train features shape:\", fused_train.shape)  # Expected: (num_train_samples, 512)\n",
    "print(\"Fused val features shape:\", fused_val.shape)\n",
    "print(\"Fused test features shape:\", fused_test.shape)\n",
    "\n",
    "# Convert fused features and labels to NumPy arrays.\n",
    "X_train = fused_train.numpy()\n",
    "X_val   = fused_val.numpy()\n",
    "X_test  = fused_test.numpy()\n",
    "y_train = fused_train_labels.numpy()\n",
    "y_val   = fused_val_labels.numpy()\n",
    "y_test  = fused_test_labels.numpy()\n",
    "\n",
    "##########################################\n",
    "# 7. Normalize the Fused Features Using StandardScaler\n",
    "##########################################\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val   = scaler.transform(X_val)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "##########################################\n",
    "# 8. Train and Evaluate Multiple Classifiers on Fused Features\n",
    "##########################################\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "classifiers = {\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"SVC\": SVC(kernel=\"rbf\", probability=True, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(\n",
    "                    iterations=500,\n",
    "                    learning_rate=0.1,\n",
    "                    depth=6,\n",
    "                    loss_function='MultiClass',\n",
    "                    verbose=100,\n",
    "                    random_seed=42\n",
    "                )\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\n--- Training {name} classifier ---\")\n",
    "    if name == \"CatBoost\":\n",
    "        clf.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)\n",
    "    else:\n",
    "        clf.fit(X_train, y_train)\n",
    "    \n",
    "    train_preds = clf.predict(X_train)\n",
    "    val_preds = clf.predict(X_val)\n",
    "    test_preds = clf.predict(X_test)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, train_preds)\n",
    "    val_acc = accuracy_score(y_val, val_preds)\n",
    "    test_acc = accuracy_score(y_test, test_preds)\n",
    "    \n",
    "    print(f\"{name} Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"{name} Validation Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"{name} Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"{name} Classification Report on Test Data:\")\n",
    "    print(classification_report(y_test, test_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b8714-6631-409d-9a4a-648f9069958e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c916927c-efac-47ac-8e74-31d842917382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3272010-16f0-4699-8c1e-03dbba5d3c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01e797c6-d35d-4fbe-b0b7-7c767a44a660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Classes: ['AD' 'CN' 'MCI']\n",
      "Epoch 1/100: Train Loss = 0.5040, Val Loss = 0.5941, Val Acc = 0.8343\n",
      "  -> New best model saved.\n",
      "Epoch 2/100: Train Loss = 0.4920, Val Loss = 0.3682, Val Acc = 0.8634\n",
      "  -> New best model saved.\n",
      "Epoch 3/100: Train Loss = 0.4488, Val Loss = 0.3287, Val Acc = 0.8721\n",
      "  -> New best model saved.\n",
      "Epoch 4/100: Train Loss = 0.4000, Val Loss = 0.3643, Val Acc = 0.8663\n",
      "Epoch 5/100: Train Loss = 0.4304, Val Loss = 0.3772, Val Acc = 0.8314\n",
      "Epoch 6/100: Train Loss = 0.3728, Val Loss = 0.3530, Val Acc = 0.8605\n",
      "Epoch 7/100: Train Loss = 0.3449, Val Loss = 0.3664, Val Acc = 0.8692\n",
      "Epoch 8/100: Train Loss = 0.3785, Val Loss = 0.3096, Val Acc = 0.8692\n",
      "  -> New best model saved.\n",
      "Epoch 9/100: Train Loss = 0.3418, Val Loss = 0.3572, Val Acc = 0.8983\n",
      "Epoch 10/100: Train Loss = 0.3560, Val Loss = 0.3275, Val Acc = 0.8663\n",
      "Epoch 11/100: Train Loss = 0.3019, Val Loss = 0.3099, Val Acc = 0.8924\n",
      "Epoch 12/100: Train Loss = 0.2994, Val Loss = 0.3830, Val Acc = 0.8779\n",
      "Epoch 13/100: Train Loss = 0.3055, Val Loss = 0.3648, Val Acc = 0.9041\n",
      "Epoch 14/100: Train Loss = 0.2700, Val Loss = 0.4779, Val Acc = 0.8663\n",
      "Epoch 15/100: Train Loss = 0.2566, Val Loss = 0.3359, Val Acc = 0.8983\n",
      "Epoch 16/100: Train Loss = 0.2505, Val Loss = 0.3532, Val Acc = 0.8953\n",
      "Epoch 17/100: Train Loss = 0.2410, Val Loss = 0.4077, Val Acc = 0.8866\n",
      "Epoch 18/100: Train Loss = 0.2186, Val Loss = 0.3228, Val Acc = 0.9128\n",
      "Early stopping triggered.\n",
      "Fused train features shape: torch.Size([1605, 256])\n",
      "Fused val features shape: torch.Size([344, 256])\n",
      "Fused test features shape: torch.Size([345, 256])\n",
      "\n",
      "--- Training MLP classifier ---\n",
      "MLP Train Accuracy: 1.0000\n",
      "MLP Validation Accuracy: 0.9099\n",
      "MLP Test Accuracy: 0.8986\n",
      "MLP Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.80        72\n",
      "           1       0.99      0.96      0.98       106\n",
      "           2       0.90      0.89      0.89       167\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.89      0.89      0.89       345\n",
      "weighted avg       0.90      0.90      0.90       345\n",
      "\n",
      "\n",
      "--- Training KNN classifier ---\n",
      "KNN Train Accuracy: 0.9931\n",
      "KNN Validation Accuracy: 0.9128\n",
      "KNN Test Accuracy: 0.9101\n",
      "KNN Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.84        72\n",
      "           1       0.98      0.95      0.97       106\n",
      "           2       0.90      0.91      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.90      0.90      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n",
      "\n",
      "--- Training Logistic Regression classifier ---\n",
      "Logistic Regression Train Accuracy: 0.9988\n",
      "Logistic Regression Validation Accuracy: 0.9099\n",
      "Logistic Regression Test Accuracy: 0.9101\n",
      "Logistic Regression Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83        72\n",
      "           1       0.99      0.95      0.97       106\n",
      "           2       0.90      0.91      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.90      0.90      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n",
      "\n",
      "--- Training SVC classifier ---\n",
      "SVC Train Accuracy: 0.9938\n",
      "SVC Validation Accuracy: 0.9070\n",
      "SVC Test Accuracy: 0.9072\n",
      "SVC Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.83        72\n",
      "           1       0.98      0.95      0.97       106\n",
      "           2       0.91      0.90      0.90       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.90      0.90      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n",
      "\n",
      "--- Training Random Forest classifier ---\n",
      "Random Forest Train Accuracy: 1.0000\n",
      "Random Forest Validation Accuracy: 0.9128\n",
      "Random Forest Test Accuracy: 0.9043\n",
      "Random Forest Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.85      0.82        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.91      0.89      0.90       167\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.89      0.90      0.90       345\n",
      "weighted avg       0.91      0.90      0.91       345\n",
      "\n",
      "\n",
      "--- Training LightGBM classifier ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004866 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65280\n",
      "[LightGBM] [Info] Number of data points in the train set: 1605, number of used features: 256\n",
      "[LightGBM] [Info] Start training from score -1.572737\n",
      "[LightGBM] [Info] Start training from score -1.180370\n",
      "[LightGBM] [Info] Start training from score -0.722868\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM Train Accuracy: 1.0000\n",
      "LightGBM Validation Accuracy: 0.9099\n",
      "LightGBM Test Accuracy: 0.9072\n",
      "LightGBM Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.86      0.82        72\n",
      "           1       0.99      0.96      0.98       106\n",
      "           2       0.91      0.89      0.90       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.90      0.91      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n",
      "\n",
      "--- Training XGBoost classifier ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [00:53:45] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Train Accuracy: 1.0000\n",
      "XGBoost Validation Accuracy: 0.9128\n",
      "XGBoost Test Accuracy: 0.9072\n",
      "XGBoost Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.86      0.82        72\n",
      "           1       0.99      0.96      0.98       106\n",
      "           2       0.91      0.89      0.90       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.90      0.91      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n",
      "\n",
      "--- Training CatBoost classifier ---\n",
      "0:\tlearn: 0.9312415\ttest: 0.9590796\tbest: 0.9590796 (0)\ttotal: 19.8ms\tremaining: 9.9s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.3101553302\n",
      "bestIteration = 32\n",
      "\n",
      "Shrink model to first 33 iterations.\n",
      "CatBoost Train Accuracy: 0.9907\n",
      "CatBoost Validation Accuracy: 0.9157\n",
      "CatBoost Test Accuracy: 0.8928\n",
      "CatBoost Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.85      0.80        72\n",
      "           1       0.98      0.95      0.97       106\n",
      "           2       0.90      0.87      0.89       167\n",
      "\n",
      "    accuracy                           0.89       345\n",
      "   macro avg       0.88      0.89      0.89       345\n",
      "weighted avg       0.90      0.89      0.89       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load Features (for each modality)\n",
    "##########################################\n",
    "# Load DeiT features (expected shape: (num_samples, 768))\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")\n",
    "\n",
    "# Load FTTransformer features (expected shape: (num_samples, 192))\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")\n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "train_data = pd.read_csv(\"train_data_3.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_3.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_3.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create a Custom Dataset for Multi-Modal Data\n",
    "##########################################\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, deit_features, ft_features, labels):\n",
    "        self.deit_features = torch.tensor(deit_features, dtype=torch.float32)\n",
    "        self.ft_features = torch.tensor(ft_features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.deit_features[idx], self.ft_features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = MultiModalDataset(deit_train, ft_train, train_labels)\n",
    "val_dataset   = MultiModalDataset(deit_val, ft_val, val_labels)\n",
    "test_dataset  = MultiModalDataset(deit_test, ft_test, test_labels)\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define the Cross-Attention Fusion Model\n",
    "##########################################\n",
    "class CrossAttentionFusionModel(nn.Module):\n",
    "    def __init__(self, ft_input_size=192, deit_input_size=768, num_classes=3,\n",
    "                 ft_proj_dim=192, deit_proj_dim=460, common_dim=256):\n",
    "        super(CrossAttentionFusionModel, self).__init__()\n",
    "        # FTTransformer branch: Here we keep FT features mostly as is,\n",
    "        # optionally project them to ft_proj_dim.\n",
    "        self.ft_proj = nn.Sequential(\n",
    "            nn.Linear(ft_input_size, ft_proj_dim),\n",
    "            nn.BatchNorm1d(ft_proj_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # DeiT branch: Reduce from 768 to deit_proj_dim.\n",
    "        self.deit_proj = nn.Sequential(\n",
    "            nn.Linear(deit_input_size, deit_proj_dim),\n",
    "            nn.BatchNorm1d(deit_proj_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        # Project both modalities to a common dimension.\n",
    "        self.ft_common = nn.Linear(ft_proj_dim, common_dim)\n",
    "        self.deit_common = nn.Linear(deit_proj_dim, common_dim)\n",
    "        \n",
    "        # Multi-head self-attention over the two tokens.\n",
    "        # batch_first=True: input shape (batch_size, seq_len, embed_dim).\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=common_dim, num_heads=4, batch_first=True)\n",
    "        \n",
    "        # Fusion classifier: aggregate the two tokens (by mean pooling) and classify.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(common_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, ft_features, deit_features):\n",
    "        # ft_features: shape (batch_size, ft_input_size)\n",
    "        # deit_features: shape (batch_size, deit_input_size)\n",
    "        # Project each modality.\n",
    "        ft_out = self.ft_proj(ft_features)          # (batch_size, ft_proj_dim)\n",
    "        deit_out = self.deit_proj(deit_features)      # (batch_size, deit_proj_dim)\n",
    "        # Project to common space.\n",
    "        ft_token = self.ft_common(ft_out)             # (batch_size, common_dim)\n",
    "        deit_token = self.deit_common(deit_out)         # (batch_size, common_dim)\n",
    "        # Create a sequence of two tokens for each sample.\n",
    "        # Shape: (batch_size, 2, common_dim)\n",
    "        tokens = torch.stack([ft_token, deit_token], dim=1)\n",
    "        # Apply multi-head self-attention. Use tokens as query, key, value.\n",
    "        attn_output, _ = self.attention(tokens, tokens, tokens)\n",
    "        # Aggregate tokens by mean pooling (across the sequence dimension).\n",
    "        fused_features = attn_output.mean(dim=1)       # (batch_size, common_dim)\n",
    "        logits = self.classifier(fused_features)       # (batch_size, num_classes)\n",
    "        return logits, fused_features\n",
    "\n",
    "# Initialize the fusion model.\n",
    "fusion_model = CrossAttentionFusionModel(ft_input_size=ft_train.shape[1],\n",
    "                                          deit_input_size=deit_train.shape[1],\n",
    "                                          num_classes=num_classes,\n",
    "                                          ft_proj_dim=192,      # Keep FT as 192.\n",
    "                                          deit_proj_dim=460,    # Reduce DeiT from 768 to 320.\n",
    "                                          common_dim=256)       # Common embedding dimension.\n",
    "fusion_model.to(device)\n",
    "\n",
    "##########################################\n",
    "# 5b. (Optional) Train the Fusion Model with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(fusion_model.parameters(), lr=1e-3)\n",
    "num_epochs = 100\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    fusion_model.train()\n",
    "    train_loss = 0.0\n",
    "    for deit_feat, ft_feat, labels in train_loader:\n",
    "        deit_feat, ft_feat, labels = deit_feat.to(device), ft_feat.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # Note: Our model expects ft_features first then deit_features.\n",
    "        logits, _ = fusion_model(ft_feat, deit_feat)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * labels.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    fusion_model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for deit_feat, ft_feat, labels in val_loader:\n",
    "            deit_feat, ft_feat, labels = deit_feat.to(device), ft_feat.to(device), labels.to(device)\n",
    "            logits, _ = fusion_model(ft_feat, deit_feat)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item() * labels.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(labels.cpu().numpy())\n",
    "    val_loss /= len(val_dataset)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    val_acc = accuracy_score(all_targets, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = fusion_model.state_dict()\n",
    "        patience_counter = 0\n",
    "        print(\"  -> New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "if best_model_state is not None:\n",
    "    fusion_model.load_state_dict(best_model_state)\n",
    "\n",
    "##########################################\n",
    "# 6. Extract Fused Features Using the Fusion Model\n",
    "##########################################\n",
    "def extract_fused_features(dataloader, model, device):\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for deit_feat, ft_feat, labels in dataloader:\n",
    "            deit_feat, ft_feat = deit_feat.to(device), ft_feat.to(device)\n",
    "            # Forward pass returns (logits, fused_features)\n",
    "            _, fused = fusion_model(ft_feat, deit_feat)\n",
    "            all_features.append(fused.cpu())\n",
    "            all_labels.append(labels)\n",
    "    features = torch.cat(all_features, dim=0)\n",
    "    labels = torch.cat(all_labels, dim=0)\n",
    "    return features, labels\n",
    "\n",
    "fused_train, fused_train_labels = extract_fused_features(train_loader, fusion_model, device)\n",
    "fused_val, fused_val_labels = extract_fused_features(val_loader, fusion_model, device)\n",
    "fused_test, fused_test_labels = extract_fused_features(test_loader, fusion_model, device)\n",
    "\n",
    "print(\"Fused train features shape:\", fused_train.shape)  # Expected: (num_train_samples, common_dim)\n",
    "print(\"Fused val features shape:\", fused_val.shape)\n",
    "print(\"Fused test features shape:\", fused_test.shape)\n",
    "\n",
    "# Convert to NumPy arrays.\n",
    "X_train = fused_train.numpy()\n",
    "X_val   = fused_val.numpy()\n",
    "X_test  = fused_test.numpy()\n",
    "y_train = fused_train_labels.numpy()\n",
    "y_val   = fused_val_labels.numpy()\n",
    "y_test  = fused_test_labels.numpy()\n",
    "\n",
    "##########################################\n",
    "# 7. Normalize the Fused Features Using StandardScaler\n",
    "##########################################\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val   = scaler.transform(X_val)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "##########################################\n",
    "# 8. Train and Evaluate Multiple Classifiers on Fused Features\n",
    "##########################################\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "classifiers = {\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"SVC\": SVC(kernel=\"rbf\", probability=True, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(\n",
    "                    iterations=500,\n",
    "                    learning_rate=0.1,\n",
    "                    depth=6,\n",
    "                    loss_function='MultiClass',\n",
    "                    verbose=100,\n",
    "                    random_seed=42\n",
    "                )\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\n--- Training {name} classifier ---\")\n",
    "    if name == \"CatBoost\":\n",
    "        clf.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)\n",
    "    else:\n",
    "        clf.fit(X_train, y_train)\n",
    "    \n",
    "    train_preds = clf.predict(X_train)\n",
    "    val_preds = clf.predict(X_val)\n",
    "    test_preds = clf.predict(X_test)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, train_preds)\n",
    "    val_acc = accuracy_score(y_val, val_preds)\n",
    "    test_acc = accuracy_score(y_test, test_preds)\n",
    "    \n",
    "    print(f\"{name} Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"{name} Validation Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"{name} Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"{name} Classification Report on Test Data:\")\n",
    "    print(classification_report(y_test, test_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92d65ac8-b6c8-480a-add5-09d65844164e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Classes: ['AD' 'CN' 'MCI']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Train Loss: 0.8634, Train Acc: 0.7184 | Val Loss: 0.4834, Val Acc: 0.8692\n",
      "  -> Best model saved.\n",
      "Epoch [2/100] Train Loss: 0.4002, Train Acc: 0.8667 | Val Loss: 0.3361, Val Acc: 0.8517\n",
      "  -> Best model saved.\n",
      "Epoch [3/100] Train Loss: 0.3493, Train Acc: 0.8829 | Val Loss: 0.3181, Val Acc: 0.8750\n",
      "  -> Best model saved.\n",
      "Epoch [4/100] Train Loss: 0.2984, Train Acc: 0.8891 | Val Loss: 0.3123, Val Acc: 0.8750\n",
      "  -> Best model saved.\n",
      "Epoch [5/100] Train Loss: 0.2966, Train Acc: 0.8947 | Val Loss: 0.2930, Val Acc: 0.8808\n",
      "  -> Best model saved.\n",
      "Epoch [6/100] Train Loss: 0.2589, Train Acc: 0.9040 | Val Loss: 0.2938, Val Acc: 0.8866\n",
      "Epoch [7/100] Train Loss: 0.2287, Train Acc: 0.9221 | Val Loss: 0.3004, Val Acc: 0.8895\n",
      "Epoch [8/100] Train Loss: 0.2283, Train Acc: 0.9140 | Val Loss: 0.2838, Val Acc: 0.8808\n",
      "  -> Best model saved.\n",
      "Epoch [9/100] Train Loss: 0.1909, Train Acc: 0.9302 | Val Loss: 0.2716, Val Acc: 0.9012\n",
      "  -> Best model saved.\n",
      "Epoch [10/100] Train Loss: 0.1869, Train Acc: 0.9371 | Val Loss: 0.2632, Val Acc: 0.9070\n",
      "  -> Best model saved.\n",
      "Epoch [11/100] Train Loss: 0.1519, Train Acc: 0.9508 | Val Loss: 0.2726, Val Acc: 0.9099\n",
      "Epoch [12/100] Train Loss: 0.1331, Train Acc: 0.9545 | Val Loss: 0.2868, Val Acc: 0.9128\n",
      "Epoch [13/100] Train Loss: 0.1166, Train Acc: 0.9657 | Val Loss: 0.2679, Val Acc: 0.9186\n",
      "Epoch [14/100] Train Loss: 0.0922, Train Acc: 0.9695 | Val Loss: 0.2639, Val Acc: 0.9186\n",
      "Epoch [15/100] Train Loss: 0.0919, Train Acc: 0.9682 | Val Loss: 0.2779, Val Acc: 0.9244\n",
      "Epoch [16/100] Train Loss: 0.0659, Train Acc: 0.9826 | Val Loss: 0.2936, Val Acc: 0.9099\n",
      "Epoch [17/100] Train Loss: 0.0699, Train Acc: 0.9782 | Val Loss: 0.2949, Val Acc: 0.9070\n",
      "Epoch [18/100] Train Loss: 0.0663, Train Acc: 0.9769 | Val Loss: 0.3119, Val Acc: 0.9070\n",
      "Epoch [19/100] Train Loss: 0.0596, Train Acc: 0.9819 | Val Loss: 0.2973, Val Acc: 0.9157\n",
      "Epoch [20/100] Train Loss: 0.0522, Train Acc: 0.9875 | Val Loss: 0.2961, Val Acc: 0.9157\n",
      "Early stopping triggered.\n",
      "Fused train features shape: torch.Size([1605, 576])\n",
      "Fused val features shape: torch.Size([344, 576])\n",
      "Fused test features shape: torch.Size([345, 576])\n",
      "\n",
      "--- Training MLP classifier ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2316065/1086754801.py:178: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  fusion_model.load_state_dict(torch.load('best_fusion_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Train Accuracy: 1.0000\n",
      "MLP Validation Accuracy: 0.9157\n",
      "MLP Test Accuracy: 0.9043\n",
      "MLP Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81        72\n",
      "           1       0.99      0.96      0.98       106\n",
      "           2       0.89      0.91      0.90       167\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.90      0.89      0.89       345\n",
      "weighted avg       0.91      0.90      0.90       345\n",
      "\n",
      "\n",
      "--- Training KNN classifier ---\n",
      "KNN Train Accuracy: 0.9682\n",
      "KNN Validation Accuracy: 0.8953\n",
      "KNN Test Accuracy: 0.9188\n",
      "KNN Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.93      0.91      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.91      0.92      0.91       345\n",
      "weighted avg       0.92      0.92      0.92       345\n",
      "\n",
      "\n",
      "--- Training Logistic Regression classifier ---\n",
      "Logistic Regression Train Accuracy: 1.0000\n",
      "Logistic Regression Validation Accuracy: 0.9157\n",
      "Logistic Regression Test Accuracy: 0.9101\n",
      "Logistic Regression Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.81        72\n",
      "           1       0.99      0.97      0.98       106\n",
      "           2       0.90      0.91      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.90      0.90      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n",
      "\n",
      "--- Training SVC classifier ---\n",
      "SVC Train Accuracy: 0.9919\n",
      "SVC Validation Accuracy: 0.9070\n",
      "SVC Test Accuracy: 0.9101\n",
      "SVC Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.83      0.83        72\n",
      "           1       0.99      0.95      0.97       106\n",
      "           2       0.90      0.92      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.90      0.90      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n",
      "\n",
      "--- Training Random Forest classifier ---\n",
      "Random Forest Train Accuracy: 1.0000\n",
      "Random Forest Validation Accuracy: 0.9070\n",
      "Random Forest Test Accuracy: 0.9246\n",
      "Random Forest Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.87        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.93      0.91      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.92      0.93      0.92       345\n",
      "weighted avg       0.93      0.92      0.93       345\n",
      "\n",
      "\n",
      "--- Training LightGBM classifier ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 130702\n",
      "[LightGBM] [Info] Number of data points in the train set: 1605, number of used features: 561\n",
      "[LightGBM] [Info] Start training from score -1.572737\n",
      "[LightGBM] [Info] Start training from score -1.180370\n",
      "[LightGBM] [Info] Start training from score -0.722868\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM Train Accuracy: 1.0000\n",
      "LightGBM Validation Accuracy: 0.9070\n",
      "LightGBM Test Accuracy: 0.9246\n",
      "LightGBM Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86        72\n",
      "           1       0.99      0.96      0.98       106\n",
      "           2       0.92      0.92      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.92      0.92      0.92       345\n",
      "weighted avg       0.93      0.92      0.93       345\n",
      "\n",
      "\n",
      "--- Training XGBoost classifier ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [01:04:21] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Train Accuracy: 1.0000\n",
      "XGBoost Validation Accuracy: 0.9099\n",
      "XGBoost Test Accuracy: 0.9188\n",
      "XGBoost Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.85        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.92      0.91      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.91      0.92      0.91       345\n",
      "weighted avg       0.92      0.92      0.92       345\n",
      "\n",
      "\n",
      "--- Training CatBoost classifier ---\n",
      "0:\tlearn: 0.9855420\ttest: 0.9937841\tbest: 0.9937841 (0)\ttotal: 47.7ms\tremaining: 23.8s\n",
      "100:\tlearn: 0.1009919\ttest: 0.2497234\tbest: 0.2497234 (100)\ttotal: 4.76s\tremaining: 18.8s\n",
      "200:\tlearn: 0.0582766\ttest: 0.2402720\tbest: 0.2402720 (200)\ttotal: 9.47s\tremaining: 14.1s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.239189467\n",
      "bestIteration = 220\n",
      "\n",
      "Shrink model to first 221 iterations.\n",
      "CatBoost Train Accuracy: 0.9938\n",
      "CatBoost Validation Accuracy: 0.9070\n",
      "CatBoost Test Accuracy: 0.9072\n",
      "CatBoost Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.83      0.83        72\n",
      "           1       0.98      0.95      0.97       106\n",
      "           2       0.90      0.91      0.90       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.90      0.90      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load Features (for each modality)\n",
    "##########################################\n",
    "# Load DeiT features (expected shape: (num_samples, 768))\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")\n",
    "\n",
    "# Load FTTransformer features (expected shape: (num_samples, 192))\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")\n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "train_data = pd.read_csv(\"train_data_4.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_4.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_4.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create a Custom Dataset for Multi-Modal Data\n",
    "##########################################\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, deit_features, ft_features, labels):\n",
    "        self.deit_features = torch.tensor(deit_features, dtype=torch.float32)\n",
    "        self.ft_features = torch.tensor(ft_features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.deit_features[idx], self.ft_features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = MultiModalDataset(deit_train, ft_train, train_labels)\n",
    "val_dataset   = MultiModalDataset(deit_val, ft_val, val_labels)\n",
    "test_dataset  = MultiModalDataset(deit_test, ft_test, test_labels)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define the Intermediate Fusion Model for DeiT and FTTransformer Features\n",
    "##########################################\n",
    "class IntermediateFusionModel(nn.Module):\n",
    "    def __init__(self, ft_input_size=192, deit_input_size=768, num_classes=3):\n",
    "        super(IntermediateFusionModel, self).__init__()\n",
    "        # FTTransformer branch: we keep the original 192 dimensions (optionally normalized).\n",
    "        self.ft_branch = nn.Sequential(\n",
    "            nn.BatchNorm1d(ft_input_size)\n",
    "        )\n",
    "        # DeiT branch: reduce from 768 to 320.\n",
    "        self.deit_branch = nn.Sequential(\n",
    "            nn.Linear(deit_input_size, 384),\n",
    "            nn.BatchNorm1d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        # Fusion: concatenate FT (192) and reduced DeiT (320) to form a 512-dim vector.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(576, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, deit_features, ft_features):\n",
    "        # Process each modality.\n",
    "        ft_out = self.ft_branch(ft_features)         # (batch_size, 192)\n",
    "        deit_out = self.deit_branch(deit_features)     # (batch_size, 320)\n",
    "        # Concatenate the outputs.\n",
    "        fused_features = torch.cat([ft_out, deit_out], dim=1)  # (batch_size, 512)\n",
    "        logits = self.classifier(fused_features)       # (batch_size, num_classes)\n",
    "        return logits, fused_features\n",
    "\n",
    "# Initialize the model.\n",
    "fusion_model = IntermediateFusionModel(ft_input_size=ft_train.shape[1],\n",
    "                                         deit_input_size=deit_train.shape[1],\n",
    "                                         num_classes=num_classes)\n",
    "fusion_model.to(device)\n",
    "\n",
    "##########################################\n",
    "# 5. Train the Fusion Model with Early Stopping\n",
    "##########################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(fusion_model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "early_stopping_patience = 5\n",
    "epochs_no_improve = 0\n",
    "best_val_loss = np.Inf\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    fusion_model.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_train_correct = 0\n",
    "    for deit_feat, ft_feat, labels in train_loader:\n",
    "        deit_feat, ft_feat, labels = deit_feat.to(device), ft_feat.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = fusion_model(deit_feat, ft_feat)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item() * labels.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total_train_correct += torch.sum(preds == labels.data)\n",
    "    train_loss = total_train_loss / len(train_dataset)\n",
    "    train_accuracy = total_train_correct.double() / len(train_dataset)\n",
    "    \n",
    "    fusion_model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for deit_feat, ft_feat, labels in val_loader:\n",
    "            deit_feat, ft_feat, labels = deit_feat.to(device), ft_feat.to(device), labels.to(device)\n",
    "            outputs, _ = fusion_model(deit_feat, ft_feat)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item() * labels.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(labels.cpu().numpy())\n",
    "    val_loss = total_val_loss / len(val_dataset)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    val_acc = accuracy_score(all_targets, all_preds)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(fusion_model.state_dict(), 'best_fusion_model.pth')\n",
    "        print(\"  -> Best model saved.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model.\n",
    "fusion_model.load_state_dict(torch.load('best_fusion_model.pth'))\n",
    "\n",
    "##########################################\n",
    "# 6b. Extract Fused Features Using the Fusion Model\n",
    "##########################################\n",
    "def extract_fused_features(dataloader, model, device):\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for deit_feat, ft_feat, labels in dataloader:\n",
    "            deit_feat, ft_feat = deit_feat.to(device), ft_feat.to(device)\n",
    "            _, fused = fusion_model(deit_feat, ft_feat)\n",
    "            all_features.append(fused.cpu())\n",
    "            all_labels.append(labels)\n",
    "    features = torch.cat(all_features, dim=0)\n",
    "    labels = torch.cat(all_labels, dim=0)\n",
    "    return features, labels\n",
    "\n",
    "fused_train, fused_train_labels = extract_fused_features(train_loader, fusion_model, device)\n",
    "fused_val, fused_val_labels = extract_fused_features(val_loader, fusion_model, device)\n",
    "fused_test, fused_test_labels = extract_fused_features(test_loader, fusion_model, device)\n",
    "\n",
    "print(\"Fused train features shape:\", fused_train.shape)  # Expected: (num_train_samples, 512)\n",
    "print(\"Fused val features shape:\", fused_val.shape)\n",
    "print(\"Fused test features shape:\", fused_test.shape)\n",
    "\n",
    "# Convert to NumPy arrays.\n",
    "X_train = fused_train.numpy()\n",
    "X_val   = fused_val.numpy()\n",
    "X_test  = fused_test.numpy()\n",
    "y_train = fused_train_labels.numpy()\n",
    "y_val   = fused_val_labels.numpy()\n",
    "y_test  = fused_test_labels.numpy()\n",
    "\n",
    "##########################################\n",
    "# 7. Normalize the Fused Features Using StandardScaler\n",
    "##########################################\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val   = scaler.transform(X_val)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "##########################################\n",
    "# 8. Train and Evaluate Multiple Classifiers on the Fused Features\n",
    "##########################################\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "classifiers = {\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"SVC\": SVC(kernel=\"rbf\", probability=True, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(\n",
    "                    iterations=500,\n",
    "                    learning_rate=0.1,\n",
    "                    depth=6,\n",
    "                    loss_function='MultiClass',\n",
    "                    verbose=100,\n",
    "                    random_seed=42\n",
    "                )\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\n--- Training {name} classifier ---\")\n",
    "    if name == \"CatBoost\":\n",
    "        clf.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)\n",
    "    else:\n",
    "        clf.fit(X_train, y_train)\n",
    "    \n",
    "    train_preds = clf.predict(X_train)\n",
    "    val_preds = clf.predict(X_val)\n",
    "    test_preds = clf.predict(X_test)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, train_preds)\n",
    "    val_acc = accuracy_score(y_val, val_preds)\n",
    "    test_acc = accuracy_score(y_test, test_preds)\n",
    "    \n",
    "    print(f\"{name} Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"{name} Validation Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"{name} Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"{name} Classification Report on Test Data:\")\n",
    "    print(classification_report(y_test, test_preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dff5ae3f-b987-4f45-b7bb-270cb5fd9eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Classes: ['AD' 'CN' 'MCI']\n",
      "Fused train features shape: torch.Size([1605, 512])\n",
      "Fused val features shape: torch.Size([344, 512])\n",
      "Fused test features shape: torch.Size([345, 512])\n",
      "\n",
      "--- Training MLP classifier ---\n",
      "MLP Train Accuracy: 1.0000\n",
      "MLP Validation Accuracy: 0.8924\n",
      "MLP Test Accuracy: 0.9188\n",
      "MLP Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85        72\n",
      "           1       0.99      0.96      0.98       106\n",
      "           2       0.92      0.91      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.91      0.92      0.91       345\n",
      "weighted avg       0.92      0.92      0.92       345\n",
      "\n",
      "\n",
      "--- Training KNN classifier ---\n",
      "KNN Train Accuracy: 0.9209\n",
      "KNN Validation Accuracy: 0.8866\n",
      "KNN Test Accuracy: 0.9101\n",
      "KNN Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.78      0.82        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.89      0.93      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.91      0.89      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n",
      "\n",
      "--- Training Logistic Regression classifier ---\n",
      "Logistic Regression Train Accuracy: 1.0000\n",
      "Logistic Regression Validation Accuracy: 0.8866\n",
      "Logistic Regression Test Accuracy: 0.8928\n",
      "Logistic Regression Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.83      0.79        72\n",
      "           1       0.98      0.95      0.97       106\n",
      "           2       0.90      0.88      0.89       167\n",
      "\n",
      "    accuracy                           0.89       345\n",
      "   macro avg       0.88      0.89      0.88       345\n",
      "weighted avg       0.90      0.89      0.89       345\n",
      "\n",
      "\n",
      "--- Training SVC classifier ---\n",
      "SVC Train Accuracy: 0.9632\n",
      "SVC Validation Accuracy: 0.8866\n",
      "SVC Test Accuracy: 0.9072\n",
      "SVC Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83        72\n",
      "           1       0.98      0.94      0.96       106\n",
      "           2       0.89      0.92      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.90      0.90      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n",
      "\n",
      "--- Training Random Forest classifier ---\n",
      "Random Forest Train Accuracy: 1.0000\n",
      "Random Forest Validation Accuracy: 0.9041\n",
      "Random Forest Test Accuracy: 0.9304\n",
      "Random Forest Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.88        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.93      0.93      0.93       167\n",
      "\n",
      "    accuracy                           0.93       345\n",
      "   macro avg       0.92      0.93      0.93       345\n",
      "weighted avg       0.93      0.93      0.93       345\n",
      "\n",
      "\n",
      "--- Training LightGBM classifier ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009589 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 87395\n",
      "[LightGBM] [Info] Number of data points in the train set: 1605, number of used features: 442\n",
      "[LightGBM] [Info] Start training from score -1.572737\n",
      "[LightGBM] [Info] Start training from score -1.180370\n",
      "[LightGBM] [Info] Start training from score -0.722868\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM Train Accuracy: 1.0000\n",
      "LightGBM Validation Accuracy: 0.9157\n",
      "LightGBM Test Accuracy: 0.9217\n",
      "LightGBM Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.85        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.91      0.93      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.92      0.91      0.91       345\n",
      "weighted avg       0.92      0.92      0.92       345\n",
      "\n",
      "\n",
      "--- Training XGBoost classifier ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [01:08:19] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Train Accuracy: 1.0000\n",
      "XGBoost Validation Accuracy: 0.9041\n",
      "XGBoost Test Accuracy: 0.9159\n",
      "XGBoost Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.92      0.90      0.91       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.91      0.91      0.91       345\n",
      "weighted avg       0.92      0.92      0.92       345\n",
      "\n",
      "\n",
      "--- Training CatBoost classifier ---\n",
      "0:\tlearn: 0.9782777\ttest: 0.9826815\tbest: 0.9826815 (0)\ttotal: 33.1ms\tremaining: 16.5s\n",
      "100:\tlearn: 0.1845579\ttest: 0.2996454\tbest: 0.2996454 (100)\ttotal: 3.37s\tremaining: 13.3s\n",
      "200:\tlearn: 0.1232633\ttest: 0.2781022\tbest: 0.2776363 (194)\ttotal: 6.7s\tremaining: 9.96s\n",
      "300:\tlearn: 0.0895497\ttest: 0.2661649\tbest: 0.2660595 (280)\ttotal: 10s\tremaining: 6.63s\n",
      "400:\tlearn: 0.0677859\ttest: 0.2636245\tbest: 0.2626179 (383)\ttotal: 13.4s\tremaining: 3.3s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.2626178796\n",
      "bestIteration = 383\n",
      "\n",
      "Shrink model to first 384 iterations.\n",
      "CatBoost Train Accuracy: 0.9938\n",
      "CatBoost Validation Accuracy: 0.8953\n",
      "CatBoost Test Accuracy: 0.9275\n",
      "CatBoost Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88        72\n",
      "           1       0.98      0.95      0.97       106\n",
      "           2       0.93      0.92      0.92       167\n",
      "\n",
      "    accuracy                           0.93       345\n",
      "   macro avg       0.92      0.93      0.92       345\n",
      "weighted avg       0.93      0.93      0.93       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load Precomputed Features (for each modality)\n",
    "##########################################\n",
    "# DeiT features (expected shape: [num_samples, 768])\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")\n",
    "\n",
    "# FTTransformer features (expected shape: [num_samples, 192])\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")\n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "train_data = pd.read_csv(\"train_data_4.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_4.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_4.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create a Custom Dataset for Fusion\n",
    "##########################################\n",
    "class FusionDataset(Dataset):\n",
    "    def __init__(self, deit_features, ft_features, labels):\n",
    "        self.deit_features = torch.tensor(deit_features, dtype=torch.float32)\n",
    "        self.ft_features = torch.tensor(ft_features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.deit_features[idx], self.ft_features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = FusionDataset(deit_train, ft_train, train_labels)\n",
    "val_dataset   = FusionDataset(deit_val, ft_val, val_labels)\n",
    "test_dataset  = FusionDataset(deit_test, ft_test, test_labels)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)  # no shuffle needed for feature extraction\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define the Simple Fusion Module\n",
    "##########################################\n",
    "class SimpleFusion(nn.Module):\n",
    "    def __init__(self, deit_input_size=768, ft_input_size=192):\n",
    "        super(SimpleFusion, self).__init__()\n",
    "        # Reduce DeiT features from 768 to 384.\n",
    "        self.deit_fc = nn.Sequential(\n",
    "            nn.Linear(deit_input_size, 384),\n",
    "            nn.BatchNorm1d(384),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Reduce FTTransformer features from 192 to 128.\n",
    "        self.ft_fc = nn.Sequential(\n",
    "            nn.Linear(ft_input_size, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, deit_features, ft_features):\n",
    "        # Pass through FC layers.\n",
    "        deit_out = self.deit_fc(deit_features)  # (batch_size, 384)\n",
    "        ft_out = self.ft_fc(ft_features)          # (batch_size, 128)\n",
    "        # Concatenate the reduced features.\n",
    "        fused_features = torch.cat([deit_out, ft_out], dim=1)  # (batch_size, 512)\n",
    "        return fused_features\n",
    "\n",
    "# Initialize the fusion module.\n",
    "fusion_module = SimpleFusion(deit_input_size=deit_train.shape[1],\n",
    "                               ft_input_size=ft_train.shape[1])\n",
    "fusion_module.to(device)\n",
    "\n",
    "##########################################\n",
    "# 5. Extract Fused Features\n",
    "##########################################\n",
    "def extract_fused_features(dataloader, model, device):\n",
    "    model.eval()\n",
    "    all_fused = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for deit_feat, ft_feat, labels in dataloader:\n",
    "            deit_feat = deit_feat.to(device)\n",
    "            ft_feat = ft_feat.to(device)\n",
    "            fused = model(deit_feat, ft_feat)  # shape: (batch_size, 512)\n",
    "            all_fused.append(fused.cpu())\n",
    "            all_labels.append(labels)\n",
    "    fused_features = torch.cat(all_fused, dim=0)\n",
    "    labels = torch.cat(all_labels, dim=0)\n",
    "    return fused_features, labels\n",
    "\n",
    "fused_train, fused_train_labels = extract_fused_features(train_loader, fusion_module, device)\n",
    "fused_val, fused_val_labels = extract_fused_features(val_loader, fusion_module, device)\n",
    "fused_test, fused_test_labels = extract_fused_features(test_loader, fusion_module, device)\n",
    "\n",
    "print(\"Fused train features shape:\", fused_train.shape)  # Expected: (num_train_samples, 512)\n",
    "print(\"Fused val features shape:\", fused_val.shape)\n",
    "print(\"Fused test features shape:\", fused_test.shape)\n",
    "\n",
    "# Convert fused features and labels to NumPy arrays.\n",
    "X_train = fused_train.numpy()\n",
    "X_val = fused_val.numpy()\n",
    "X_test = fused_test.numpy()\n",
    "y_train = fused_train_labels.numpy()\n",
    "y_val = fused_val_labels.numpy()\n",
    "y_test = fused_test_labels.numpy()\n",
    "\n",
    "##########################################\n",
    "# 6. Normalize the Fused Features\n",
    "##########################################\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "##########################################\n",
    "# 7. Train and Evaluate Multiple Classifiers on the Fused Features\n",
    "##########################################\n",
    "classifiers = {\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"SVC\": SVC(kernel=\"rbf\", probability=True, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(\n",
    "                    iterations=500,\n",
    "                    learning_rate=0.1,\n",
    "                    depth=6,\n",
    "                    loss_function='MultiClass',\n",
    "                    verbose=100,\n",
    "                    random_seed=42\n",
    "                )\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\n--- Training {name} classifier ---\")\n",
    "    if name == \"CatBoost\":\n",
    "        clf.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)\n",
    "    else:\n",
    "        clf.fit(X_train, y_train)\n",
    "    \n",
    "    train_preds = clf.predict(X_train)\n",
    "    val_preds = clf.predict(X_val)\n",
    "    test_preds = clf.predict(X_test)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, train_preds)\n",
    "    val_acc = accuracy_score(y_val, val_preds)\n",
    "    test_acc = accuracy_score(y_test, test_preds)\n",
    "    \n",
    "    print(f\"{name} Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"{name} Validation Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"{name} Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"{name} Classification Report on Test Data:\")\n",
    "    print(classification_report(y_test, test_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1346d4db-2f33-4095-8ead-5b2e9bd9aea9",
   "metadata": {},
   "source": [
    "## Mid Fusion Best Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "54147c8a-6c21-442a-9c51-1ef6d2dd52ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "DeiT train features shape: (1605, 768)\n",
      "FTTransformer train features shape: (1605, 192)\n",
      "Classes: ['AD' 'CN' 'MCI']\n",
      "Fused train features shape: torch.Size([1605, 512])\n",
      "Fused val features shape: torch.Size([344, 512])\n",
      "Fused test features shape: torch.Size([345, 512])\n",
      "\n",
      "--- Training MLP classifier ---\n",
      "MLP Train Accuracy: 1.0000\n",
      "MLP Validation Accuracy: 0.9012\n",
      "MLP Test Accuracy: 0.9101\n",
      "MLP Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.82        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.89      0.93      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.91      0.89      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n",
      "\n",
      "--- Training KNN classifier ---\n",
      "KNN Train Accuracy: 0.9277\n",
      "KNN Validation Accuracy: 0.8779\n",
      "KNN Test Accuracy: 0.9072\n",
      "KNN Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.81      0.82        72\n",
      "           1       0.98      0.95      0.97       106\n",
      "           2       0.89      0.92      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.90      0.89      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n",
      "\n",
      "--- Training Logistic Regression classifier ---\n",
      "Logistic Regression Train Accuracy: 0.9570\n",
      "Logistic Regression Validation Accuracy: 0.8895\n",
      "Logistic Regression Test Accuracy: 0.9014\n",
      "Logistic Regression Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80        72\n",
      "           1       0.99      0.95      0.97       106\n",
      "           2       0.88      0.92      0.90       167\n",
      "\n",
      "    accuracy                           0.90       345\n",
      "   macro avg       0.90      0.89      0.89       345\n",
      "weighted avg       0.90      0.90      0.90       345\n",
      "\n",
      "\n",
      "--- Training SVC classifier ---\n",
      "SVC Train Accuracy: 0.9109\n",
      "SVC Validation Accuracy: 0.8721\n",
      "SVC Test Accuracy: 0.9159\n",
      "SVC Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85        72\n",
      "           1       0.98      0.95      0.97       106\n",
      "           2       0.91      0.92      0.91       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.91      0.91      0.91       345\n",
      "weighted avg       0.92      0.92      0.92       345\n",
      "\n",
      "\n",
      "--- Training Random Forest classifier ---\n",
      "Random Forest Train Accuracy: 1.0000\n",
      "Random Forest Validation Accuracy: 0.9070\n",
      "Random Forest Test Accuracy: 0.9304\n",
      "Random Forest Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.88        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.93      0.93      0.93       167\n",
      "\n",
      "    accuracy                           0.93       345\n",
      "   macro avg       0.92      0.93      0.93       345\n",
      "weighted avg       0.93      0.93      0.93       345\n",
      "\n",
      "\n",
      "--- Training LightGBM classifier ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009920 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 85908\n",
      "[LightGBM] [Info] Number of data points in the train set: 1605, number of used features: 443\n",
      "[LightGBM] [Info] Start training from score -1.572737\n",
      "[LightGBM] [Info] Start training from score -1.180370\n",
      "[LightGBM] [Info] Start training from score -0.722868\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM Train Accuracy: 1.0000\n",
      "LightGBM Validation Accuracy: 0.9012\n",
      "LightGBM Test Accuracy: 0.9362\n",
      "LightGBM Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.86      0.89        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.92      0.95      0.94       167\n",
      "\n",
      "    accuracy                           0.94       345\n",
      "   macro avg       0.94      0.93      0.93       345\n",
      "weighted avg       0.94      0.94      0.94       345\n",
      "\n",
      "\n",
      "--- Training XGBoost classifier ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sran-m36/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [03:42:42] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Train Accuracy: 1.0000\n",
      "XGBoost Validation Accuracy: 0.9041\n",
      "XGBoost Test Accuracy: 0.9217\n",
      "XGBoost Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86        72\n",
      "           1       0.98      0.96      0.97       106\n",
      "           2       0.92      0.92      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.91      0.92      0.92       345\n",
      "weighted avg       0.92      0.92      0.92       345\n",
      "\n",
      "\n",
      "--- Training CatBoost classifier ---\n",
      "0:\tlearn: 0.9768554\ttest: 0.9899042\tbest: 0.9899042 (0)\ttotal: 33ms\tremaining: 16.5s\n",
      "100:\tlearn: 0.1858623\ttest: 0.3090965\tbest: 0.3090520 (96)\ttotal: 3.37s\tremaining: 13.3s\n",
      "200:\tlearn: 0.1251310\ttest: 0.2869832\tbest: 0.2869832 (200)\ttotal: 6.7s\tremaining: 9.97s\n",
      "300:\tlearn: 0.0905925\ttest: 0.2777175\tbest: 0.2775941 (298)\ttotal: 10s\tremaining: 6.64s\n",
      "400:\tlearn: 0.0690426\ttest: 0.2720659\tbest: 0.2717252 (399)\ttotal: 13.4s\tremaining: 3.3s\n",
      "499:\tlearn: 0.0540719\ttest: 0.2703894\tbest: 0.2700196 (484)\ttotal: 16.7s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.2700195608\n",
      "bestIteration = 484\n",
      "\n",
      "Shrink model to first 485 iterations.\n",
      "CatBoost Train Accuracy: 0.9981\n",
      "CatBoost Validation Accuracy: 0.8983\n",
      "CatBoost Test Accuracy: 0.9188\n",
      "CatBoost Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.89      0.86        72\n",
      "           1       0.98      0.95      0.97       106\n",
      "           2       0.92      0.91      0.92       167\n",
      "\n",
      "    accuracy                           0.92       345\n",
      "   macro avg       0.91      0.92      0.91       345\n",
      "weighted avg       0.92      0.92      0.92       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "##########################################\n",
    "# 1. Load Precomputed Features (for each modality)\n",
    "##########################################\n",
    "# DeiT features (expected shape: [num_samples, 768])\n",
    "deit_train = np.load(\"Deit_train_features_1.npy\")\n",
    "deit_val   = np.load(\"Deit_val_features_1.npy\")\n",
    "deit_test  = np.load(\"Deit_test_features_1.npy\")\n",
    "\n",
    "# FTTransformer features (expected shape: [num_samples, 192])\n",
    "ft_train = np.load(\"ft_train_embeddings.npy\")\n",
    "ft_val   = np.load(\"ft_val_embeddings.npy\")\n",
    "ft_test  = np.load(\"ft_test_embeddings.npy\")\n",
    "\n",
    "print(\"DeiT train features shape:\", deit_train.shape)\n",
    "print(\"FTTransformer train features shape:\", ft_train.shape)\n",
    "\n",
    "##########################################\n",
    "# 2. Load Labels (from CSV files)\n",
    "##########################################\n",
    "train_data = pd.read_csv(\"train_data_4.csv\")\n",
    "val_data   = pd.read_csv(\"val_data_4.csv\")\n",
    "test_data  = pd.read_csv(\"test_data_4.csv\")\n",
    "\n",
    "label_col = \"Group\"\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[label_col])\n",
    "val_labels   = label_encoder.transform(val_data[label_col])\n",
    "test_labels  = label_encoder.transform(test_data[label_col])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "\n",
    "##########################################\n",
    "# 3. Create a Custom Dataset for Fusion\n",
    "##########################################\n",
    "class FusionDataset(Dataset):\n",
    "    def __init__(self, deit_features, ft_features, labels):\n",
    "        self.deit_features = torch.tensor(deit_features, dtype=torch.float32)\n",
    "        self.ft_features = torch.tensor(ft_features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.deit_features[idx], self.ft_features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = FusionDataset(deit_train, ft_train, train_labels)\n",
    "val_dataset   = FusionDataset(deit_val, ft_val, val_labels)\n",
    "test_dataset  = FusionDataset(deit_test, ft_test, test_labels)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)  # no shuffle needed for feature extraction\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##########################################\n",
    "# 4. Define the Simple Fusion Module\n",
    "##########################################\n",
    "class SimpleFusion(nn.Module):\n",
    "    def __init__(self, deit_input_size=768, ft_input_size=192):\n",
    "        super(SimpleFusion, self).__init__()\n",
    "        # Reduce DeiT features from 768 to 384.\n",
    "        self.deit_fc = nn.Sequential(\n",
    "            nn.Linear(deit_input_size, 384),\n",
    "            nn.BatchNorm1d(384),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Reduce FTTransformer features from 192 to 128.\n",
    "        self.ft_fc = nn.Sequential(\n",
    "            nn.Linear(ft_input_size, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, deit_features, ft_features):\n",
    "        # Pass through FC layers.\n",
    "        deit_out = self.deit_fc(deit_features)  # (batch_size, 384)\n",
    "        ft_out = self.ft_fc(ft_features)          # (batch_size, 128)\n",
    "        # Concatenate the reduced features.\n",
    "        fused_features = torch.cat([deit_out, ft_out], dim=1)  # (batch_size, 512)\n",
    "        return fused_features\n",
    "\n",
    "# Initialize the fusion module.\n",
    "fusion_module = SimpleFusion(deit_input_size=deit_train.shape[1],\n",
    "                               ft_input_size=ft_train.shape[1])\n",
    "fusion_module.to(device)\n",
    "\n",
    "##########################################\n",
    "# 5. Extract Fused Features\n",
    "##########################################\n",
    "def extract_fused_features(dataloader, model, device):\n",
    "    model.eval()\n",
    "    all_fused = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for deit_feat, ft_feat, labels in dataloader:\n",
    "            deit_feat = deit_feat.to(device)\n",
    "            ft_feat = ft_feat.to(device)\n",
    "            fused = model(deit_feat, ft_feat)  # shape: (batch_size, 512)\n",
    "            all_fused.append(fused.cpu())\n",
    "            all_labels.append(labels)\n",
    "    fused_features = torch.cat(all_fused, dim=0)\n",
    "    labels = torch.cat(all_labels, dim=0)\n",
    "    return fused_features, labels\n",
    "\n",
    "fused_train, fused_train_labels = extract_fused_features(train_loader, fusion_module, device)\n",
    "fused_val, fused_val_labels = extract_fused_features(val_loader, fusion_module, device)\n",
    "fused_test, fused_test_labels = extract_fused_features(test_loader, fusion_module, device)\n",
    "\n",
    "print(\"Fused train features shape:\", fused_train.shape)  # Expected: (num_train_samples, 512)\n",
    "print(\"Fused val features shape:\", fused_val.shape)\n",
    "print(\"Fused test features shape:\", fused_test.shape)\n",
    "\n",
    "# Convert fused features and labels to NumPy arrays.\n",
    "X_train = fused_train.numpy()\n",
    "X_val = fused_val.numpy()\n",
    "X_test = fused_test.numpy()\n",
    "y_train = fused_train_labels.numpy()\n",
    "y_val = fused_val_labels.numpy()\n",
    "y_test = fused_test_labels.numpy()\n",
    "\n",
    "##########################################\n",
    "# 6. Normalize the Fused Features\n",
    "##########################################\n",
    "\n",
    "##########################################\n",
    "# 7. Train and Evaluate Multiple Classifiers on the Fused Features\n",
    "##########################################\n",
    "classifiers = {\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"SVC\": SVC(kernel=\"rbf\", probability=True, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(\n",
    "                    iterations=500,\n",
    "                    learning_rate=0.1,\n",
    "                    depth=6,\n",
    "                    loss_function='MultiClass',\n",
    "                    verbose=100,\n",
    "                    random_seed=42\n",
    "                )\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\n--- Training {name} classifier ---\")\n",
    "    if name == \"CatBoost\":\n",
    "        clf.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)\n",
    "    else:\n",
    "        clf.fit(X_train, y_train)\n",
    "    \n",
    "    train_preds = clf.predict(X_train)\n",
    "    val_preds = clf.predict(X_val)\n",
    "    test_preds = clf.predict(X_test)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, train_preds)\n",
    "    val_acc = accuracy_score(y_val, val_preds)\n",
    "    test_acc = accuracy_score(y_test, test_preds)\n",
    "    \n",
    "    print(f\"{name} Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"{name} Validation Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"{name} Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"{name} Classification Report on Test Data:\")\n",
    "    print(classification_report(y_test, test_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a35ec106-14eb-43db-ad44-dfc28b7d5d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused train features shape: torch.Size([1605, 512])\n",
      "Fused val features shape: torch.Size([344, 512])\n",
      "Fused test features shape: torch.Size([345, 512])\n",
      "Saved mid_train, mid_val, and mid_test features.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 5. Extract Fused Features Using the Fusion Model\n",
    "# ---------------------------\n",
    "def extract_fused_features(dataloader, model, device):\n",
    "    \"\"\"\n",
    "    Pass data through the fusion model to extract fused features.\n",
    "    We ignore the final classifier outputs and use the fused features from the concatenation.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_fused = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for deit_feat, ft_feat, labels in dataloader:\n",
    "            deit_feat = deit_feat.to(device)\n",
    "            ft_feat = ft_feat.to(device)\n",
    "            fused = fusion_module(deit_feat, ft_feat)  # fused features shape: (batch_size, 512)\n",
    "            all_fused.append(fused.cpu())\n",
    "            all_labels.append(labels)\n",
    "    fused_features = torch.cat(all_fused, dim=0)\n",
    "    labels = torch.cat(all_labels, dim=0)\n",
    "    return fused_features, labels\n",
    "\n",
    "fused_train, fused_train_labels = extract_fused_features(train_loader, fusion_module, device)\n",
    "fused_val, fused_val_labels = extract_fused_features(val_loader, fusion_module, device)\n",
    "fused_test, fused_test_labels = extract_fused_features(test_loader, fusion_module, device)\n",
    "\n",
    "print(\"Fused train features shape:\", fused_train.shape)  # Expected: (num_train_samples, 512)\n",
    "print(\"Fused val features shape:\", fused_val.shape)\n",
    "print(\"Fused test features shape:\", fused_test.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# Save the Reduced Dimensional (Fused) Features\n",
    "# ---------------------------\n",
    "# Convert fused features and labels to NumPy arrays.\n",
    "mid_train = fused_train.numpy()\n",
    "mid_val   = fused_val.numpy()\n",
    "mid_test  = fused_test.numpy()\n",
    "y_train   = fused_train_labels.numpy()\n",
    "y_val     = fused_val_labels.numpy()\n",
    "y_test    = fused_test_labels.numpy()\n",
    "\n",
    "# Save these arrays to disk for later use by the TSC classifier.\n",
    "np.save(\"mid_train.npy\", mid_train)\n",
    "np.save(\"mid_val.npy\", mid_val)\n",
    "np.save(\"mid_test.npy\", mid_test)\n",
    "\n",
    "print(\"Saved mid_train, mid_val, and mid_test features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccc8a0d-c268-4a70-9a22-705572d36231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c62975-a6a3-43fd-9f59-362baa3e6e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f90401-a4d8-406f-827b-69210431feb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## End of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550b8040-dc9b-4d51-a26e-8659879326cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3f2b3f-b67b-4fff-899c-c3e8cae79487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7f99cf79-e3b0-472b-a361-68ef24b8c920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance matrix shape: (1605, 1605)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# ---------------------------\n",
    "# Provided: Converted fused features and labels from FTTransformer.\n",
    "# These are NumPy arrays with:\n",
    "# X_train: shape (1605, 192)\n",
    "# X_val:   shape (344, 192)\n",
    "# X_test:  shape (345, 192)\n",
    "# y_train, y_val, y_test: raw labels (e.g., \"CN\", \"MCI\", \"AD\")\n",
    "# ---------------------------\n",
    "\n",
    "# Step 1: Normalize the features (L2 normalization)\n",
    "X_train_norm = X_train / np.linalg.norm(X_train, axis=1, keepdims=True)\n",
    "X_val_norm   = X_val   / np.linalg.norm(X_val, axis=1, keepdims=True)\n",
    "X_test_norm  = X_test  / np.linalg.norm(X_test, axis=1, keepdims=True)\n",
    "\n",
    "# Step 2: Compute the cosine similarity matrix for the training set.\n",
    "train_cos_sim = cosine_similarity(X_train_norm)  # shape: (num_train, num_train)\n",
    "# Cosine distance = 1 - cosine similarity.\n",
    "train_cos_dist = 1 - train_cos_sim\n",
    "\n",
    "print(\"Cosine distance matrix shape:\", train_cos_dist.shape)\n",
    "\n",
    "# Step 3: Plot a histogram of the cosine distances (using the upper triangle, excluding the diagonal)\n",
    "n = train_cos_dist.shape[0]\n",
    "upper_tri_indices = np.triu_indices(n, k=1)\n",
    "dist_values = train_cos_dist[upper_tri_indices]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(dist_values, bins=50, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Cosine Distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Cosine Distances (Training Set)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "21334b6c-5047-45c7-8e31-646b008a7b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tolerance classes formed: 1411\n",
      "Tolerance Class 1: 1 samples\n",
      "Tolerance Class 2: 7 samples\n",
      "Tolerance Class 3: 1 samples\n",
      "Tolerance Class 4: 1 samples\n",
      "Tolerance Class 5: 1 samples\n",
      "Tolerance Class 6: 1 samples\n",
      "Tolerance Class 7: 1 samples\n",
      "Tolerance Class 8: 1 samples\n",
      "Tolerance Class 9: 2 samples\n",
      "Tolerance Class 10: 1 samples\n",
      "Tolerance Class 11: 2 samples\n",
      "Tolerance Class 12: 2 samples\n",
      "Tolerance Class 13: 1 samples\n",
      "Tolerance Class 14: 1 samples\n",
      "Tolerance Class 15: 1 samples\n",
      "Tolerance Class 16: 1 samples\n",
      "Tolerance Class 17: 1 samples\n",
      "Tolerance Class 18: 1 samples\n",
      "Tolerance Class 19: 1 samples\n",
      "Tolerance Class 20: 1 samples\n",
      "Tolerance Class 21: 1 samples\n",
      "Tolerance Class 22: 1 samples\n",
      "Tolerance Class 23: 1 samples\n",
      "Tolerance Class 24: 2 samples\n",
      "Tolerance Class 25: 1 samples\n",
      "Tolerance Class 26: 1 samples\n",
      "Tolerance Class 27: 1 samples\n",
      "Tolerance Class 28: 1 samples\n",
      "Tolerance Class 29: 1 samples\n",
      "Tolerance Class 30: 1 samples\n",
      "Tolerance Class 31: 2 samples\n",
      "Tolerance Class 32: 1 samples\n",
      "Tolerance Class 33: 1 samples\n",
      "Tolerance Class 34: 1 samples\n",
      "Tolerance Class 35: 1 samples\n",
      "Tolerance Class 36: 2 samples\n",
      "Tolerance Class 37: 1 samples\n",
      "Tolerance Class 38: 1 samples\n",
      "Tolerance Class 39: 1 samples\n",
      "Tolerance Class 40: 1 samples\n",
      "Tolerance Class 41: 2 samples\n",
      "Tolerance Class 42: 1 samples\n",
      "Tolerance Class 43: 1 samples\n",
      "Tolerance Class 44: 1 samples\n",
      "Tolerance Class 45: 1 samples\n",
      "Tolerance Class 46: 1 samples\n",
      "Tolerance Class 47: 1 samples\n",
      "Tolerance Class 48: 2 samples\n",
      "Tolerance Class 49: 1 samples\n",
      "Tolerance Class 50: 2 samples\n",
      "Tolerance Class 51: 1 samples\n",
      "Tolerance Class 52: 1 samples\n",
      "Tolerance Class 53: 1 samples\n",
      "Tolerance Class 54: 1 samples\n",
      "Tolerance Class 55: 2 samples\n",
      "Tolerance Class 56: 1 samples\n",
      "Tolerance Class 57: 2 samples\n",
      "Tolerance Class 58: 1 samples\n",
      "Tolerance Class 59: 1 samples\n",
      "Tolerance Class 60: 1 samples\n",
      "Tolerance Class 61: 1 samples\n",
      "Tolerance Class 62: 1 samples\n",
      "Tolerance Class 63: 2 samples\n",
      "Tolerance Class 64: 1 samples\n",
      "Tolerance Class 65: 2 samples\n",
      "Tolerance Class 66: 2 samples\n",
      "Tolerance Class 67: 1 samples\n",
      "Tolerance Class 68: 1 samples\n",
      "Tolerance Class 69: 1 samples\n",
      "Tolerance Class 70: 1 samples\n",
      "Tolerance Class 71: 1 samples\n",
      "Tolerance Class 72: 1 samples\n",
      "Tolerance Class 73: 1 samples\n",
      "Tolerance Class 74: 1 samples\n",
      "Tolerance Class 75: 1 samples\n",
      "Tolerance Class 76: 1 samples\n",
      "Tolerance Class 77: 1 samples\n",
      "Tolerance Class 78: 2 samples\n",
      "Tolerance Class 79: 2 samples\n",
      "Tolerance Class 80: 1 samples\n",
      "Tolerance Class 81: 1 samples\n",
      "Tolerance Class 82: 2 samples\n",
      "Tolerance Class 83: 1 samples\n",
      "Tolerance Class 84: 2 samples\n",
      "Tolerance Class 85: 1 samples\n",
      "Tolerance Class 86: 1 samples\n",
      "Tolerance Class 87: 2 samples\n",
      "Tolerance Class 88: 1 samples\n",
      "Tolerance Class 89: 1 samples\n",
      "Tolerance Class 90: 1 samples\n",
      "Tolerance Class 91: 1 samples\n",
      "Tolerance Class 92: 1 samples\n",
      "Tolerance Class 93: 1 samples\n",
      "Tolerance Class 94: 1 samples\n",
      "Tolerance Class 95: 1 samples\n",
      "Tolerance Class 96: 1 samples\n",
      "Tolerance Class 97: 1 samples\n",
      "Tolerance Class 98: 1 samples\n",
      "Tolerance Class 99: 2 samples\n",
      "Tolerance Class 100: 1 samples\n",
      "Tolerance Class 101: 1 samples\n",
      "Tolerance Class 102: 1 samples\n",
      "Tolerance Class 103: 2 samples\n",
      "Tolerance Class 104: 1 samples\n",
      "Tolerance Class 105: 1 samples\n",
      "Tolerance Class 106: 2 samples\n",
      "Tolerance Class 107: 1 samples\n",
      "Tolerance Class 108: 1 samples\n",
      "Tolerance Class 109: 1 samples\n",
      "Tolerance Class 110: 2 samples\n",
      "Tolerance Class 111: 2 samples\n",
      "Tolerance Class 112: 2 samples\n",
      "Tolerance Class 113: 1 samples\n",
      "Tolerance Class 114: 1 samples\n",
      "Tolerance Class 115: 1 samples\n",
      "Tolerance Class 116: 1 samples\n",
      "Tolerance Class 117: 1 samples\n",
      "Tolerance Class 118: 1 samples\n",
      "Tolerance Class 119: 2 samples\n",
      "Tolerance Class 120: 1 samples\n",
      "Tolerance Class 121: 1 samples\n",
      "Tolerance Class 122: 1 samples\n",
      "Tolerance Class 123: 1 samples\n",
      "Tolerance Class 124: 2 samples\n",
      "Tolerance Class 125: 1 samples\n",
      "Tolerance Class 126: 1 samples\n",
      "Tolerance Class 127: 1 samples\n",
      "Tolerance Class 128: 1 samples\n",
      "Tolerance Class 129: 1 samples\n",
      "Tolerance Class 130: 1 samples\n",
      "Tolerance Class 131: 2 samples\n",
      "Tolerance Class 132: 2 samples\n",
      "Tolerance Class 133: 1 samples\n",
      "Tolerance Class 134: 2 samples\n",
      "Tolerance Class 135: 1 samples\n",
      "Tolerance Class 136: 1 samples\n",
      "Tolerance Class 137: 1 samples\n",
      "Tolerance Class 138: 1 samples\n",
      "Tolerance Class 139: 1 samples\n",
      "Tolerance Class 140: 1 samples\n",
      "Tolerance Class 141: 1 samples\n",
      "Tolerance Class 142: 1 samples\n",
      "Tolerance Class 143: 1 samples\n",
      "Tolerance Class 144: 1 samples\n",
      "Tolerance Class 145: 1 samples\n",
      "Tolerance Class 146: 2 samples\n",
      "Tolerance Class 147: 1 samples\n",
      "Tolerance Class 148: 1 samples\n",
      "Tolerance Class 149: 1 samples\n",
      "Tolerance Class 150: 1 samples\n",
      "Tolerance Class 151: 1 samples\n",
      "Tolerance Class 152: 1 samples\n",
      "Tolerance Class 153: 1 samples\n",
      "Tolerance Class 154: 1 samples\n",
      "Tolerance Class 155: 1 samples\n",
      "Tolerance Class 156: 1 samples\n",
      "Tolerance Class 157: 1 samples\n",
      "Tolerance Class 158: 1 samples\n",
      "Tolerance Class 159: 1 samples\n",
      "Tolerance Class 160: 1 samples\n",
      "Tolerance Class 161: 1 samples\n",
      "Tolerance Class 162: 1 samples\n",
      "Tolerance Class 163: 1 samples\n",
      "Tolerance Class 164: 1 samples\n",
      "Tolerance Class 165: 1 samples\n",
      "Tolerance Class 166: 2 samples\n",
      "Tolerance Class 167: 2 samples\n",
      "Tolerance Class 168: 1 samples\n",
      "Tolerance Class 169: 1 samples\n",
      "Tolerance Class 170: 1 samples\n",
      "Tolerance Class 171: 1 samples\n",
      "Tolerance Class 172: 1 samples\n",
      "Tolerance Class 173: 1 samples\n",
      "Tolerance Class 174: 1 samples\n",
      "Tolerance Class 175: 1 samples\n",
      "Tolerance Class 176: 1 samples\n",
      "Tolerance Class 177: 1 samples\n",
      "Tolerance Class 178: 1 samples\n",
      "Tolerance Class 179: 1 samples\n",
      "Tolerance Class 180: 1 samples\n",
      "Tolerance Class 181: 1 samples\n",
      "Tolerance Class 182: 1 samples\n",
      "Tolerance Class 183: 1 samples\n",
      "Tolerance Class 184: 2 samples\n",
      "Tolerance Class 185: 2 samples\n",
      "Tolerance Class 186: 1 samples\n",
      "Tolerance Class 187: 1 samples\n",
      "Tolerance Class 188: 2 samples\n",
      "Tolerance Class 189: 1 samples\n",
      "Tolerance Class 190: 2 samples\n",
      "Tolerance Class 191: 1 samples\n",
      "Tolerance Class 192: 1 samples\n",
      "Tolerance Class 193: 1 samples\n",
      "Tolerance Class 194: 1 samples\n",
      "Tolerance Class 195: 1 samples\n",
      "Tolerance Class 196: 2 samples\n",
      "Tolerance Class 197: 1 samples\n",
      "Tolerance Class 198: 1 samples\n",
      "Tolerance Class 199: 2 samples\n",
      "Tolerance Class 200: 1 samples\n",
      "Tolerance Class 201: 2 samples\n",
      "Tolerance Class 202: 1 samples\n",
      "Tolerance Class 203: 2 samples\n",
      "Tolerance Class 204: 1 samples\n",
      "Tolerance Class 205: 2 samples\n",
      "Tolerance Class 206: 1 samples\n",
      "Tolerance Class 207: 1 samples\n",
      "Tolerance Class 208: 1 samples\n",
      "Tolerance Class 209: 1 samples\n",
      "Tolerance Class 210: 2 samples\n",
      "Tolerance Class 211: 1 samples\n",
      "Tolerance Class 212: 2 samples\n",
      "Tolerance Class 213: 1 samples\n",
      "Tolerance Class 214: 2 samples\n",
      "Tolerance Class 215: 1 samples\n",
      "Tolerance Class 216: 1 samples\n",
      "Tolerance Class 217: 1 samples\n",
      "Tolerance Class 218: 1 samples\n",
      "Tolerance Class 219: 1 samples\n",
      "Tolerance Class 220: 1 samples\n",
      "Tolerance Class 221: 1 samples\n",
      "Tolerance Class 222: 2 samples\n",
      "Tolerance Class 223: 2 samples\n",
      "Tolerance Class 224: 2 samples\n",
      "Tolerance Class 225: 2 samples\n",
      "Tolerance Class 226: 1 samples\n",
      "Tolerance Class 227: 1 samples\n",
      "Tolerance Class 228: 1 samples\n",
      "Tolerance Class 229: 1 samples\n",
      "Tolerance Class 230: 1 samples\n",
      "Tolerance Class 231: 1 samples\n",
      "Tolerance Class 232: 1 samples\n",
      "Tolerance Class 233: 1 samples\n",
      "Tolerance Class 234: 2 samples\n",
      "Tolerance Class 235: 1 samples\n",
      "Tolerance Class 236: 1 samples\n",
      "Tolerance Class 237: 1 samples\n",
      "Tolerance Class 238: 1 samples\n",
      "Tolerance Class 239: 1 samples\n",
      "Tolerance Class 240: 2 samples\n",
      "Tolerance Class 241: 1 samples\n",
      "Tolerance Class 242: 1 samples\n",
      "Tolerance Class 243: 2 samples\n",
      "Tolerance Class 244: 2 samples\n",
      "Tolerance Class 245: 1 samples\n",
      "Tolerance Class 246: 1 samples\n",
      "Tolerance Class 247: 1 samples\n",
      "Tolerance Class 248: 1 samples\n",
      "Tolerance Class 249: 1 samples\n",
      "Tolerance Class 250: 1 samples\n",
      "Tolerance Class 251: 1 samples\n",
      "Tolerance Class 252: 1 samples\n",
      "Tolerance Class 253: 1 samples\n",
      "Tolerance Class 254: 1 samples\n",
      "Tolerance Class 255: 1 samples\n",
      "Tolerance Class 256: 2 samples\n",
      "Tolerance Class 257: 1 samples\n",
      "Tolerance Class 258: 1 samples\n",
      "Tolerance Class 259: 1 samples\n",
      "Tolerance Class 260: 1 samples\n",
      "Tolerance Class 261: 1 samples\n",
      "Tolerance Class 262: 1 samples\n",
      "Tolerance Class 263: 2 samples\n",
      "Tolerance Class 264: 1 samples\n",
      "Tolerance Class 265: 1 samples\n",
      "Tolerance Class 266: 1 samples\n",
      "Tolerance Class 267: 1 samples\n",
      "Tolerance Class 268: 1 samples\n",
      "Tolerance Class 269: 1 samples\n",
      "Tolerance Class 270: 1 samples\n",
      "Tolerance Class 271: 1 samples\n",
      "Tolerance Class 272: 1 samples\n",
      "Tolerance Class 273: 1 samples\n",
      "Tolerance Class 274: 1 samples\n",
      "Tolerance Class 275: 1 samples\n",
      "Tolerance Class 276: 2 samples\n",
      "Tolerance Class 277: 1 samples\n",
      "Tolerance Class 278: 1 samples\n",
      "Tolerance Class 279: 1 samples\n",
      "Tolerance Class 280: 1 samples\n",
      "Tolerance Class 281: 1 samples\n",
      "Tolerance Class 282: 1 samples\n",
      "Tolerance Class 283: 1 samples\n",
      "Tolerance Class 284: 2 samples\n",
      "Tolerance Class 285: 1 samples\n",
      "Tolerance Class 286: 1 samples\n",
      "Tolerance Class 287: 2 samples\n",
      "Tolerance Class 288: 1 samples\n",
      "Tolerance Class 289: 2 samples\n",
      "Tolerance Class 290: 1 samples\n",
      "Tolerance Class 291: 1 samples\n",
      "Tolerance Class 292: 1 samples\n",
      "Tolerance Class 293: 2 samples\n",
      "Tolerance Class 294: 1 samples\n",
      "Tolerance Class 295: 1 samples\n",
      "Tolerance Class 296: 1 samples\n",
      "Tolerance Class 297: 1 samples\n",
      "Tolerance Class 298: 1 samples\n",
      "Tolerance Class 299: 2 samples\n",
      "Tolerance Class 300: 1 samples\n",
      "Tolerance Class 301: 1 samples\n",
      "Tolerance Class 302: 1 samples\n",
      "Tolerance Class 303: 1 samples\n",
      "Tolerance Class 304: 1 samples\n",
      "Tolerance Class 305: 1 samples\n",
      "Tolerance Class 306: 1 samples\n",
      "Tolerance Class 307: 2 samples\n",
      "Tolerance Class 308: 1 samples\n",
      "Tolerance Class 309: 2 samples\n",
      "Tolerance Class 310: 1 samples\n",
      "Tolerance Class 311: 1 samples\n",
      "Tolerance Class 312: 1 samples\n",
      "Tolerance Class 313: 2 samples\n",
      "Tolerance Class 314: 1 samples\n",
      "Tolerance Class 315: 1 samples\n",
      "Tolerance Class 316: 1 samples\n",
      "Tolerance Class 317: 1 samples\n",
      "Tolerance Class 318: 1 samples\n",
      "Tolerance Class 319: 1 samples\n",
      "Tolerance Class 320: 1 samples\n",
      "Tolerance Class 321: 1 samples\n",
      "Tolerance Class 322: 1 samples\n",
      "Tolerance Class 323: 1 samples\n",
      "Tolerance Class 324: 1 samples\n",
      "Tolerance Class 325: 2 samples\n",
      "Tolerance Class 326: 1 samples\n",
      "Tolerance Class 327: 1 samples\n",
      "Tolerance Class 328: 2 samples\n",
      "Tolerance Class 329: 1 samples\n",
      "Tolerance Class 330: 1 samples\n",
      "Tolerance Class 331: 1 samples\n",
      "Tolerance Class 332: 1 samples\n",
      "Tolerance Class 333: 1 samples\n",
      "Tolerance Class 334: 1 samples\n",
      "Tolerance Class 335: 2 samples\n",
      "Tolerance Class 336: 1 samples\n",
      "Tolerance Class 337: 1 samples\n",
      "Tolerance Class 338: 1 samples\n",
      "Tolerance Class 339: 1 samples\n",
      "Tolerance Class 340: 1 samples\n",
      "Tolerance Class 341: 2 samples\n",
      "Tolerance Class 342: 2 samples\n",
      "Tolerance Class 343: 1 samples\n",
      "Tolerance Class 344: 1 samples\n",
      "Tolerance Class 345: 1 samples\n",
      "Tolerance Class 346: 1 samples\n",
      "Tolerance Class 347: 1 samples\n",
      "Tolerance Class 348: 1 samples\n",
      "Tolerance Class 349: 1 samples\n",
      "Tolerance Class 350: 2 samples\n",
      "Tolerance Class 351: 2 samples\n",
      "Tolerance Class 352: 1 samples\n",
      "Tolerance Class 353: 1 samples\n",
      "Tolerance Class 354: 1 samples\n",
      "Tolerance Class 355: 2 samples\n",
      "Tolerance Class 356: 2 samples\n",
      "Tolerance Class 357: 1 samples\n",
      "Tolerance Class 358: 1 samples\n",
      "Tolerance Class 359: 2 samples\n",
      "Tolerance Class 360: 1 samples\n",
      "Tolerance Class 361: 1 samples\n",
      "Tolerance Class 362: 2 samples\n",
      "Tolerance Class 363: 1 samples\n",
      "Tolerance Class 364: 2 samples\n",
      "Tolerance Class 365: 1 samples\n",
      "Tolerance Class 366: 1 samples\n",
      "Tolerance Class 367: 1 samples\n",
      "Tolerance Class 368: 1 samples\n",
      "Tolerance Class 369: 1 samples\n",
      "Tolerance Class 370: 1 samples\n",
      "Tolerance Class 371: 1 samples\n",
      "Tolerance Class 372: 1 samples\n",
      "Tolerance Class 373: 1 samples\n",
      "Tolerance Class 374: 1 samples\n",
      "Tolerance Class 375: 1 samples\n",
      "Tolerance Class 376: 1 samples\n",
      "Tolerance Class 377: 1 samples\n",
      "Tolerance Class 378: 1 samples\n",
      "Tolerance Class 379: 1 samples\n",
      "Tolerance Class 380: 1 samples\n",
      "Tolerance Class 381: 1 samples\n",
      "Tolerance Class 382: 2 samples\n",
      "Tolerance Class 383: 1 samples\n",
      "Tolerance Class 384: 1 samples\n",
      "Tolerance Class 385: 1 samples\n",
      "Tolerance Class 386: 2 samples\n",
      "Tolerance Class 387: 1 samples\n",
      "Tolerance Class 388: 1 samples\n",
      "Tolerance Class 389: 1 samples\n",
      "Tolerance Class 390: 1 samples\n",
      "Tolerance Class 391: 2 samples\n",
      "Tolerance Class 392: 1 samples\n",
      "Tolerance Class 393: 2 samples\n",
      "Tolerance Class 394: 1 samples\n",
      "Tolerance Class 395: 1 samples\n",
      "Tolerance Class 396: 1 samples\n",
      "Tolerance Class 397: 1 samples\n",
      "Tolerance Class 398: 1 samples\n",
      "Tolerance Class 399: 1 samples\n",
      "Tolerance Class 400: 1 samples\n",
      "Tolerance Class 401: 1 samples\n",
      "Tolerance Class 402: 1 samples\n",
      "Tolerance Class 403: 1 samples\n",
      "Tolerance Class 404: 2 samples\n",
      "Tolerance Class 405: 1 samples\n",
      "Tolerance Class 406: 1 samples\n",
      "Tolerance Class 407: 1 samples\n",
      "Tolerance Class 408: 1 samples\n",
      "Tolerance Class 409: 1 samples\n",
      "Tolerance Class 410: 1 samples\n",
      "Tolerance Class 411: 2 samples\n",
      "Tolerance Class 412: 1 samples\n",
      "Tolerance Class 413: 1 samples\n",
      "Tolerance Class 414: 1 samples\n",
      "Tolerance Class 415: 1 samples\n",
      "Tolerance Class 416: 1 samples\n",
      "Tolerance Class 417: 2 samples\n",
      "Tolerance Class 418: 1 samples\n",
      "Tolerance Class 419: 1 samples\n",
      "Tolerance Class 420: 1 samples\n",
      "Tolerance Class 421: 1 samples\n",
      "Tolerance Class 422: 1 samples\n",
      "Tolerance Class 423: 1 samples\n",
      "Tolerance Class 424: 1 samples\n",
      "Tolerance Class 425: 2 samples\n",
      "Tolerance Class 426: 1 samples\n",
      "Tolerance Class 427: 1 samples\n",
      "Tolerance Class 428: 1 samples\n",
      "Tolerance Class 429: 1 samples\n",
      "Tolerance Class 430: 2 samples\n",
      "Tolerance Class 431: 1 samples\n",
      "Tolerance Class 432: 2 samples\n",
      "Tolerance Class 433: 1 samples\n",
      "Tolerance Class 434: 1 samples\n",
      "Tolerance Class 435: 1 samples\n",
      "Tolerance Class 436: 1 samples\n",
      "Tolerance Class 437: 1 samples\n",
      "Tolerance Class 438: 1 samples\n",
      "Tolerance Class 439: 1 samples\n",
      "Tolerance Class 440: 1 samples\n",
      "Tolerance Class 441: 1 samples\n",
      "Tolerance Class 442: 1 samples\n",
      "Tolerance Class 443: 2 samples\n",
      "Tolerance Class 444: 1 samples\n",
      "Tolerance Class 445: 1 samples\n",
      "Tolerance Class 446: 1 samples\n",
      "Tolerance Class 447: 1 samples\n",
      "Tolerance Class 448: 1 samples\n",
      "Tolerance Class 449: 1 samples\n",
      "Tolerance Class 450: 1 samples\n",
      "Tolerance Class 451: 1 samples\n",
      "Tolerance Class 452: 1 samples\n",
      "Tolerance Class 453: 1 samples\n",
      "Tolerance Class 454: 1 samples\n",
      "Tolerance Class 455: 1 samples\n",
      "Tolerance Class 456: 2 samples\n",
      "Tolerance Class 457: 1 samples\n",
      "Tolerance Class 458: 1 samples\n",
      "Tolerance Class 459: 1 samples\n",
      "Tolerance Class 460: 1 samples\n",
      "Tolerance Class 461: 1 samples\n",
      "Tolerance Class 462: 2 samples\n",
      "Tolerance Class 463: 1 samples\n",
      "Tolerance Class 464: 1 samples\n",
      "Tolerance Class 465: 2 samples\n",
      "Tolerance Class 466: 2 samples\n",
      "Tolerance Class 467: 1 samples\n",
      "Tolerance Class 468: 1 samples\n",
      "Tolerance Class 469: 1 samples\n",
      "Tolerance Class 470: 2 samples\n",
      "Tolerance Class 471: 1 samples\n",
      "Tolerance Class 472: 1 samples\n",
      "Tolerance Class 473: 1 samples\n",
      "Tolerance Class 474: 1 samples\n",
      "Tolerance Class 475: 1 samples\n",
      "Tolerance Class 476: 1 samples\n",
      "Tolerance Class 477: 1 samples\n",
      "Tolerance Class 478: 1 samples\n",
      "Tolerance Class 479: 2 samples\n",
      "Tolerance Class 480: 1 samples\n",
      "Tolerance Class 481: 1 samples\n",
      "Tolerance Class 482: 1 samples\n",
      "Tolerance Class 483: 1 samples\n",
      "Tolerance Class 484: 1 samples\n",
      "Tolerance Class 485: 2 samples\n",
      "Tolerance Class 486: 1 samples\n",
      "Tolerance Class 487: 1 samples\n",
      "Tolerance Class 488: 1 samples\n",
      "Tolerance Class 489: 1 samples\n",
      "Tolerance Class 490: 2 samples\n",
      "Tolerance Class 491: 1 samples\n",
      "Tolerance Class 492: 1 samples\n",
      "Tolerance Class 493: 1 samples\n",
      "Tolerance Class 494: 1 samples\n",
      "Tolerance Class 495: 1 samples\n",
      "Tolerance Class 496: 1 samples\n",
      "Tolerance Class 497: 1 samples\n",
      "Tolerance Class 498: 1 samples\n",
      "Tolerance Class 499: 1 samples\n",
      "Tolerance Class 500: 2 samples\n",
      "Tolerance Class 501: 1 samples\n",
      "Tolerance Class 502: 1 samples\n",
      "Tolerance Class 503: 1 samples\n",
      "Tolerance Class 504: 1 samples\n",
      "Tolerance Class 505: 1 samples\n",
      "Tolerance Class 506: 1 samples\n",
      "Tolerance Class 507: 1 samples\n",
      "Tolerance Class 508: 1 samples\n",
      "Tolerance Class 509: 1 samples\n",
      "Tolerance Class 510: 1 samples\n",
      "Tolerance Class 511: 1 samples\n",
      "Tolerance Class 512: 1 samples\n",
      "Tolerance Class 513: 1 samples\n",
      "Tolerance Class 514: 1 samples\n",
      "Tolerance Class 515: 1 samples\n",
      "Tolerance Class 516: 1 samples\n",
      "Tolerance Class 517: 1 samples\n",
      "Tolerance Class 518: 1 samples\n",
      "Tolerance Class 519: 1 samples\n",
      "Tolerance Class 520: 2 samples\n",
      "Tolerance Class 521: 1 samples\n",
      "Tolerance Class 522: 1 samples\n",
      "Tolerance Class 523: 1 samples\n",
      "Tolerance Class 524: 1 samples\n",
      "Tolerance Class 525: 1 samples\n",
      "Tolerance Class 526: 1 samples\n",
      "Tolerance Class 527: 1 samples\n",
      "Tolerance Class 528: 2 samples\n",
      "Tolerance Class 529: 1 samples\n",
      "Tolerance Class 530: 1 samples\n",
      "Tolerance Class 531: 1 samples\n",
      "Tolerance Class 532: 1 samples\n",
      "Tolerance Class 533: 1 samples\n",
      "Tolerance Class 534: 1 samples\n",
      "Tolerance Class 535: 1 samples\n",
      "Tolerance Class 536: 1 samples\n",
      "Tolerance Class 537: 2 samples\n",
      "Tolerance Class 538: 1 samples\n",
      "Tolerance Class 539: 1 samples\n",
      "Tolerance Class 540: 1 samples\n",
      "Tolerance Class 541: 1 samples\n",
      "Tolerance Class 542: 1 samples\n",
      "Tolerance Class 543: 1 samples\n",
      "Tolerance Class 544: 1 samples\n",
      "Tolerance Class 545: 1 samples\n",
      "Tolerance Class 546: 1 samples\n",
      "Tolerance Class 547: 1 samples\n",
      "Tolerance Class 548: 1 samples\n",
      "Tolerance Class 549: 2 samples\n",
      "Tolerance Class 550: 1 samples\n",
      "Tolerance Class 551: 1 samples\n",
      "Tolerance Class 552: 1 samples\n",
      "Tolerance Class 553: 1 samples\n",
      "Tolerance Class 554: 1 samples\n",
      "Tolerance Class 555: 2 samples\n",
      "Tolerance Class 556: 1 samples\n",
      "Tolerance Class 557: 1 samples\n",
      "Tolerance Class 558: 1 samples\n",
      "Tolerance Class 559: 1 samples\n",
      "Tolerance Class 560: 2 samples\n",
      "Tolerance Class 561: 2 samples\n",
      "Tolerance Class 562: 1 samples\n",
      "Tolerance Class 563: 1 samples\n",
      "Tolerance Class 564: 1 samples\n",
      "Tolerance Class 565: 1 samples\n",
      "Tolerance Class 566: 2 samples\n",
      "Tolerance Class 567: 1 samples\n",
      "Tolerance Class 568: 2 samples\n",
      "Tolerance Class 569: 1 samples\n",
      "Tolerance Class 570: 1 samples\n",
      "Tolerance Class 571: 1 samples\n",
      "Tolerance Class 572: 1 samples\n",
      "Tolerance Class 573: 1 samples\n",
      "Tolerance Class 574: 1 samples\n",
      "Tolerance Class 575: 1 samples\n",
      "Tolerance Class 576: 2 samples\n",
      "Tolerance Class 577: 1 samples\n",
      "Tolerance Class 578: 1 samples\n",
      "Tolerance Class 579: 2 samples\n",
      "Tolerance Class 580: 2 samples\n",
      "Tolerance Class 581: 1 samples\n",
      "Tolerance Class 582: 1 samples\n",
      "Tolerance Class 583: 1 samples\n",
      "Tolerance Class 584: 1 samples\n",
      "Tolerance Class 585: 1 samples\n",
      "Tolerance Class 586: 1 samples\n",
      "Tolerance Class 587: 1 samples\n",
      "Tolerance Class 588: 1 samples\n",
      "Tolerance Class 589: 1 samples\n",
      "Tolerance Class 590: 1 samples\n",
      "Tolerance Class 591: 1 samples\n",
      "Tolerance Class 592: 2 samples\n",
      "Tolerance Class 593: 1 samples\n",
      "Tolerance Class 594: 1 samples\n",
      "Tolerance Class 595: 1 samples\n",
      "Tolerance Class 596: 1 samples\n",
      "Tolerance Class 597: 1 samples\n",
      "Tolerance Class 598: 1 samples\n",
      "Tolerance Class 599: 1 samples\n",
      "Tolerance Class 600: 1 samples\n",
      "Tolerance Class 601: 1 samples\n",
      "Tolerance Class 602: 1 samples\n",
      "Tolerance Class 603: 1 samples\n",
      "Tolerance Class 604: 1 samples\n",
      "Tolerance Class 605: 1 samples\n",
      "Tolerance Class 606: 1 samples\n",
      "Tolerance Class 607: 2 samples\n",
      "Tolerance Class 608: 1 samples\n",
      "Tolerance Class 609: 1 samples\n",
      "Tolerance Class 610: 1 samples\n",
      "Tolerance Class 611: 1 samples\n",
      "Tolerance Class 612: 1 samples\n",
      "Tolerance Class 613: 1 samples\n",
      "Tolerance Class 614: 2 samples\n",
      "Tolerance Class 615: 1 samples\n",
      "Tolerance Class 616: 1 samples\n",
      "Tolerance Class 617: 1 samples\n",
      "Tolerance Class 618: 1 samples\n",
      "Tolerance Class 619: 1 samples\n",
      "Tolerance Class 620: 2 samples\n",
      "Tolerance Class 621: 2 samples\n",
      "Tolerance Class 622: 1 samples\n",
      "Tolerance Class 623: 2 samples\n",
      "Tolerance Class 624: 1 samples\n",
      "Tolerance Class 625: 1 samples\n",
      "Tolerance Class 626: 1 samples\n",
      "Tolerance Class 627: 1 samples\n",
      "Tolerance Class 628: 1 samples\n",
      "Tolerance Class 629: 1 samples\n",
      "Tolerance Class 630: 1 samples\n",
      "Tolerance Class 631: 1 samples\n",
      "Tolerance Class 632: 1 samples\n",
      "Tolerance Class 633: 2 samples\n",
      "Tolerance Class 634: 2 samples\n",
      "Tolerance Class 635: 1 samples\n",
      "Tolerance Class 636: 1 samples\n",
      "Tolerance Class 637: 1 samples\n",
      "Tolerance Class 638: 1 samples\n",
      "Tolerance Class 639: 1 samples\n",
      "Tolerance Class 640: 2 samples\n",
      "Tolerance Class 641: 1 samples\n",
      "Tolerance Class 642: 1 samples\n",
      "Tolerance Class 643: 1 samples\n",
      "Tolerance Class 644: 1 samples\n",
      "Tolerance Class 645: 1 samples\n",
      "Tolerance Class 646: 1 samples\n",
      "Tolerance Class 647: 1 samples\n",
      "Tolerance Class 648: 1 samples\n",
      "Tolerance Class 649: 1 samples\n",
      "Tolerance Class 650: 1 samples\n",
      "Tolerance Class 651: 1 samples\n",
      "Tolerance Class 652: 1 samples\n",
      "Tolerance Class 653: 2 samples\n",
      "Tolerance Class 654: 2 samples\n",
      "Tolerance Class 655: 1 samples\n",
      "Tolerance Class 656: 1 samples\n",
      "Tolerance Class 657: 1 samples\n",
      "Tolerance Class 658: 1 samples\n",
      "Tolerance Class 659: 1 samples\n",
      "Tolerance Class 660: 2 samples\n",
      "Tolerance Class 661: 1 samples\n",
      "Tolerance Class 662: 1 samples\n",
      "Tolerance Class 663: 1 samples\n",
      "Tolerance Class 664: 1 samples\n",
      "Tolerance Class 665: 2 samples\n",
      "Tolerance Class 666: 1 samples\n",
      "Tolerance Class 667: 1 samples\n",
      "Tolerance Class 668: 1 samples\n",
      "Tolerance Class 669: 1 samples\n",
      "Tolerance Class 670: 1 samples\n",
      "Tolerance Class 671: 1 samples\n",
      "Tolerance Class 672: 1 samples\n",
      "Tolerance Class 673: 1 samples\n",
      "Tolerance Class 674: 1 samples\n",
      "Tolerance Class 675: 1 samples\n",
      "Tolerance Class 676: 1 samples\n",
      "Tolerance Class 677: 2 samples\n",
      "Tolerance Class 678: 1 samples\n",
      "Tolerance Class 679: 1 samples\n",
      "Tolerance Class 680: 1 samples\n",
      "Tolerance Class 681: 2 samples\n",
      "Tolerance Class 682: 1 samples\n",
      "Tolerance Class 683: 1 samples\n",
      "Tolerance Class 684: 1 samples\n",
      "Tolerance Class 685: 1 samples\n",
      "Tolerance Class 686: 1 samples\n",
      "Tolerance Class 687: 2 samples\n",
      "Tolerance Class 688: 2 samples\n",
      "Tolerance Class 689: 1 samples\n",
      "Tolerance Class 690: 1 samples\n",
      "Tolerance Class 691: 1 samples\n",
      "Tolerance Class 692: 1 samples\n",
      "Tolerance Class 693: 2 samples\n",
      "Tolerance Class 694: 1 samples\n",
      "Tolerance Class 695: 1 samples\n",
      "Tolerance Class 696: 1 samples\n",
      "Tolerance Class 697: 1 samples\n",
      "Tolerance Class 698: 1 samples\n",
      "Tolerance Class 699: 1 samples\n",
      "Tolerance Class 700: 1 samples\n",
      "Tolerance Class 701: 1 samples\n",
      "Tolerance Class 702: 1 samples\n",
      "Tolerance Class 703: 2 samples\n",
      "Tolerance Class 704: 1 samples\n",
      "Tolerance Class 705: 1 samples\n",
      "Tolerance Class 706: 2 samples\n",
      "Tolerance Class 707: 1 samples\n",
      "Tolerance Class 708: 1 samples\n",
      "Tolerance Class 709: 1 samples\n",
      "Tolerance Class 710: 1 samples\n",
      "Tolerance Class 711: 1 samples\n",
      "Tolerance Class 712: 1 samples\n",
      "Tolerance Class 713: 1 samples\n",
      "Tolerance Class 714: 1 samples\n",
      "Tolerance Class 715: 1 samples\n",
      "Tolerance Class 716: 1 samples\n",
      "Tolerance Class 717: 1 samples\n",
      "Tolerance Class 718: 1 samples\n",
      "Tolerance Class 719: 1 samples\n",
      "Tolerance Class 720: 1 samples\n",
      "Tolerance Class 721: 1 samples\n",
      "Tolerance Class 722: 1 samples\n",
      "Tolerance Class 723: 1 samples\n",
      "Tolerance Class 724: 1 samples\n",
      "Tolerance Class 725: 1 samples\n",
      "Tolerance Class 726: 2 samples\n",
      "Tolerance Class 727: 2 samples\n",
      "Tolerance Class 728: 1 samples\n",
      "Tolerance Class 729: 1 samples\n",
      "Tolerance Class 730: 1 samples\n",
      "Tolerance Class 731: 1 samples\n",
      "Tolerance Class 732: 1 samples\n",
      "Tolerance Class 733: 1 samples\n",
      "Tolerance Class 734: 1 samples\n",
      "Tolerance Class 735: 1 samples\n",
      "Tolerance Class 736: 2 samples\n",
      "Tolerance Class 737: 1 samples\n",
      "Tolerance Class 738: 1 samples\n",
      "Tolerance Class 739: 1 samples\n",
      "Tolerance Class 740: 1 samples\n",
      "Tolerance Class 741: 1 samples\n",
      "Tolerance Class 742: 1 samples\n",
      "Tolerance Class 743: 2 samples\n",
      "Tolerance Class 744: 1 samples\n",
      "Tolerance Class 745: 1 samples\n",
      "Tolerance Class 746: 1 samples\n",
      "Tolerance Class 747: 2 samples\n",
      "Tolerance Class 748: 1 samples\n",
      "Tolerance Class 749: 1 samples\n",
      "Tolerance Class 750: 1 samples\n",
      "Tolerance Class 751: 1 samples\n",
      "Tolerance Class 752: 1 samples\n",
      "Tolerance Class 753: 1 samples\n",
      "Tolerance Class 754: 1 samples\n",
      "Tolerance Class 755: 2 samples\n",
      "Tolerance Class 756: 2 samples\n",
      "Tolerance Class 757: 1 samples\n",
      "Tolerance Class 758: 2 samples\n",
      "Tolerance Class 759: 1 samples\n",
      "Tolerance Class 760: 1 samples\n",
      "Tolerance Class 761: 2 samples\n",
      "Tolerance Class 762: 1 samples\n",
      "Tolerance Class 763: 1 samples\n",
      "Tolerance Class 764: 1 samples\n",
      "Tolerance Class 765: 2 samples\n",
      "Tolerance Class 766: 1 samples\n",
      "Tolerance Class 767: 1 samples\n",
      "Tolerance Class 768: 2 samples\n",
      "Tolerance Class 769: 1 samples\n",
      "Tolerance Class 770: 1 samples\n",
      "Tolerance Class 771: 1 samples\n",
      "Tolerance Class 772: 1 samples\n",
      "Tolerance Class 773: 1 samples\n",
      "Tolerance Class 774: 1 samples\n",
      "Tolerance Class 775: 1 samples\n",
      "Tolerance Class 776: 1 samples\n",
      "Tolerance Class 777: 1 samples\n",
      "Tolerance Class 778: 1 samples\n",
      "Tolerance Class 779: 1 samples\n",
      "Tolerance Class 780: 1 samples\n",
      "Tolerance Class 781: 2 samples\n",
      "Tolerance Class 782: 1 samples\n",
      "Tolerance Class 783: 1 samples\n",
      "Tolerance Class 784: 1 samples\n",
      "Tolerance Class 785: 1 samples\n",
      "Tolerance Class 786: 1 samples\n",
      "Tolerance Class 787: 1 samples\n",
      "Tolerance Class 788: 1 samples\n",
      "Tolerance Class 789: 1 samples\n",
      "Tolerance Class 790: 1 samples\n",
      "Tolerance Class 791: 1 samples\n",
      "Tolerance Class 792: 1 samples\n",
      "Tolerance Class 793: 1 samples\n",
      "Tolerance Class 794: 1 samples\n",
      "Tolerance Class 795: 1 samples\n",
      "Tolerance Class 796: 1 samples\n",
      "Tolerance Class 797: 1 samples\n",
      "Tolerance Class 798: 2 samples\n",
      "Tolerance Class 799: 1 samples\n",
      "Tolerance Class 800: 1 samples\n",
      "Tolerance Class 801: 1 samples\n",
      "Tolerance Class 802: 1 samples\n",
      "Tolerance Class 803: 2 samples\n",
      "Tolerance Class 804: 2 samples\n",
      "Tolerance Class 805: 1 samples\n",
      "Tolerance Class 806: 2 samples\n",
      "Tolerance Class 807: 1 samples\n",
      "Tolerance Class 808: 1 samples\n",
      "Tolerance Class 809: 1 samples\n",
      "Tolerance Class 810: 1 samples\n",
      "Tolerance Class 811: 1 samples\n",
      "Tolerance Class 812: 2 samples\n",
      "Tolerance Class 813: 1 samples\n",
      "Tolerance Class 814: 1 samples\n",
      "Tolerance Class 815: 1 samples\n",
      "Tolerance Class 816: 2 samples\n",
      "Tolerance Class 817: 1 samples\n",
      "Tolerance Class 818: 1 samples\n",
      "Tolerance Class 819: 1 samples\n",
      "Tolerance Class 820: 1 samples\n",
      "Tolerance Class 821: 1 samples\n",
      "Tolerance Class 822: 2 samples\n",
      "Tolerance Class 823: 1 samples\n",
      "Tolerance Class 824: 1 samples\n",
      "Tolerance Class 825: 1 samples\n",
      "Tolerance Class 826: 1 samples\n",
      "Tolerance Class 827: 1 samples\n",
      "Tolerance Class 828: 1 samples\n",
      "Tolerance Class 829: 1 samples\n",
      "Tolerance Class 830: 1 samples\n",
      "Tolerance Class 831: 1 samples\n",
      "Tolerance Class 832: 2 samples\n",
      "Tolerance Class 833: 2 samples\n",
      "Tolerance Class 834: 1 samples\n",
      "Tolerance Class 835: 1 samples\n",
      "Tolerance Class 836: 1 samples\n",
      "Tolerance Class 837: 1 samples\n",
      "Tolerance Class 838: 1 samples\n",
      "Tolerance Class 839: 1 samples\n",
      "Tolerance Class 840: 1 samples\n",
      "Tolerance Class 841: 1 samples\n",
      "Tolerance Class 842: 1 samples\n",
      "Tolerance Class 843: 1 samples\n",
      "Tolerance Class 844: 1 samples\n",
      "Tolerance Class 845: 2 samples\n",
      "Tolerance Class 846: 1 samples\n",
      "Tolerance Class 847: 1 samples\n",
      "Tolerance Class 848: 1 samples\n",
      "Tolerance Class 849: 1 samples\n",
      "Tolerance Class 850: 1 samples\n",
      "Tolerance Class 851: 1 samples\n",
      "Tolerance Class 852: 2 samples\n",
      "Tolerance Class 853: 1 samples\n",
      "Tolerance Class 854: 1 samples\n",
      "Tolerance Class 855: 1 samples\n",
      "Tolerance Class 856: 1 samples\n",
      "Tolerance Class 857: 1 samples\n",
      "Tolerance Class 858: 1 samples\n",
      "Tolerance Class 859: 2 samples\n",
      "Tolerance Class 860: 2 samples\n",
      "Tolerance Class 861: 1 samples\n",
      "Tolerance Class 862: 1 samples\n",
      "Tolerance Class 863: 1 samples\n",
      "Tolerance Class 864: 1 samples\n",
      "Tolerance Class 865: 1 samples\n",
      "Tolerance Class 866: 1 samples\n",
      "Tolerance Class 867: 1 samples\n",
      "Tolerance Class 868: 1 samples\n",
      "Tolerance Class 869: 1 samples\n",
      "Tolerance Class 870: 1 samples\n",
      "Tolerance Class 871: 1 samples\n",
      "Tolerance Class 872: 2 samples\n",
      "Tolerance Class 873: 1 samples\n",
      "Tolerance Class 874: 1 samples\n",
      "Tolerance Class 875: 1 samples\n",
      "Tolerance Class 876: 1 samples\n",
      "Tolerance Class 877: 1 samples\n",
      "Tolerance Class 878: 1 samples\n",
      "Tolerance Class 879: 1 samples\n",
      "Tolerance Class 880: 1 samples\n",
      "Tolerance Class 881: 2 samples\n",
      "Tolerance Class 882: 1 samples\n",
      "Tolerance Class 883: 1 samples\n",
      "Tolerance Class 884: 1 samples\n",
      "Tolerance Class 885: 1 samples\n",
      "Tolerance Class 886: 1 samples\n",
      "Tolerance Class 887: 1 samples\n",
      "Tolerance Class 888: 1 samples\n",
      "Tolerance Class 889: 1 samples\n",
      "Tolerance Class 890: 1 samples\n",
      "Tolerance Class 891: 1 samples\n",
      "Tolerance Class 892: 1 samples\n",
      "Tolerance Class 893: 1 samples\n",
      "Tolerance Class 894: 1 samples\n",
      "Tolerance Class 895: 1 samples\n",
      "Tolerance Class 896: 1 samples\n",
      "Tolerance Class 897: 2 samples\n",
      "Tolerance Class 898: 1 samples\n",
      "Tolerance Class 899: 2 samples\n",
      "Tolerance Class 900: 1 samples\n",
      "Tolerance Class 901: 1 samples\n",
      "Tolerance Class 902: 1 samples\n",
      "Tolerance Class 903: 1 samples\n",
      "Tolerance Class 904: 1 samples\n",
      "Tolerance Class 905: 1 samples\n",
      "Tolerance Class 906: 1 samples\n",
      "Tolerance Class 907: 1 samples\n",
      "Tolerance Class 908: 1 samples\n",
      "Tolerance Class 909: 1 samples\n",
      "Tolerance Class 910: 1 samples\n",
      "Tolerance Class 911: 1 samples\n",
      "Tolerance Class 912: 2 samples\n",
      "Tolerance Class 913: 1 samples\n",
      "Tolerance Class 914: 1 samples\n",
      "Tolerance Class 915: 1 samples\n",
      "Tolerance Class 916: 1 samples\n",
      "Tolerance Class 917: 1 samples\n",
      "Tolerance Class 918: 1 samples\n",
      "Tolerance Class 919: 1 samples\n",
      "Tolerance Class 920: 1 samples\n",
      "Tolerance Class 921: 1 samples\n",
      "Tolerance Class 922: 1 samples\n",
      "Tolerance Class 923: 1 samples\n",
      "Tolerance Class 924: 1 samples\n",
      "Tolerance Class 925: 1 samples\n",
      "Tolerance Class 926: 1 samples\n",
      "Tolerance Class 927: 1 samples\n",
      "Tolerance Class 928: 1 samples\n",
      "Tolerance Class 929: 2 samples\n",
      "Tolerance Class 930: 2 samples\n",
      "Tolerance Class 931: 1 samples\n",
      "Tolerance Class 932: 1 samples\n",
      "Tolerance Class 933: 1 samples\n",
      "Tolerance Class 934: 1 samples\n",
      "Tolerance Class 935: 2 samples\n",
      "Tolerance Class 936: 1 samples\n",
      "Tolerance Class 937: 1 samples\n",
      "Tolerance Class 938: 1 samples\n",
      "Tolerance Class 939: 1 samples\n",
      "Tolerance Class 940: 1 samples\n",
      "Tolerance Class 941: 1 samples\n",
      "Tolerance Class 942: 1 samples\n",
      "Tolerance Class 943: 1 samples\n",
      "Tolerance Class 944: 1 samples\n",
      "Tolerance Class 945: 1 samples\n",
      "Tolerance Class 946: 1 samples\n",
      "Tolerance Class 947: 1 samples\n",
      "Tolerance Class 948: 1 samples\n",
      "Tolerance Class 949: 1 samples\n",
      "Tolerance Class 950: 1 samples\n",
      "Tolerance Class 951: 1 samples\n",
      "Tolerance Class 952: 1 samples\n",
      "Tolerance Class 953: 1 samples\n",
      "Tolerance Class 954: 1 samples\n",
      "Tolerance Class 955: 1 samples\n",
      "Tolerance Class 956: 1 samples\n",
      "Tolerance Class 957: 1 samples\n",
      "Tolerance Class 958: 1 samples\n",
      "Tolerance Class 959: 2 samples\n",
      "Tolerance Class 960: 1 samples\n",
      "Tolerance Class 961: 1 samples\n",
      "Tolerance Class 962: 1 samples\n",
      "Tolerance Class 963: 2 samples\n",
      "Tolerance Class 964: 1 samples\n",
      "Tolerance Class 965: 1 samples\n",
      "Tolerance Class 966: 1 samples\n",
      "Tolerance Class 967: 1 samples\n",
      "Tolerance Class 968: 1 samples\n",
      "Tolerance Class 969: 1 samples\n",
      "Tolerance Class 970: 1 samples\n",
      "Tolerance Class 971: 1 samples\n",
      "Tolerance Class 972: 1 samples\n",
      "Tolerance Class 973: 1 samples\n",
      "Tolerance Class 974: 1 samples\n",
      "Tolerance Class 975: 1 samples\n",
      "Tolerance Class 976: 1 samples\n",
      "Tolerance Class 977: 1 samples\n",
      "Tolerance Class 978: 1 samples\n",
      "Tolerance Class 979: 1 samples\n",
      "Tolerance Class 980: 1 samples\n",
      "Tolerance Class 981: 1 samples\n",
      "Tolerance Class 982: 1 samples\n",
      "Tolerance Class 983: 1 samples\n",
      "Tolerance Class 984: 1 samples\n",
      "Tolerance Class 985: 1 samples\n",
      "Tolerance Class 986: 1 samples\n",
      "Tolerance Class 987: 1 samples\n",
      "Tolerance Class 988: 1 samples\n",
      "Tolerance Class 989: 1 samples\n",
      "Tolerance Class 990: 1 samples\n",
      "Tolerance Class 991: 1 samples\n",
      "Tolerance Class 992: 2 samples\n",
      "Tolerance Class 993: 1 samples\n",
      "Tolerance Class 994: 1 samples\n",
      "Tolerance Class 995: 1 samples\n",
      "Tolerance Class 996: 1 samples\n",
      "Tolerance Class 997: 1 samples\n",
      "Tolerance Class 998: 1 samples\n",
      "Tolerance Class 999: 1 samples\n",
      "Tolerance Class 1000: 1 samples\n",
      "Tolerance Class 1001: 1 samples\n",
      "Tolerance Class 1002: 1 samples\n",
      "Tolerance Class 1003: 1 samples\n",
      "Tolerance Class 1004: 2 samples\n",
      "Tolerance Class 1005: 1 samples\n",
      "Tolerance Class 1006: 1 samples\n",
      "Tolerance Class 1007: 1 samples\n",
      "Tolerance Class 1008: 1 samples\n",
      "Tolerance Class 1009: 1 samples\n",
      "Tolerance Class 1010: 1 samples\n",
      "Tolerance Class 1011: 2 samples\n",
      "Tolerance Class 1012: 1 samples\n",
      "Tolerance Class 1013: 1 samples\n",
      "Tolerance Class 1014: 2 samples\n",
      "Tolerance Class 1015: 1 samples\n",
      "Tolerance Class 1016: 1 samples\n",
      "Tolerance Class 1017: 1 samples\n",
      "Tolerance Class 1018: 1 samples\n",
      "Tolerance Class 1019: 1 samples\n",
      "Tolerance Class 1020: 1 samples\n",
      "Tolerance Class 1021: 1 samples\n",
      "Tolerance Class 1022: 1 samples\n",
      "Tolerance Class 1023: 1 samples\n",
      "Tolerance Class 1024: 1 samples\n",
      "Tolerance Class 1025: 1 samples\n",
      "Tolerance Class 1026: 2 samples\n",
      "Tolerance Class 1027: 1 samples\n",
      "Tolerance Class 1028: 1 samples\n",
      "Tolerance Class 1029: 1 samples\n",
      "Tolerance Class 1030: 1 samples\n",
      "Tolerance Class 1031: 1 samples\n",
      "Tolerance Class 1032: 1 samples\n",
      "Tolerance Class 1033: 1 samples\n",
      "Tolerance Class 1034: 1 samples\n",
      "Tolerance Class 1035: 1 samples\n",
      "Tolerance Class 1036: 1 samples\n",
      "Tolerance Class 1037: 1 samples\n",
      "Tolerance Class 1038: 1 samples\n",
      "Tolerance Class 1039: 1 samples\n",
      "Tolerance Class 1040: 1 samples\n",
      "Tolerance Class 1041: 2 samples\n",
      "Tolerance Class 1042: 1 samples\n",
      "Tolerance Class 1043: 1 samples\n",
      "Tolerance Class 1044: 1 samples\n",
      "Tolerance Class 1045: 1 samples\n",
      "Tolerance Class 1046: 1 samples\n",
      "Tolerance Class 1047: 1 samples\n",
      "Tolerance Class 1048: 1 samples\n",
      "Tolerance Class 1049: 1 samples\n",
      "Tolerance Class 1050: 1 samples\n",
      "Tolerance Class 1051: 1 samples\n",
      "Tolerance Class 1052: 1 samples\n",
      "Tolerance Class 1053: 1 samples\n",
      "Tolerance Class 1054: 1 samples\n",
      "Tolerance Class 1055: 1 samples\n",
      "Tolerance Class 1056: 1 samples\n",
      "Tolerance Class 1057: 1 samples\n",
      "Tolerance Class 1058: 1 samples\n",
      "Tolerance Class 1059: 1 samples\n",
      "Tolerance Class 1060: 1 samples\n",
      "Tolerance Class 1061: 1 samples\n",
      "Tolerance Class 1062: 1 samples\n",
      "Tolerance Class 1063: 1 samples\n",
      "Tolerance Class 1064: 1 samples\n",
      "Tolerance Class 1065: 1 samples\n",
      "Tolerance Class 1066: 1 samples\n",
      "Tolerance Class 1067: 1 samples\n",
      "Tolerance Class 1068: 1 samples\n",
      "Tolerance Class 1069: 1 samples\n",
      "Tolerance Class 1070: 1 samples\n",
      "Tolerance Class 1071: 2 samples\n",
      "Tolerance Class 1072: 1 samples\n",
      "Tolerance Class 1073: 1 samples\n",
      "Tolerance Class 1074: 1 samples\n",
      "Tolerance Class 1075: 1 samples\n",
      "Tolerance Class 1076: 1 samples\n",
      "Tolerance Class 1077: 1 samples\n",
      "Tolerance Class 1078: 2 samples\n",
      "Tolerance Class 1079: 1 samples\n",
      "Tolerance Class 1080: 1 samples\n",
      "Tolerance Class 1081: 2 samples\n",
      "Tolerance Class 1082: 1 samples\n",
      "Tolerance Class 1083: 1 samples\n",
      "Tolerance Class 1084: 1 samples\n",
      "Tolerance Class 1085: 1 samples\n",
      "Tolerance Class 1086: 1 samples\n",
      "Tolerance Class 1087: 1 samples\n",
      "Tolerance Class 1088: 1 samples\n",
      "Tolerance Class 1089: 1 samples\n",
      "Tolerance Class 1090: 2 samples\n",
      "Tolerance Class 1091: 1 samples\n",
      "Tolerance Class 1092: 1 samples\n",
      "Tolerance Class 1093: 1 samples\n",
      "Tolerance Class 1094: 1 samples\n",
      "Tolerance Class 1095: 1 samples\n",
      "Tolerance Class 1096: 1 samples\n",
      "Tolerance Class 1097: 1 samples\n",
      "Tolerance Class 1098: 1 samples\n",
      "Tolerance Class 1099: 1 samples\n",
      "Tolerance Class 1100: 1 samples\n",
      "Tolerance Class 1101: 1 samples\n",
      "Tolerance Class 1102: 2 samples\n",
      "Tolerance Class 1103: 1 samples\n",
      "Tolerance Class 1104: 1 samples\n",
      "Tolerance Class 1105: 1 samples\n",
      "Tolerance Class 1106: 2 samples\n",
      "Tolerance Class 1107: 1 samples\n",
      "Tolerance Class 1108: 1 samples\n",
      "Tolerance Class 1109: 1 samples\n",
      "Tolerance Class 1110: 1 samples\n",
      "Tolerance Class 1111: 1 samples\n",
      "Tolerance Class 1112: 1 samples\n",
      "Tolerance Class 1113: 1 samples\n",
      "Tolerance Class 1114: 2 samples\n",
      "Tolerance Class 1115: 2 samples\n",
      "Tolerance Class 1116: 1 samples\n",
      "Tolerance Class 1117: 1 samples\n",
      "Tolerance Class 1118: 1 samples\n",
      "Tolerance Class 1119: 2 samples\n",
      "Tolerance Class 1120: 2 samples\n",
      "Tolerance Class 1121: 1 samples\n",
      "Tolerance Class 1122: 1 samples\n",
      "Tolerance Class 1123: 1 samples\n",
      "Tolerance Class 1124: 1 samples\n",
      "Tolerance Class 1125: 1 samples\n",
      "Tolerance Class 1126: 1 samples\n",
      "Tolerance Class 1127: 1 samples\n",
      "Tolerance Class 1128: 1 samples\n",
      "Tolerance Class 1129: 1 samples\n",
      "Tolerance Class 1130: 2 samples\n",
      "Tolerance Class 1131: 1 samples\n",
      "Tolerance Class 1132: 1 samples\n",
      "Tolerance Class 1133: 1 samples\n",
      "Tolerance Class 1134: 1 samples\n",
      "Tolerance Class 1135: 1 samples\n",
      "Tolerance Class 1136: 1 samples\n",
      "Tolerance Class 1137: 1 samples\n",
      "Tolerance Class 1138: 1 samples\n",
      "Tolerance Class 1139: 1 samples\n",
      "Tolerance Class 1140: 2 samples\n",
      "Tolerance Class 1141: 1 samples\n",
      "Tolerance Class 1142: 1 samples\n",
      "Tolerance Class 1143: 1 samples\n",
      "Tolerance Class 1144: 1 samples\n",
      "Tolerance Class 1145: 1 samples\n",
      "Tolerance Class 1146: 1 samples\n",
      "Tolerance Class 1147: 1 samples\n",
      "Tolerance Class 1148: 1 samples\n",
      "Tolerance Class 1149: 1 samples\n",
      "Tolerance Class 1150: 1 samples\n",
      "Tolerance Class 1151: 1 samples\n",
      "Tolerance Class 1152: 1 samples\n",
      "Tolerance Class 1153: 1 samples\n",
      "Tolerance Class 1154: 1 samples\n",
      "Tolerance Class 1155: 1 samples\n",
      "Tolerance Class 1156: 1 samples\n",
      "Tolerance Class 1157: 1 samples\n",
      "Tolerance Class 1158: 1 samples\n",
      "Tolerance Class 1159: 1 samples\n",
      "Tolerance Class 1160: 1 samples\n",
      "Tolerance Class 1161: 1 samples\n",
      "Tolerance Class 1162: 1 samples\n",
      "Tolerance Class 1163: 1 samples\n",
      "Tolerance Class 1164: 1 samples\n",
      "Tolerance Class 1165: 1 samples\n",
      "Tolerance Class 1166: 2 samples\n",
      "Tolerance Class 1167: 1 samples\n",
      "Tolerance Class 1168: 1 samples\n",
      "Tolerance Class 1169: 1 samples\n",
      "Tolerance Class 1170: 1 samples\n",
      "Tolerance Class 1171: 1 samples\n",
      "Tolerance Class 1172: 1 samples\n",
      "Tolerance Class 1173: 1 samples\n",
      "Tolerance Class 1174: 1 samples\n",
      "Tolerance Class 1175: 1 samples\n",
      "Tolerance Class 1176: 1 samples\n",
      "Tolerance Class 1177: 1 samples\n",
      "Tolerance Class 1178: 1 samples\n",
      "Tolerance Class 1179: 1 samples\n",
      "Tolerance Class 1180: 1 samples\n",
      "Tolerance Class 1181: 1 samples\n",
      "Tolerance Class 1182: 2 samples\n",
      "Tolerance Class 1183: 1 samples\n",
      "Tolerance Class 1184: 1 samples\n",
      "Tolerance Class 1185: 1 samples\n",
      "Tolerance Class 1186: 1 samples\n",
      "Tolerance Class 1187: 1 samples\n",
      "Tolerance Class 1188: 1 samples\n",
      "Tolerance Class 1189: 1 samples\n",
      "Tolerance Class 1190: 1 samples\n",
      "Tolerance Class 1191: 1 samples\n",
      "Tolerance Class 1192: 1 samples\n",
      "Tolerance Class 1193: 1 samples\n",
      "Tolerance Class 1194: 1 samples\n",
      "Tolerance Class 1195: 1 samples\n",
      "Tolerance Class 1196: 1 samples\n",
      "Tolerance Class 1197: 1 samples\n",
      "Tolerance Class 1198: 1 samples\n",
      "Tolerance Class 1199: 1 samples\n",
      "Tolerance Class 1200: 1 samples\n",
      "Tolerance Class 1201: 1 samples\n",
      "Tolerance Class 1202: 1 samples\n",
      "Tolerance Class 1203: 1 samples\n",
      "Tolerance Class 1204: 1 samples\n",
      "Tolerance Class 1205: 2 samples\n",
      "Tolerance Class 1206: 1 samples\n",
      "Tolerance Class 1207: 1 samples\n",
      "Tolerance Class 1208: 1 samples\n",
      "Tolerance Class 1209: 1 samples\n",
      "Tolerance Class 1210: 1 samples\n",
      "Tolerance Class 1211: 1 samples\n",
      "Tolerance Class 1212: 1 samples\n",
      "Tolerance Class 1213: 1 samples\n",
      "Tolerance Class 1214: 1 samples\n",
      "Tolerance Class 1215: 1 samples\n",
      "Tolerance Class 1216: 1 samples\n",
      "Tolerance Class 1217: 1 samples\n",
      "Tolerance Class 1218: 1 samples\n",
      "Tolerance Class 1219: 1 samples\n",
      "Tolerance Class 1220: 1 samples\n",
      "Tolerance Class 1221: 1 samples\n",
      "Tolerance Class 1222: 1 samples\n",
      "Tolerance Class 1223: 1 samples\n",
      "Tolerance Class 1224: 1 samples\n",
      "Tolerance Class 1225: 1 samples\n",
      "Tolerance Class 1226: 1 samples\n",
      "Tolerance Class 1227: 1 samples\n",
      "Tolerance Class 1228: 1 samples\n",
      "Tolerance Class 1229: 1 samples\n",
      "Tolerance Class 1230: 1 samples\n",
      "Tolerance Class 1231: 1 samples\n",
      "Tolerance Class 1232: 1 samples\n",
      "Tolerance Class 1233: 1 samples\n",
      "Tolerance Class 1234: 1 samples\n",
      "Tolerance Class 1235: 1 samples\n",
      "Tolerance Class 1236: 1 samples\n",
      "Tolerance Class 1237: 1 samples\n",
      "Tolerance Class 1238: 1 samples\n",
      "Tolerance Class 1239: 1 samples\n",
      "Tolerance Class 1240: 1 samples\n",
      "Tolerance Class 1241: 1 samples\n",
      "Tolerance Class 1242: 1 samples\n",
      "Tolerance Class 1243: 1 samples\n",
      "Tolerance Class 1244: 1 samples\n",
      "Tolerance Class 1245: 1 samples\n",
      "Tolerance Class 1246: 1 samples\n",
      "Tolerance Class 1247: 1 samples\n",
      "Tolerance Class 1248: 1 samples\n",
      "Tolerance Class 1249: 1 samples\n",
      "Tolerance Class 1250: 1 samples\n",
      "Tolerance Class 1251: 1 samples\n",
      "Tolerance Class 1252: 1 samples\n",
      "Tolerance Class 1253: 1 samples\n",
      "Tolerance Class 1254: 1 samples\n",
      "Tolerance Class 1255: 1 samples\n",
      "Tolerance Class 1256: 1 samples\n",
      "Tolerance Class 1257: 1 samples\n",
      "Tolerance Class 1258: 1 samples\n",
      "Tolerance Class 1259: 1 samples\n",
      "Tolerance Class 1260: 1 samples\n",
      "Tolerance Class 1261: 1 samples\n",
      "Tolerance Class 1262: 1 samples\n",
      "Tolerance Class 1263: 1 samples\n",
      "Tolerance Class 1264: 1 samples\n",
      "Tolerance Class 1265: 1 samples\n",
      "Tolerance Class 1266: 2 samples\n",
      "Tolerance Class 1267: 1 samples\n",
      "Tolerance Class 1268: 1 samples\n",
      "Tolerance Class 1269: 1 samples\n",
      "Tolerance Class 1270: 2 samples\n",
      "Tolerance Class 1271: 1 samples\n",
      "Tolerance Class 1272: 1 samples\n",
      "Tolerance Class 1273: 1 samples\n",
      "Tolerance Class 1274: 1 samples\n",
      "Tolerance Class 1275: 1 samples\n",
      "Tolerance Class 1276: 1 samples\n",
      "Tolerance Class 1277: 1 samples\n",
      "Tolerance Class 1278: 1 samples\n",
      "Tolerance Class 1279: 1 samples\n",
      "Tolerance Class 1280: 1 samples\n",
      "Tolerance Class 1281: 1 samples\n",
      "Tolerance Class 1282: 1 samples\n",
      "Tolerance Class 1283: 1 samples\n",
      "Tolerance Class 1284: 1 samples\n",
      "Tolerance Class 1285: 1 samples\n",
      "Tolerance Class 1286: 1 samples\n",
      "Tolerance Class 1287: 1 samples\n",
      "Tolerance Class 1288: 1 samples\n",
      "Tolerance Class 1289: 1 samples\n",
      "Tolerance Class 1290: 1 samples\n",
      "Tolerance Class 1291: 1 samples\n",
      "Tolerance Class 1292: 1 samples\n",
      "Tolerance Class 1293: 1 samples\n",
      "Tolerance Class 1294: 1 samples\n",
      "Tolerance Class 1295: 1 samples\n",
      "Tolerance Class 1296: 1 samples\n",
      "Tolerance Class 1297: 1 samples\n",
      "Tolerance Class 1298: 1 samples\n",
      "Tolerance Class 1299: 1 samples\n",
      "Tolerance Class 1300: 1 samples\n",
      "Tolerance Class 1301: 1 samples\n",
      "Tolerance Class 1302: 1 samples\n",
      "Tolerance Class 1303: 1 samples\n",
      "Tolerance Class 1304: 1 samples\n",
      "Tolerance Class 1305: 1 samples\n",
      "Tolerance Class 1306: 1 samples\n",
      "Tolerance Class 1307: 1 samples\n",
      "Tolerance Class 1308: 1 samples\n",
      "Tolerance Class 1309: 1 samples\n",
      "Tolerance Class 1310: 1 samples\n",
      "Tolerance Class 1311: 1 samples\n",
      "Tolerance Class 1312: 1 samples\n",
      "Tolerance Class 1313: 1 samples\n",
      "Tolerance Class 1314: 1 samples\n",
      "Tolerance Class 1315: 1 samples\n",
      "Tolerance Class 1316: 1 samples\n",
      "Tolerance Class 1317: 1 samples\n",
      "Tolerance Class 1318: 1 samples\n",
      "Tolerance Class 1319: 1 samples\n",
      "Tolerance Class 1320: 1 samples\n",
      "Tolerance Class 1321: 1 samples\n",
      "Tolerance Class 1322: 1 samples\n",
      "Tolerance Class 1323: 1 samples\n",
      "Tolerance Class 1324: 1 samples\n",
      "Tolerance Class 1325: 1 samples\n",
      "Tolerance Class 1326: 1 samples\n",
      "Tolerance Class 1327: 1 samples\n",
      "Tolerance Class 1328: 1 samples\n",
      "Tolerance Class 1329: 1 samples\n",
      "Tolerance Class 1330: 1 samples\n",
      "Tolerance Class 1331: 1 samples\n",
      "Tolerance Class 1332: 1 samples\n",
      "Tolerance Class 1333: 1 samples\n",
      "Tolerance Class 1334: 1 samples\n",
      "Tolerance Class 1335: 1 samples\n",
      "Tolerance Class 1336: 1 samples\n",
      "Tolerance Class 1337: 1 samples\n",
      "Tolerance Class 1338: 1 samples\n",
      "Tolerance Class 1339: 1 samples\n",
      "Tolerance Class 1340: 1 samples\n",
      "Tolerance Class 1341: 1 samples\n",
      "Tolerance Class 1342: 1 samples\n",
      "Tolerance Class 1343: 1 samples\n",
      "Tolerance Class 1344: 1 samples\n",
      "Tolerance Class 1345: 1 samples\n",
      "Tolerance Class 1346: 1 samples\n",
      "Tolerance Class 1347: 1 samples\n",
      "Tolerance Class 1348: 1 samples\n",
      "Tolerance Class 1349: 1 samples\n",
      "Tolerance Class 1350: 1 samples\n",
      "Tolerance Class 1351: 1 samples\n",
      "Tolerance Class 1352: 2 samples\n",
      "Tolerance Class 1353: 1 samples\n",
      "Tolerance Class 1354: 2 samples\n",
      "Tolerance Class 1355: 1 samples\n",
      "Tolerance Class 1356: 1 samples\n",
      "Tolerance Class 1357: 1 samples\n",
      "Tolerance Class 1358: 1 samples\n",
      "Tolerance Class 1359: 1 samples\n",
      "Tolerance Class 1360: 1 samples\n",
      "Tolerance Class 1361: 1 samples\n",
      "Tolerance Class 1362: 1 samples\n",
      "Tolerance Class 1363: 1 samples\n",
      "Tolerance Class 1364: 1 samples\n",
      "Tolerance Class 1365: 1 samples\n",
      "Tolerance Class 1366: 1 samples\n",
      "Tolerance Class 1367: 1 samples\n",
      "Tolerance Class 1368: 1 samples\n",
      "Tolerance Class 1369: 1 samples\n",
      "Tolerance Class 1370: 1 samples\n",
      "Tolerance Class 1371: 1 samples\n",
      "Tolerance Class 1372: 1 samples\n",
      "Tolerance Class 1373: 1 samples\n",
      "Tolerance Class 1374: 1 samples\n",
      "Tolerance Class 1375: 1 samples\n",
      "Tolerance Class 1376: 1 samples\n",
      "Tolerance Class 1377: 1 samples\n",
      "Tolerance Class 1378: 1 samples\n",
      "Tolerance Class 1379: 1 samples\n",
      "Tolerance Class 1380: 1 samples\n",
      "Tolerance Class 1381: 1 samples\n",
      "Tolerance Class 1382: 1 samples\n",
      "Tolerance Class 1383: 1 samples\n",
      "Tolerance Class 1384: 1 samples\n",
      "Tolerance Class 1385: 1 samples\n",
      "Tolerance Class 1386: 1 samples\n",
      "Tolerance Class 1387: 1 samples\n",
      "Tolerance Class 1388: 1 samples\n",
      "Tolerance Class 1389: 1 samples\n",
      "Tolerance Class 1390: 1 samples\n",
      "Tolerance Class 1391: 1 samples\n",
      "Tolerance Class 1392: 1 samples\n",
      "Tolerance Class 1393: 1 samples\n",
      "Tolerance Class 1394: 1 samples\n",
      "Tolerance Class 1395: 1 samples\n",
      "Tolerance Class 1396: 1 samples\n",
      "Tolerance Class 1397: 1 samples\n",
      "Tolerance Class 1398: 1 samples\n",
      "Tolerance Class 1399: 1 samples\n",
      "Tolerance Class 1400: 1 samples\n",
      "Tolerance Class 1401: 1 samples\n",
      "Tolerance Class 1402: 1 samples\n",
      "Tolerance Class 1403: 1 samples\n",
      "Tolerance Class 1404: 1 samples\n",
      "Tolerance Class 1405: 1 samples\n",
      "Tolerance Class 1406: 1 samples\n",
      "Tolerance Class 1407: 1 samples\n",
      "Tolerance Class 1408: 1 samples\n",
      "Tolerance Class 1409: 1 samples\n",
      "Tolerance Class 1410: 1 samples\n",
      "Tolerance Class 1411: 1 samples\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Form Global Tolerance Classes\n",
    "# Choose an epsilon threshold based on the histogram; here, we use epsilon = 0.15 as an example.\n",
    "epsilon = 0.09\n",
    "\n",
    "# Create a binary tolerance matrix: True if distance < epsilon.\n",
    "tolerance_matrix = (train_cos_dist < epsilon)\n",
    "\n",
    "# Build a graph where each training sample is a node.\n",
    "num_train = X_train_norm.shape[0]\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(num_train))\n",
    "for i in range(num_train):\n",
    "    for j in range(i+1, num_train):\n",
    "        if tolerance_matrix[i, j]:\n",
    "            G.add_edge(i, j)\n",
    "\n",
    "# Extract connected components from the graph as tolerance classes.\n",
    "tolerance_classes = list(nx.connected_components(G))\n",
    "print(\"Number of tolerance classes formed:\", len(tolerance_classes))\n",
    "for idx, cls in enumerate(tolerance_classes, start=1):\n",
    "    print(f\"Tolerance Class {idx}: {len(cls)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9a998fe4-4dec-43ba-b78f-4adc23f77284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.85      0.82        72\n",
      "           1       0.97      0.99      0.98       106\n",
      "           2       0.93      0.89      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.90      0.91      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = [str(x) for x in np.unique(y_train)]\n",
    "print(classification_report(y_test, predicted_labels, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "dc4beba6-2067-4d60-8e60-fa32201517dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prototypes (tolerance classes): 1411\n",
      "Prototype 1: Label = 1, Class Size = 1\n",
      "Prototype 2: Label = 1, Class Size = 7\n",
      "Prototype 3: Label = 1, Class Size = 1\n",
      "Prototype 4: Label = 1, Class Size = 1\n",
      "Prototype 5: Label = 1, Class Size = 1\n",
      "Prototype 6: Label = 1, Class Size = 1\n",
      "Prototype 7: Label = 2, Class Size = 1\n",
      "Prototype 8: Label = 2, Class Size = 1\n",
      "Prototype 9: Label = 0, Class Size = 2\n",
      "Prototype 10: Label = 2, Class Size = 1\n",
      "Prototype 11: Label = 1, Class Size = 2\n",
      "Prototype 12: Label = 2, Class Size = 2\n",
      "Prototype 13: Label = 1, Class Size = 1\n",
      "Prototype 14: Label = 0, Class Size = 1\n",
      "Prototype 15: Label = 2, Class Size = 1\n",
      "Prototype 16: Label = 2, Class Size = 1\n",
      "Prototype 17: Label = 0, Class Size = 1\n",
      "Prototype 18: Label = 2, Class Size = 1\n",
      "Prototype 19: Label = 1, Class Size = 1\n",
      "Prototype 20: Label = 2, Class Size = 1\n",
      "Prototype 21: Label = 2, Class Size = 1\n",
      "Prototype 22: Label = 1, Class Size = 1\n",
      "Prototype 23: Label = 0, Class Size = 1\n",
      "Prototype 24: Label = 2, Class Size = 2\n",
      "Prototype 25: Label = 1, Class Size = 1\n",
      "Prototype 26: Label = 0, Class Size = 1\n",
      "Prototype 27: Label = 1, Class Size = 1\n",
      "Prototype 28: Label = 2, Class Size = 1\n",
      "Prototype 29: Label = 2, Class Size = 1\n",
      "Prototype 30: Label = 1, Class Size = 1\n",
      "Prototype 31: Label = 1, Class Size = 2\n",
      "Prototype 32: Label = 2, Class Size = 1\n",
      "Prototype 33: Label = 1, Class Size = 1\n",
      "Prototype 34: Label = 2, Class Size = 1\n",
      "Prototype 35: Label = 2, Class Size = 1\n",
      "Prototype 36: Label = 2, Class Size = 2\n",
      "Prototype 37: Label = 1, Class Size = 1\n",
      "Prototype 38: Label = 2, Class Size = 1\n",
      "Prototype 39: Label = 2, Class Size = 1\n",
      "Prototype 40: Label = 2, Class Size = 1\n",
      "Prototype 41: Label = 2, Class Size = 2\n",
      "Prototype 42: Label = 1, Class Size = 1\n",
      "Prototype 43: Label = 0, Class Size = 1\n",
      "Prototype 44: Label = 2, Class Size = 1\n",
      "Prototype 45: Label = 0, Class Size = 1\n",
      "Prototype 46: Label = 2, Class Size = 1\n",
      "Prototype 47: Label = 1, Class Size = 1\n",
      "Prototype 48: Label = 1, Class Size = 2\n",
      "Prototype 49: Label = 1, Class Size = 1\n",
      "Prototype 50: Label = 2, Class Size = 2\n",
      "Prototype 51: Label = 1, Class Size = 1\n",
      "Prototype 52: Label = 0, Class Size = 1\n",
      "Prototype 53: Label = 0, Class Size = 1\n",
      "Prototype 54: Label = 0, Class Size = 1\n",
      "Prototype 55: Label = 0, Class Size = 2\n",
      "Prototype 56: Label = 1, Class Size = 1\n",
      "Prototype 57: Label = 2, Class Size = 2\n",
      "Prototype 58: Label = 0, Class Size = 1\n",
      "Prototype 59: Label = 1, Class Size = 1\n",
      "Prototype 60: Label = 1, Class Size = 1\n",
      "Prototype 61: Label = 1, Class Size = 1\n",
      "Prototype 62: Label = 2, Class Size = 1\n",
      "Prototype 63: Label = 1, Class Size = 2\n",
      "Prototype 64: Label = 2, Class Size = 1\n",
      "Prototype 65: Label = 1, Class Size = 2\n",
      "Prototype 66: Label = 2, Class Size = 2\n",
      "Prototype 67: Label = 2, Class Size = 1\n",
      "Prototype 68: Label = 2, Class Size = 1\n",
      "Prototype 69: Label = 2, Class Size = 1\n",
      "Prototype 70: Label = 1, Class Size = 1\n",
      "Prototype 71: Label = 2, Class Size = 1\n",
      "Prototype 72: Label = 2, Class Size = 1\n",
      "Prototype 73: Label = 0, Class Size = 1\n",
      "Prototype 74: Label = 2, Class Size = 1\n",
      "Prototype 75: Label = 1, Class Size = 1\n",
      "Prototype 76: Label = 0, Class Size = 1\n",
      "Prototype 77: Label = 2, Class Size = 1\n",
      "Prototype 78: Label = 0, Class Size = 2\n",
      "Prototype 79: Label = 0, Class Size = 2\n",
      "Prototype 80: Label = 1, Class Size = 1\n",
      "Prototype 81: Label = 0, Class Size = 1\n",
      "Prototype 82: Label = 1, Class Size = 2\n",
      "Prototype 83: Label = 0, Class Size = 1\n",
      "Prototype 84: Label = 2, Class Size = 2\n",
      "Prototype 85: Label = 1, Class Size = 1\n",
      "Prototype 86: Label = 2, Class Size = 1\n",
      "Prototype 87: Label = 2, Class Size = 2\n",
      "Prototype 88: Label = 2, Class Size = 1\n",
      "Prototype 89: Label = 1, Class Size = 1\n",
      "Prototype 90: Label = 2, Class Size = 1\n",
      "Prototype 91: Label = 1, Class Size = 1\n",
      "Prototype 92: Label = 2, Class Size = 1\n",
      "Prototype 93: Label = 1, Class Size = 1\n",
      "Prototype 94: Label = 2, Class Size = 1\n",
      "Prototype 95: Label = 2, Class Size = 1\n",
      "Prototype 96: Label = 1, Class Size = 1\n",
      "Prototype 97: Label = 2, Class Size = 1\n",
      "Prototype 98: Label = 0, Class Size = 1\n",
      "Prototype 99: Label = 1, Class Size = 2\n",
      "Prototype 100: Label = 2, Class Size = 1\n",
      "Prototype 101: Label = 0, Class Size = 1\n",
      "Prototype 102: Label = 2, Class Size = 1\n",
      "Prototype 103: Label = 1, Class Size = 2\n",
      "Prototype 104: Label = 2, Class Size = 1\n",
      "Prototype 105: Label = 2, Class Size = 1\n",
      "Prototype 106: Label = 2, Class Size = 2\n",
      "Prototype 107: Label = 0, Class Size = 1\n",
      "Prototype 108: Label = 2, Class Size = 1\n",
      "Prototype 109: Label = 2, Class Size = 1\n",
      "Prototype 110: Label = 2, Class Size = 2\n",
      "Prototype 111: Label = 2, Class Size = 2\n",
      "Prototype 112: Label = 2, Class Size = 2\n",
      "Prototype 113: Label = 1, Class Size = 1\n",
      "Prototype 114: Label = 2, Class Size = 1\n",
      "Prototype 115: Label = 1, Class Size = 1\n",
      "Prototype 116: Label = 1, Class Size = 1\n",
      "Prototype 117: Label = 1, Class Size = 1\n",
      "Prototype 118: Label = 2, Class Size = 1\n",
      "Prototype 119: Label = 2, Class Size = 2\n",
      "Prototype 120: Label = 2, Class Size = 1\n",
      "Prototype 121: Label = 2, Class Size = 1\n",
      "Prototype 122: Label = 0, Class Size = 1\n",
      "Prototype 123: Label = 0, Class Size = 1\n",
      "Prototype 124: Label = 2, Class Size = 2\n",
      "Prototype 125: Label = 2, Class Size = 1\n",
      "Prototype 126: Label = 1, Class Size = 1\n",
      "Prototype 127: Label = 2, Class Size = 1\n",
      "Prototype 128: Label = 0, Class Size = 1\n",
      "Prototype 129: Label = 1, Class Size = 1\n",
      "Prototype 130: Label = 2, Class Size = 1\n",
      "Prototype 131: Label = 1, Class Size = 2\n",
      "Prototype 132: Label = 0, Class Size = 2\n",
      "Prototype 133: Label = 2, Class Size = 1\n",
      "Prototype 134: Label = 2, Class Size = 2\n",
      "Prototype 135: Label = 0, Class Size = 1\n",
      "Prototype 136: Label = 1, Class Size = 1\n",
      "Prototype 137: Label = 1, Class Size = 1\n",
      "Prototype 138: Label = 1, Class Size = 1\n",
      "Prototype 139: Label = 2, Class Size = 1\n",
      "Prototype 140: Label = 2, Class Size = 1\n",
      "Prototype 141: Label = 2, Class Size = 1\n",
      "Prototype 142: Label = 0, Class Size = 1\n",
      "Prototype 143: Label = 1, Class Size = 1\n",
      "Prototype 144: Label = 0, Class Size = 1\n",
      "Prototype 145: Label = 1, Class Size = 1\n",
      "Prototype 146: Label = 2, Class Size = 2\n",
      "Prototype 147: Label = 2, Class Size = 1\n",
      "Prototype 148: Label = 2, Class Size = 1\n",
      "Prototype 149: Label = 0, Class Size = 1\n",
      "Prototype 150: Label = 1, Class Size = 1\n",
      "Prototype 151: Label = 1, Class Size = 1\n",
      "Prototype 152: Label = 1, Class Size = 1\n",
      "Prototype 153: Label = 0, Class Size = 1\n",
      "Prototype 154: Label = 1, Class Size = 1\n",
      "Prototype 155: Label = 1, Class Size = 1\n",
      "Prototype 156: Label = 0, Class Size = 1\n",
      "Prototype 157: Label = 0, Class Size = 1\n",
      "Prototype 158: Label = 2, Class Size = 1\n",
      "Prototype 159: Label = 1, Class Size = 1\n",
      "Prototype 160: Label = 2, Class Size = 1\n",
      "Prototype 161: Label = 2, Class Size = 1\n",
      "Prototype 162: Label = 1, Class Size = 1\n",
      "Prototype 163: Label = 2, Class Size = 1\n",
      "Prototype 164: Label = 2, Class Size = 1\n",
      "Prototype 165: Label = 0, Class Size = 1\n",
      "Prototype 166: Label = 2, Class Size = 2\n",
      "Prototype 167: Label = 2, Class Size = 2\n",
      "Prototype 168: Label = 2, Class Size = 1\n",
      "Prototype 169: Label = 1, Class Size = 1\n",
      "Prototype 170: Label = 1, Class Size = 1\n",
      "Prototype 171: Label = 0, Class Size = 1\n",
      "Prototype 172: Label = 2, Class Size = 1\n",
      "Prototype 173: Label = 2, Class Size = 1\n",
      "Prototype 174: Label = 1, Class Size = 1\n",
      "Prototype 175: Label = 1, Class Size = 1\n",
      "Prototype 176: Label = 1, Class Size = 1\n",
      "Prototype 177: Label = 1, Class Size = 1\n",
      "Prototype 178: Label = 2, Class Size = 1\n",
      "Prototype 179: Label = 0, Class Size = 1\n",
      "Prototype 180: Label = 2, Class Size = 1\n",
      "Prototype 181: Label = 1, Class Size = 1\n",
      "Prototype 182: Label = 2, Class Size = 1\n",
      "Prototype 183: Label = 0, Class Size = 1\n",
      "Prototype 184: Label = 1, Class Size = 2\n",
      "Prototype 185: Label = 1, Class Size = 2\n",
      "Prototype 186: Label = 2, Class Size = 1\n",
      "Prototype 187: Label = 0, Class Size = 1\n",
      "Prototype 188: Label = 2, Class Size = 2\n",
      "Prototype 189: Label = 0, Class Size = 1\n",
      "Prototype 190: Label = 2, Class Size = 2\n",
      "Prototype 191: Label = 1, Class Size = 1\n",
      "Prototype 192: Label = 2, Class Size = 1\n",
      "Prototype 193: Label = 2, Class Size = 1\n",
      "Prototype 194: Label = 2, Class Size = 1\n",
      "Prototype 195: Label = 2, Class Size = 1\n",
      "Prototype 196: Label = 0, Class Size = 2\n",
      "Prototype 197: Label = 0, Class Size = 1\n",
      "Prototype 198: Label = 0, Class Size = 1\n",
      "Prototype 199: Label = 1, Class Size = 2\n",
      "Prototype 200: Label = 0, Class Size = 1\n",
      "Prototype 201: Label = 1, Class Size = 2\n",
      "Prototype 202: Label = 0, Class Size = 1\n",
      "Prototype 203: Label = 2, Class Size = 2\n",
      "Prototype 204: Label = 1, Class Size = 1\n",
      "Prototype 205: Label = 1, Class Size = 2\n",
      "Prototype 206: Label = 1, Class Size = 1\n",
      "Prototype 207: Label = 0, Class Size = 1\n",
      "Prototype 208: Label = 1, Class Size = 1\n",
      "Prototype 209: Label = 1, Class Size = 1\n",
      "Prototype 210: Label = 2, Class Size = 2\n",
      "Prototype 211: Label = 2, Class Size = 1\n",
      "Prototype 212: Label = 0, Class Size = 2\n",
      "Prototype 213: Label = 0, Class Size = 1\n",
      "Prototype 214: Label = 0, Class Size = 2\n",
      "Prototype 215: Label = 1, Class Size = 1\n",
      "Prototype 216: Label = 2, Class Size = 1\n",
      "Prototype 217: Label = 2, Class Size = 1\n",
      "Prototype 218: Label = 2, Class Size = 1\n",
      "Prototype 219: Label = 2, Class Size = 1\n",
      "Prototype 220: Label = 1, Class Size = 1\n",
      "Prototype 221: Label = 2, Class Size = 1\n",
      "Prototype 222: Label = 0, Class Size = 2\n",
      "Prototype 223: Label = 1, Class Size = 2\n",
      "Prototype 224: Label = 2, Class Size = 2\n",
      "Prototype 225: Label = 1, Class Size = 2\n",
      "Prototype 226: Label = 1, Class Size = 1\n",
      "Prototype 227: Label = 0, Class Size = 1\n",
      "Prototype 228: Label = 1, Class Size = 1\n",
      "Prototype 229: Label = 2, Class Size = 1\n",
      "Prototype 230: Label = 2, Class Size = 1\n",
      "Prototype 231: Label = 2, Class Size = 1\n",
      "Prototype 232: Label = 2, Class Size = 1\n",
      "Prototype 233: Label = 1, Class Size = 1\n",
      "Prototype 234: Label = 2, Class Size = 2\n",
      "Prototype 235: Label = 2, Class Size = 1\n",
      "Prototype 236: Label = 2, Class Size = 1\n",
      "Prototype 237: Label = 1, Class Size = 1\n",
      "Prototype 238: Label = 0, Class Size = 1\n",
      "Prototype 239: Label = 0, Class Size = 1\n",
      "Prototype 240: Label = 2, Class Size = 2\n",
      "Prototype 241: Label = 2, Class Size = 1\n",
      "Prototype 242: Label = 2, Class Size = 1\n",
      "Prototype 243: Label = 2, Class Size = 2\n",
      "Prototype 244: Label = 0, Class Size = 2\n",
      "Prototype 245: Label = 1, Class Size = 1\n",
      "Prototype 246: Label = 2, Class Size = 1\n",
      "Prototype 247: Label = 2, Class Size = 1\n",
      "Prototype 248: Label = 2, Class Size = 1\n",
      "Prototype 249: Label = 2, Class Size = 1\n",
      "Prototype 250: Label = 2, Class Size = 1\n",
      "Prototype 251: Label = 2, Class Size = 1\n",
      "Prototype 252: Label = 0, Class Size = 1\n",
      "Prototype 253: Label = 0, Class Size = 1\n",
      "Prototype 254: Label = 2, Class Size = 1\n",
      "Prototype 255: Label = 2, Class Size = 1\n",
      "Prototype 256: Label = 0, Class Size = 2\n",
      "Prototype 257: Label = 2, Class Size = 1\n",
      "Prototype 258: Label = 2, Class Size = 1\n",
      "Prototype 259: Label = 2, Class Size = 1\n",
      "Prototype 260: Label = 2, Class Size = 1\n",
      "Prototype 261: Label = 2, Class Size = 1\n",
      "Prototype 262: Label = 2, Class Size = 1\n",
      "Prototype 263: Label = 2, Class Size = 2\n",
      "Prototype 264: Label = 1, Class Size = 1\n",
      "Prototype 265: Label = 2, Class Size = 1\n",
      "Prototype 266: Label = 1, Class Size = 1\n",
      "Prototype 267: Label = 0, Class Size = 1\n",
      "Prototype 268: Label = 0, Class Size = 1\n",
      "Prototype 269: Label = 0, Class Size = 1\n",
      "Prototype 270: Label = 2, Class Size = 1\n",
      "Prototype 271: Label = 2, Class Size = 1\n",
      "Prototype 272: Label = 0, Class Size = 1\n",
      "Prototype 273: Label = 1, Class Size = 1\n",
      "Prototype 274: Label = 2, Class Size = 1\n",
      "Prototype 275: Label = 0, Class Size = 1\n",
      "Prototype 276: Label = 2, Class Size = 2\n",
      "Prototype 277: Label = 2, Class Size = 1\n",
      "Prototype 278: Label = 0, Class Size = 1\n",
      "Prototype 279: Label = 0, Class Size = 1\n",
      "Prototype 280: Label = 0, Class Size = 1\n",
      "Prototype 281: Label = 1, Class Size = 1\n",
      "Prototype 282: Label = 0, Class Size = 1\n",
      "Prototype 283: Label = 2, Class Size = 1\n",
      "Prototype 284: Label = 1, Class Size = 2\n",
      "Prototype 285: Label = 0, Class Size = 1\n",
      "Prototype 286: Label = 2, Class Size = 1\n",
      "Prototype 287: Label = 2, Class Size = 2\n",
      "Prototype 288: Label = 2, Class Size = 1\n",
      "Prototype 289: Label = 1, Class Size = 2\n",
      "Prototype 290: Label = 2, Class Size = 1\n",
      "Prototype 291: Label = 0, Class Size = 1\n",
      "Prototype 292: Label = 0, Class Size = 1\n",
      "Prototype 293: Label = 2, Class Size = 2\n",
      "Prototype 294: Label = 0, Class Size = 1\n",
      "Prototype 295: Label = 2, Class Size = 1\n",
      "Prototype 296: Label = 1, Class Size = 1\n",
      "Prototype 297: Label = 0, Class Size = 1\n",
      "Prototype 298: Label = 2, Class Size = 1\n",
      "Prototype 299: Label = 2, Class Size = 2\n",
      "Prototype 300: Label = 2, Class Size = 1\n",
      "Prototype 301: Label = 1, Class Size = 1\n",
      "Prototype 302: Label = 1, Class Size = 1\n",
      "Prototype 303: Label = 2, Class Size = 1\n",
      "Prototype 304: Label = 2, Class Size = 1\n",
      "Prototype 305: Label = 2, Class Size = 1\n",
      "Prototype 306: Label = 0, Class Size = 1\n",
      "Prototype 307: Label = 0, Class Size = 2\n",
      "Prototype 308: Label = 1, Class Size = 1\n",
      "Prototype 309: Label = 1, Class Size = 2\n",
      "Prototype 310: Label = 2, Class Size = 1\n",
      "Prototype 311: Label = 1, Class Size = 1\n",
      "Prototype 312: Label = 2, Class Size = 1\n",
      "Prototype 313: Label = 1, Class Size = 2\n",
      "Prototype 314: Label = 2, Class Size = 1\n",
      "Prototype 315: Label = 2, Class Size = 1\n",
      "Prototype 316: Label = 0, Class Size = 1\n",
      "Prototype 317: Label = 0, Class Size = 1\n",
      "Prototype 318: Label = 2, Class Size = 1\n",
      "Prototype 319: Label = 2, Class Size = 1\n",
      "Prototype 320: Label = 1, Class Size = 1\n",
      "Prototype 321: Label = 1, Class Size = 1\n",
      "Prototype 322: Label = 1, Class Size = 1\n",
      "Prototype 323: Label = 2, Class Size = 1\n",
      "Prototype 324: Label = 2, Class Size = 1\n",
      "Prototype 325: Label = 1, Class Size = 2\n",
      "Prototype 326: Label = 1, Class Size = 1\n",
      "Prototype 327: Label = 1, Class Size = 1\n",
      "Prototype 328: Label = 1, Class Size = 2\n",
      "Prototype 329: Label = 1, Class Size = 1\n",
      "Prototype 330: Label = 2, Class Size = 1\n",
      "Prototype 331: Label = 2, Class Size = 1\n",
      "Prototype 332: Label = 0, Class Size = 1\n",
      "Prototype 333: Label = 1, Class Size = 1\n",
      "Prototype 334: Label = 2, Class Size = 1\n",
      "Prototype 335: Label = 0, Class Size = 2\n",
      "Prototype 336: Label = 1, Class Size = 1\n",
      "Prototype 337: Label = 1, Class Size = 1\n",
      "Prototype 338: Label = 0, Class Size = 1\n",
      "Prototype 339: Label = 1, Class Size = 1\n",
      "Prototype 340: Label = 2, Class Size = 1\n",
      "Prototype 341: Label = 2, Class Size = 2\n",
      "Prototype 342: Label = 1, Class Size = 2\n",
      "Prototype 343: Label = 1, Class Size = 1\n",
      "Prototype 344: Label = 0, Class Size = 1\n",
      "Prototype 345: Label = 1, Class Size = 1\n",
      "Prototype 346: Label = 0, Class Size = 1\n",
      "Prototype 347: Label = 2, Class Size = 1\n",
      "Prototype 348: Label = 2, Class Size = 1\n",
      "Prototype 349: Label = 0, Class Size = 1\n",
      "Prototype 350: Label = 1, Class Size = 2\n",
      "Prototype 351: Label = 2, Class Size = 2\n",
      "Prototype 352: Label = 0, Class Size = 1\n",
      "Prototype 353: Label = 0, Class Size = 1\n",
      "Prototype 354: Label = 0, Class Size = 1\n",
      "Prototype 355: Label = 2, Class Size = 2\n",
      "Prototype 356: Label = 2, Class Size = 2\n",
      "Prototype 357: Label = 2, Class Size = 1\n",
      "Prototype 358: Label = 2, Class Size = 1\n",
      "Prototype 359: Label = 2, Class Size = 2\n",
      "Prototype 360: Label = 1, Class Size = 1\n",
      "Prototype 361: Label = 2, Class Size = 1\n",
      "Prototype 362: Label = 1, Class Size = 2\n",
      "Prototype 363: Label = 1, Class Size = 1\n",
      "Prototype 364: Label = 2, Class Size = 2\n",
      "Prototype 365: Label = 1, Class Size = 1\n",
      "Prototype 366: Label = 1, Class Size = 1\n",
      "Prototype 367: Label = 2, Class Size = 1\n",
      "Prototype 368: Label = 2, Class Size = 1\n",
      "Prototype 369: Label = 1, Class Size = 1\n",
      "Prototype 370: Label = 2, Class Size = 1\n",
      "Prototype 371: Label = 1, Class Size = 1\n",
      "Prototype 372: Label = 2, Class Size = 1\n",
      "Prototype 373: Label = 1, Class Size = 1\n",
      "Prototype 374: Label = 2, Class Size = 1\n",
      "Prototype 375: Label = 2, Class Size = 1\n",
      "Prototype 376: Label = 1, Class Size = 1\n",
      "Prototype 377: Label = 2, Class Size = 1\n",
      "Prototype 378: Label = 1, Class Size = 1\n",
      "Prototype 379: Label = 2, Class Size = 1\n",
      "Prototype 380: Label = 2, Class Size = 1\n",
      "Prototype 381: Label = 1, Class Size = 1\n",
      "Prototype 382: Label = 2, Class Size = 2\n",
      "Prototype 383: Label = 2, Class Size = 1\n",
      "Prototype 384: Label = 0, Class Size = 1\n",
      "Prototype 385: Label = 1, Class Size = 1\n",
      "Prototype 386: Label = 1, Class Size = 2\n",
      "Prototype 387: Label = 2, Class Size = 1\n",
      "Prototype 388: Label = 0, Class Size = 1\n",
      "Prototype 389: Label = 2, Class Size = 1\n",
      "Prototype 390: Label = 2, Class Size = 1\n",
      "Prototype 391: Label = 1, Class Size = 2\n",
      "Prototype 392: Label = 1, Class Size = 1\n",
      "Prototype 393: Label = 2, Class Size = 2\n",
      "Prototype 394: Label = 2, Class Size = 1\n",
      "Prototype 395: Label = 0, Class Size = 1\n",
      "Prototype 396: Label = 2, Class Size = 1\n",
      "Prototype 397: Label = 1, Class Size = 1\n",
      "Prototype 398: Label = 2, Class Size = 1\n",
      "Prototype 399: Label = 2, Class Size = 1\n",
      "Prototype 400: Label = 0, Class Size = 1\n",
      "Prototype 401: Label = 2, Class Size = 1\n",
      "Prototype 402: Label = 1, Class Size = 1\n",
      "Prototype 403: Label = 2, Class Size = 1\n",
      "Prototype 404: Label = 2, Class Size = 2\n",
      "Prototype 405: Label = 2, Class Size = 1\n",
      "Prototype 406: Label = 1, Class Size = 1\n",
      "Prototype 407: Label = 0, Class Size = 1\n",
      "Prototype 408: Label = 2, Class Size = 1\n",
      "Prototype 409: Label = 2, Class Size = 1\n",
      "Prototype 410: Label = 1, Class Size = 1\n",
      "Prototype 411: Label = 0, Class Size = 2\n",
      "Prototype 412: Label = 2, Class Size = 1\n",
      "Prototype 413: Label = 1, Class Size = 1\n",
      "Prototype 414: Label = 2, Class Size = 1\n",
      "Prototype 415: Label = 0, Class Size = 1\n",
      "Prototype 416: Label = 1, Class Size = 1\n",
      "Prototype 417: Label = 2, Class Size = 2\n",
      "Prototype 418: Label = 2, Class Size = 1\n",
      "Prototype 419: Label = 2, Class Size = 1\n",
      "Prototype 420: Label = 2, Class Size = 1\n",
      "Prototype 421: Label = 2, Class Size = 1\n",
      "Prototype 422: Label = 2, Class Size = 1\n",
      "Prototype 423: Label = 1, Class Size = 1\n",
      "Prototype 424: Label = 0, Class Size = 1\n",
      "Prototype 425: Label = 2, Class Size = 2\n",
      "Prototype 426: Label = 0, Class Size = 1\n",
      "Prototype 427: Label = 0, Class Size = 1\n",
      "Prototype 428: Label = 2, Class Size = 1\n",
      "Prototype 429: Label = 1, Class Size = 1\n",
      "Prototype 430: Label = 2, Class Size = 2\n",
      "Prototype 431: Label = 2, Class Size = 1\n",
      "Prototype 432: Label = 2, Class Size = 2\n",
      "Prototype 433: Label = 1, Class Size = 1\n",
      "Prototype 434: Label = 2, Class Size = 1\n",
      "Prototype 435: Label = 0, Class Size = 1\n",
      "Prototype 436: Label = 2, Class Size = 1\n",
      "Prototype 437: Label = 0, Class Size = 1\n",
      "Prototype 438: Label = 0, Class Size = 1\n",
      "Prototype 439: Label = 2, Class Size = 1\n",
      "Prototype 440: Label = 2, Class Size = 1\n",
      "Prototype 441: Label = 1, Class Size = 1\n",
      "Prototype 442: Label = 2, Class Size = 1\n",
      "Prototype 443: Label = 0, Class Size = 2\n",
      "Prototype 444: Label = 0, Class Size = 1\n",
      "Prototype 445: Label = 2, Class Size = 1\n",
      "Prototype 446: Label = 2, Class Size = 1\n",
      "Prototype 447: Label = 2, Class Size = 1\n",
      "Prototype 448: Label = 1, Class Size = 1\n",
      "Prototype 449: Label = 2, Class Size = 1\n",
      "Prototype 450: Label = 0, Class Size = 1\n",
      "Prototype 451: Label = 2, Class Size = 1\n",
      "Prototype 452: Label = 2, Class Size = 1\n",
      "Prototype 453: Label = 2, Class Size = 1\n",
      "Prototype 454: Label = 1, Class Size = 1\n",
      "Prototype 455: Label = 1, Class Size = 1\n",
      "Prototype 456: Label = 0, Class Size = 2\n",
      "Prototype 457: Label = 2, Class Size = 1\n",
      "Prototype 458: Label = 2, Class Size = 1\n",
      "Prototype 459: Label = 2, Class Size = 1\n",
      "Prototype 460: Label = 2, Class Size = 1\n",
      "Prototype 461: Label = 1, Class Size = 1\n",
      "Prototype 462: Label = 0, Class Size = 2\n",
      "Prototype 463: Label = 2, Class Size = 1\n",
      "Prototype 464: Label = 2, Class Size = 1\n",
      "Prototype 465: Label = 0, Class Size = 2\n",
      "Prototype 466: Label = 2, Class Size = 2\n",
      "Prototype 467: Label = 1, Class Size = 1\n",
      "Prototype 468: Label = 2, Class Size = 1\n",
      "Prototype 469: Label = 0, Class Size = 1\n",
      "Prototype 470: Label = 2, Class Size = 2\n",
      "Prototype 471: Label = 1, Class Size = 1\n",
      "Prototype 472: Label = 1, Class Size = 1\n",
      "Prototype 473: Label = 1, Class Size = 1\n",
      "Prototype 474: Label = 2, Class Size = 1\n",
      "Prototype 475: Label = 2, Class Size = 1\n",
      "Prototype 476: Label = 2, Class Size = 1\n",
      "Prototype 477: Label = 2, Class Size = 1\n",
      "Prototype 478: Label = 2, Class Size = 1\n",
      "Prototype 479: Label = 1, Class Size = 2\n",
      "Prototype 480: Label = 2, Class Size = 1\n",
      "Prototype 481: Label = 2, Class Size = 1\n",
      "Prototype 482: Label = 2, Class Size = 1\n",
      "Prototype 483: Label = 1, Class Size = 1\n",
      "Prototype 484: Label = 2, Class Size = 1\n",
      "Prototype 485: Label = 0, Class Size = 2\n",
      "Prototype 486: Label = 1, Class Size = 1\n",
      "Prototype 487: Label = 0, Class Size = 1\n",
      "Prototype 488: Label = 2, Class Size = 1\n",
      "Prototype 489: Label = 2, Class Size = 1\n",
      "Prototype 490: Label = 0, Class Size = 2\n",
      "Prototype 491: Label = 2, Class Size = 1\n",
      "Prototype 492: Label = 2, Class Size = 1\n",
      "Prototype 493: Label = 1, Class Size = 1\n",
      "Prototype 494: Label = 1, Class Size = 1\n",
      "Prototype 495: Label = 0, Class Size = 1\n",
      "Prototype 496: Label = 2, Class Size = 1\n",
      "Prototype 497: Label = 2, Class Size = 1\n",
      "Prototype 498: Label = 1, Class Size = 1\n",
      "Prototype 499: Label = 0, Class Size = 1\n",
      "Prototype 500: Label = 0, Class Size = 2\n",
      "Prototype 501: Label = 1, Class Size = 1\n",
      "Prototype 502: Label = 1, Class Size = 1\n",
      "Prototype 503: Label = 2, Class Size = 1\n",
      "Prototype 504: Label = 2, Class Size = 1\n",
      "Prototype 505: Label = 1, Class Size = 1\n",
      "Prototype 506: Label = 2, Class Size = 1\n",
      "Prototype 507: Label = 1, Class Size = 1\n",
      "Prototype 508: Label = 1, Class Size = 1\n",
      "Prototype 509: Label = 2, Class Size = 1\n",
      "Prototype 510: Label = 2, Class Size = 1\n",
      "Prototype 511: Label = 2, Class Size = 1\n",
      "Prototype 512: Label = 1, Class Size = 1\n",
      "Prototype 513: Label = 2, Class Size = 1\n",
      "Prototype 514: Label = 2, Class Size = 1\n",
      "Prototype 515: Label = 1, Class Size = 1\n",
      "Prototype 516: Label = 1, Class Size = 1\n",
      "Prototype 517: Label = 1, Class Size = 1\n",
      "Prototype 518: Label = 0, Class Size = 1\n",
      "Prototype 519: Label = 1, Class Size = 1\n",
      "Prototype 520: Label = 1, Class Size = 2\n",
      "Prototype 521: Label = 2, Class Size = 1\n",
      "Prototype 522: Label = 2, Class Size = 1\n",
      "Prototype 523: Label = 1, Class Size = 1\n",
      "Prototype 524: Label = 2, Class Size = 1\n",
      "Prototype 525: Label = 2, Class Size = 1\n",
      "Prototype 526: Label = 0, Class Size = 1\n",
      "Prototype 527: Label = 2, Class Size = 1\n",
      "Prototype 528: Label = 1, Class Size = 2\n",
      "Prototype 529: Label = 1, Class Size = 1\n",
      "Prototype 530: Label = 2, Class Size = 1\n",
      "Prototype 531: Label = 0, Class Size = 1\n",
      "Prototype 532: Label = 1, Class Size = 1\n",
      "Prototype 533: Label = 1, Class Size = 1\n",
      "Prototype 534: Label = 2, Class Size = 1\n",
      "Prototype 535: Label = 1, Class Size = 1\n",
      "Prototype 536: Label = 1, Class Size = 1\n",
      "Prototype 537: Label = 2, Class Size = 2\n",
      "Prototype 538: Label = 0, Class Size = 1\n",
      "Prototype 539: Label = 2, Class Size = 1\n",
      "Prototype 540: Label = 2, Class Size = 1\n",
      "Prototype 541: Label = 1, Class Size = 1\n",
      "Prototype 542: Label = 2, Class Size = 1\n",
      "Prototype 543: Label = 1, Class Size = 1\n",
      "Prototype 544: Label = 2, Class Size = 1\n",
      "Prototype 545: Label = 2, Class Size = 1\n",
      "Prototype 546: Label = 0, Class Size = 1\n",
      "Prototype 547: Label = 2, Class Size = 1\n",
      "Prototype 548: Label = 2, Class Size = 1\n",
      "Prototype 549: Label = 2, Class Size = 2\n",
      "Prototype 550: Label = 2, Class Size = 1\n",
      "Prototype 551: Label = 2, Class Size = 1\n",
      "Prototype 552: Label = 2, Class Size = 1\n",
      "Prototype 553: Label = 2, Class Size = 1\n",
      "Prototype 554: Label = 2, Class Size = 1\n",
      "Prototype 555: Label = 0, Class Size = 2\n",
      "Prototype 556: Label = 2, Class Size = 1\n",
      "Prototype 557: Label = 1, Class Size = 1\n",
      "Prototype 558: Label = 2, Class Size = 1\n",
      "Prototype 559: Label = 0, Class Size = 1\n",
      "Prototype 560: Label = 2, Class Size = 2\n",
      "Prototype 561: Label = 2, Class Size = 2\n",
      "Prototype 562: Label = 1, Class Size = 1\n",
      "Prototype 563: Label = 0, Class Size = 1\n",
      "Prototype 564: Label = 1, Class Size = 1\n",
      "Prototype 565: Label = 2, Class Size = 1\n",
      "Prototype 566: Label = 2, Class Size = 2\n",
      "Prototype 567: Label = 0, Class Size = 1\n",
      "Prototype 568: Label = 1, Class Size = 2\n",
      "Prototype 569: Label = 1, Class Size = 1\n",
      "Prototype 570: Label = 1, Class Size = 1\n",
      "Prototype 571: Label = 2, Class Size = 1\n",
      "Prototype 572: Label = 2, Class Size = 1\n",
      "Prototype 573: Label = 1, Class Size = 1\n",
      "Prototype 574: Label = 2, Class Size = 1\n",
      "Prototype 575: Label = 0, Class Size = 1\n",
      "Prototype 576: Label = 2, Class Size = 2\n",
      "Prototype 577: Label = 2, Class Size = 1\n",
      "Prototype 578: Label = 1, Class Size = 1\n",
      "Prototype 579: Label = 1, Class Size = 2\n",
      "Prototype 580: Label = 1, Class Size = 2\n",
      "Prototype 581: Label = 1, Class Size = 1\n",
      "Prototype 582: Label = 0, Class Size = 1\n",
      "Prototype 583: Label = 0, Class Size = 1\n",
      "Prototype 584: Label = 1, Class Size = 1\n",
      "Prototype 585: Label = 0, Class Size = 1\n",
      "Prototype 586: Label = 1, Class Size = 1\n",
      "Prototype 587: Label = 2, Class Size = 1\n",
      "Prototype 588: Label = 0, Class Size = 1\n",
      "Prototype 589: Label = 1, Class Size = 1\n",
      "Prototype 590: Label = 2, Class Size = 1\n",
      "Prototype 591: Label = 2, Class Size = 1\n",
      "Prototype 592: Label = 2, Class Size = 2\n",
      "Prototype 593: Label = 2, Class Size = 1\n",
      "Prototype 594: Label = 1, Class Size = 1\n",
      "Prototype 595: Label = 0, Class Size = 1\n",
      "Prototype 596: Label = 2, Class Size = 1\n",
      "Prototype 597: Label = 2, Class Size = 1\n",
      "Prototype 598: Label = 2, Class Size = 1\n",
      "Prototype 599: Label = 2, Class Size = 1\n",
      "Prototype 600: Label = 1, Class Size = 1\n",
      "Prototype 601: Label = 2, Class Size = 1\n",
      "Prototype 602: Label = 1, Class Size = 1\n",
      "Prototype 603: Label = 2, Class Size = 1\n",
      "Prototype 604: Label = 2, Class Size = 1\n",
      "Prototype 605: Label = 0, Class Size = 1\n",
      "Prototype 606: Label = 1, Class Size = 1\n",
      "Prototype 607: Label = 0, Class Size = 2\n",
      "Prototype 608: Label = 1, Class Size = 1\n",
      "Prototype 609: Label = 0, Class Size = 1\n",
      "Prototype 610: Label = 1, Class Size = 1\n",
      "Prototype 611: Label = 2, Class Size = 1\n",
      "Prototype 612: Label = 0, Class Size = 1\n",
      "Prototype 613: Label = 0, Class Size = 1\n",
      "Prototype 614: Label = 1, Class Size = 2\n",
      "Prototype 615: Label = 1, Class Size = 1\n",
      "Prototype 616: Label = 0, Class Size = 1\n",
      "Prototype 617: Label = 2, Class Size = 1\n",
      "Prototype 618: Label = 1, Class Size = 1\n",
      "Prototype 619: Label = 1, Class Size = 1\n",
      "Prototype 620: Label = 2, Class Size = 2\n",
      "Prototype 621: Label = 1, Class Size = 2\n",
      "Prototype 622: Label = 2, Class Size = 1\n",
      "Prototype 623: Label = 2, Class Size = 2\n",
      "Prototype 624: Label = 1, Class Size = 1\n",
      "Prototype 625: Label = 0, Class Size = 1\n",
      "Prototype 626: Label = 0, Class Size = 1\n",
      "Prototype 627: Label = 2, Class Size = 1\n",
      "Prototype 628: Label = 2, Class Size = 1\n",
      "Prototype 629: Label = 1, Class Size = 1\n",
      "Prototype 630: Label = 0, Class Size = 1\n",
      "Prototype 631: Label = 1, Class Size = 1\n",
      "Prototype 632: Label = 1, Class Size = 1\n",
      "Prototype 633: Label = 0, Class Size = 2\n",
      "Prototype 634: Label = 2, Class Size = 2\n",
      "Prototype 635: Label = 2, Class Size = 1\n",
      "Prototype 636: Label = 0, Class Size = 1\n",
      "Prototype 637: Label = 2, Class Size = 1\n",
      "Prototype 638: Label = 1, Class Size = 1\n",
      "Prototype 639: Label = 2, Class Size = 1\n",
      "Prototype 640: Label = 1, Class Size = 2\n",
      "Prototype 641: Label = 2, Class Size = 1\n",
      "Prototype 642: Label = 0, Class Size = 1\n",
      "Prototype 643: Label = 2, Class Size = 1\n",
      "Prototype 644: Label = 1, Class Size = 1\n",
      "Prototype 645: Label = 2, Class Size = 1\n",
      "Prototype 646: Label = 2, Class Size = 1\n",
      "Prototype 647: Label = 2, Class Size = 1\n",
      "Prototype 648: Label = 1, Class Size = 1\n",
      "Prototype 649: Label = 2, Class Size = 1\n",
      "Prototype 650: Label = 0, Class Size = 1\n",
      "Prototype 651: Label = 0, Class Size = 1\n",
      "Prototype 652: Label = 2, Class Size = 1\n",
      "Prototype 653: Label = 2, Class Size = 2\n",
      "Prototype 654: Label = 2, Class Size = 2\n",
      "Prototype 655: Label = 2, Class Size = 1\n",
      "Prototype 656: Label = 2, Class Size = 1\n",
      "Prototype 657: Label = 1, Class Size = 1\n",
      "Prototype 658: Label = 1, Class Size = 1\n",
      "Prototype 659: Label = 2, Class Size = 1\n",
      "Prototype 660: Label = 2, Class Size = 2\n",
      "Prototype 661: Label = 0, Class Size = 1\n",
      "Prototype 662: Label = 2, Class Size = 1\n",
      "Prototype 663: Label = 0, Class Size = 1\n",
      "Prototype 664: Label = 2, Class Size = 1\n",
      "Prototype 665: Label = 1, Class Size = 2\n",
      "Prototype 666: Label = 0, Class Size = 1\n",
      "Prototype 667: Label = 2, Class Size = 1\n",
      "Prototype 668: Label = 0, Class Size = 1\n",
      "Prototype 669: Label = 2, Class Size = 1\n",
      "Prototype 670: Label = 2, Class Size = 1\n",
      "Prototype 671: Label = 1, Class Size = 1\n",
      "Prototype 672: Label = 2, Class Size = 1\n",
      "Prototype 673: Label = 2, Class Size = 1\n",
      "Prototype 674: Label = 1, Class Size = 1\n",
      "Prototype 675: Label = 2, Class Size = 1\n",
      "Prototype 676: Label = 0, Class Size = 1\n",
      "Prototype 677: Label = 1, Class Size = 2\n",
      "Prototype 678: Label = 1, Class Size = 1\n",
      "Prototype 679: Label = 2, Class Size = 1\n",
      "Prototype 680: Label = 2, Class Size = 1\n",
      "Prototype 681: Label = 2, Class Size = 2\n",
      "Prototype 682: Label = 2, Class Size = 1\n",
      "Prototype 683: Label = 0, Class Size = 1\n",
      "Prototype 684: Label = 2, Class Size = 1\n",
      "Prototype 685: Label = 2, Class Size = 1\n",
      "Prototype 686: Label = 0, Class Size = 1\n",
      "Prototype 687: Label = 2, Class Size = 2\n",
      "Prototype 688: Label = 1, Class Size = 2\n",
      "Prototype 689: Label = 1, Class Size = 1\n",
      "Prototype 690: Label = 0, Class Size = 1\n",
      "Prototype 691: Label = 2, Class Size = 1\n",
      "Prototype 692: Label = 2, Class Size = 1\n",
      "Prototype 693: Label = 1, Class Size = 2\n",
      "Prototype 694: Label = 2, Class Size = 1\n",
      "Prototype 695: Label = 2, Class Size = 1\n",
      "Prototype 696: Label = 1, Class Size = 1\n",
      "Prototype 697: Label = 2, Class Size = 1\n",
      "Prototype 698: Label = 0, Class Size = 1\n",
      "Prototype 699: Label = 1, Class Size = 1\n",
      "Prototype 700: Label = 1, Class Size = 1\n",
      "Prototype 701: Label = 1, Class Size = 1\n",
      "Prototype 702: Label = 2, Class Size = 1\n",
      "Prototype 703: Label = 1, Class Size = 2\n",
      "Prototype 704: Label = 2, Class Size = 1\n",
      "Prototype 705: Label = 2, Class Size = 1\n",
      "Prototype 706: Label = 2, Class Size = 2\n",
      "Prototype 707: Label = 2, Class Size = 1\n",
      "Prototype 708: Label = 1, Class Size = 1\n",
      "Prototype 709: Label = 2, Class Size = 1\n",
      "Prototype 710: Label = 0, Class Size = 1\n",
      "Prototype 711: Label = 0, Class Size = 1\n",
      "Prototype 712: Label = 1, Class Size = 1\n",
      "Prototype 713: Label = 2, Class Size = 1\n",
      "Prototype 714: Label = 2, Class Size = 1\n",
      "Prototype 715: Label = 2, Class Size = 1\n",
      "Prototype 716: Label = 0, Class Size = 1\n",
      "Prototype 717: Label = 0, Class Size = 1\n",
      "Prototype 718: Label = 0, Class Size = 1\n",
      "Prototype 719: Label = 2, Class Size = 1\n",
      "Prototype 720: Label = 2, Class Size = 1\n",
      "Prototype 721: Label = 0, Class Size = 1\n",
      "Prototype 722: Label = 1, Class Size = 1\n",
      "Prototype 723: Label = 0, Class Size = 1\n",
      "Prototype 724: Label = 2, Class Size = 1\n",
      "Prototype 725: Label = 2, Class Size = 1\n",
      "Prototype 726: Label = 2, Class Size = 2\n",
      "Prototype 727: Label = 0, Class Size = 2\n",
      "Prototype 728: Label = 2, Class Size = 1\n",
      "Prototype 729: Label = 2, Class Size = 1\n",
      "Prototype 730: Label = 0, Class Size = 1\n",
      "Prototype 731: Label = 1, Class Size = 1\n",
      "Prototype 732: Label = 2, Class Size = 1\n",
      "Prototype 733: Label = 0, Class Size = 1\n",
      "Prototype 734: Label = 2, Class Size = 1\n",
      "Prototype 735: Label = 2, Class Size = 1\n",
      "Prototype 736: Label = 0, Class Size = 2\n",
      "Prototype 737: Label = 1, Class Size = 1\n",
      "Prototype 738: Label = 1, Class Size = 1\n",
      "Prototype 739: Label = 2, Class Size = 1\n",
      "Prototype 740: Label = 2, Class Size = 1\n",
      "Prototype 741: Label = 1, Class Size = 1\n",
      "Prototype 742: Label = 2, Class Size = 1\n",
      "Prototype 743: Label = 1, Class Size = 2\n",
      "Prototype 744: Label = 1, Class Size = 1\n",
      "Prototype 745: Label = 2, Class Size = 1\n",
      "Prototype 746: Label = 2, Class Size = 1\n",
      "Prototype 747: Label = 0, Class Size = 2\n",
      "Prototype 748: Label = 2, Class Size = 1\n",
      "Prototype 749: Label = 2, Class Size = 1\n",
      "Prototype 750: Label = 1, Class Size = 1\n",
      "Prototype 751: Label = 2, Class Size = 1\n",
      "Prototype 752: Label = 0, Class Size = 1\n",
      "Prototype 753: Label = 0, Class Size = 1\n",
      "Prototype 754: Label = 2, Class Size = 1\n",
      "Prototype 755: Label = 2, Class Size = 2\n",
      "Prototype 756: Label = 2, Class Size = 2\n",
      "Prototype 757: Label = 2, Class Size = 1\n",
      "Prototype 758: Label = 1, Class Size = 2\n",
      "Prototype 759: Label = 1, Class Size = 1\n",
      "Prototype 760: Label = 0, Class Size = 1\n",
      "Prototype 761: Label = 1, Class Size = 2\n",
      "Prototype 762: Label = 2, Class Size = 1\n",
      "Prototype 763: Label = 2, Class Size = 1\n",
      "Prototype 764: Label = 2, Class Size = 1\n",
      "Prototype 765: Label = 2, Class Size = 2\n",
      "Prototype 766: Label = 2, Class Size = 1\n",
      "Prototype 767: Label = 2, Class Size = 1\n",
      "Prototype 768: Label = 1, Class Size = 2\n",
      "Prototype 769: Label = 2, Class Size = 1\n",
      "Prototype 770: Label = 0, Class Size = 1\n",
      "Prototype 771: Label = 0, Class Size = 1\n",
      "Prototype 772: Label = 0, Class Size = 1\n",
      "Prototype 773: Label = 1, Class Size = 1\n",
      "Prototype 774: Label = 1, Class Size = 1\n",
      "Prototype 775: Label = 2, Class Size = 1\n",
      "Prototype 776: Label = 1, Class Size = 1\n",
      "Prototype 777: Label = 0, Class Size = 1\n",
      "Prototype 778: Label = 1, Class Size = 1\n",
      "Prototype 779: Label = 2, Class Size = 1\n",
      "Prototype 780: Label = 1, Class Size = 1\n",
      "Prototype 781: Label = 2, Class Size = 2\n",
      "Prototype 782: Label = 1, Class Size = 1\n",
      "Prototype 783: Label = 2, Class Size = 1\n",
      "Prototype 784: Label = 1, Class Size = 1\n",
      "Prototype 785: Label = 0, Class Size = 1\n",
      "Prototype 786: Label = 2, Class Size = 1\n",
      "Prototype 787: Label = 2, Class Size = 1\n",
      "Prototype 788: Label = 2, Class Size = 1\n",
      "Prototype 789: Label = 2, Class Size = 1\n",
      "Prototype 790: Label = 0, Class Size = 1\n",
      "Prototype 791: Label = 1, Class Size = 1\n",
      "Prototype 792: Label = 1, Class Size = 1\n",
      "Prototype 793: Label = 2, Class Size = 1\n",
      "Prototype 794: Label = 2, Class Size = 1\n",
      "Prototype 795: Label = 2, Class Size = 1\n",
      "Prototype 796: Label = 1, Class Size = 1\n",
      "Prototype 797: Label = 2, Class Size = 1\n",
      "Prototype 798: Label = 2, Class Size = 2\n",
      "Prototype 799: Label = 1, Class Size = 1\n",
      "Prototype 800: Label = 2, Class Size = 1\n",
      "Prototype 801: Label = 0, Class Size = 1\n",
      "Prototype 802: Label = 2, Class Size = 1\n",
      "Prototype 803: Label = 2, Class Size = 2\n",
      "Prototype 804: Label = 2, Class Size = 2\n",
      "Prototype 805: Label = 1, Class Size = 1\n",
      "Prototype 806: Label = 2, Class Size = 2\n",
      "Prototype 807: Label = 2, Class Size = 1\n",
      "Prototype 808: Label = 0, Class Size = 1\n",
      "Prototype 809: Label = 1, Class Size = 1\n",
      "Prototype 810: Label = 0, Class Size = 1\n",
      "Prototype 811: Label = 2, Class Size = 1\n",
      "Prototype 812: Label = 1, Class Size = 2\n",
      "Prototype 813: Label = 2, Class Size = 1\n",
      "Prototype 814: Label = 2, Class Size = 1\n",
      "Prototype 815: Label = 1, Class Size = 1\n",
      "Prototype 816: Label = 2, Class Size = 2\n",
      "Prototype 817: Label = 1, Class Size = 1\n",
      "Prototype 818: Label = 2, Class Size = 1\n",
      "Prototype 819: Label = 2, Class Size = 1\n",
      "Prototype 820: Label = 1, Class Size = 1\n",
      "Prototype 821: Label = 0, Class Size = 1\n",
      "Prototype 822: Label = 0, Class Size = 2\n",
      "Prototype 823: Label = 2, Class Size = 1\n",
      "Prototype 824: Label = 1, Class Size = 1\n",
      "Prototype 825: Label = 0, Class Size = 1\n",
      "Prototype 826: Label = 2, Class Size = 1\n",
      "Prototype 827: Label = 1, Class Size = 1\n",
      "Prototype 828: Label = 2, Class Size = 1\n",
      "Prototype 829: Label = 2, Class Size = 1\n",
      "Prototype 830: Label = 2, Class Size = 1\n",
      "Prototype 831: Label = 2, Class Size = 1\n",
      "Prototype 832: Label = 0, Class Size = 2\n",
      "Prototype 833: Label = 1, Class Size = 2\n",
      "Prototype 834: Label = 0, Class Size = 1\n",
      "Prototype 835: Label = 2, Class Size = 1\n",
      "Prototype 836: Label = 2, Class Size = 1\n",
      "Prototype 837: Label = 2, Class Size = 1\n",
      "Prototype 838: Label = 1, Class Size = 1\n",
      "Prototype 839: Label = 2, Class Size = 1\n",
      "Prototype 840: Label = 2, Class Size = 1\n",
      "Prototype 841: Label = 0, Class Size = 1\n",
      "Prototype 842: Label = 0, Class Size = 1\n",
      "Prototype 843: Label = 2, Class Size = 1\n",
      "Prototype 844: Label = 2, Class Size = 1\n",
      "Prototype 845: Label = 2, Class Size = 2\n",
      "Prototype 846: Label = 1, Class Size = 1\n",
      "Prototype 847: Label = 2, Class Size = 1\n",
      "Prototype 848: Label = 2, Class Size = 1\n",
      "Prototype 849: Label = 0, Class Size = 1\n",
      "Prototype 850: Label = 1, Class Size = 1\n",
      "Prototype 851: Label = 1, Class Size = 1\n",
      "Prototype 852: Label = 1, Class Size = 2\n",
      "Prototype 853: Label = 0, Class Size = 1\n",
      "Prototype 854: Label = 2, Class Size = 1\n",
      "Prototype 855: Label = 2, Class Size = 1\n",
      "Prototype 856: Label = 1, Class Size = 1\n",
      "Prototype 857: Label = 2, Class Size = 1\n",
      "Prototype 858: Label = 0, Class Size = 1\n",
      "Prototype 859: Label = 2, Class Size = 2\n",
      "Prototype 860: Label = 1, Class Size = 2\n",
      "Prototype 861: Label = 0, Class Size = 1\n",
      "Prototype 862: Label = 2, Class Size = 1\n",
      "Prototype 863: Label = 2, Class Size = 1\n",
      "Prototype 864: Label = 2, Class Size = 1\n",
      "Prototype 865: Label = 2, Class Size = 1\n",
      "Prototype 866: Label = 1, Class Size = 1\n",
      "Prototype 867: Label = 1, Class Size = 1\n",
      "Prototype 868: Label = 2, Class Size = 1\n",
      "Prototype 869: Label = 1, Class Size = 1\n",
      "Prototype 870: Label = 2, Class Size = 1\n",
      "Prototype 871: Label = 2, Class Size = 1\n",
      "Prototype 872: Label = 2, Class Size = 2\n",
      "Prototype 873: Label = 2, Class Size = 1\n",
      "Prototype 874: Label = 1, Class Size = 1\n",
      "Prototype 875: Label = 0, Class Size = 1\n",
      "Prototype 876: Label = 2, Class Size = 1\n",
      "Prototype 877: Label = 2, Class Size = 1\n",
      "Prototype 878: Label = 2, Class Size = 1\n",
      "Prototype 879: Label = 1, Class Size = 1\n",
      "Prototype 880: Label = 0, Class Size = 1\n",
      "Prototype 881: Label = 2, Class Size = 2\n",
      "Prototype 882: Label = 2, Class Size = 1\n",
      "Prototype 883: Label = 1, Class Size = 1\n",
      "Prototype 884: Label = 2, Class Size = 1\n",
      "Prototype 885: Label = 2, Class Size = 1\n",
      "Prototype 886: Label = 2, Class Size = 1\n",
      "Prototype 887: Label = 1, Class Size = 1\n",
      "Prototype 888: Label = 2, Class Size = 1\n",
      "Prototype 889: Label = 2, Class Size = 1\n",
      "Prototype 890: Label = 2, Class Size = 1\n",
      "Prototype 891: Label = 2, Class Size = 1\n",
      "Prototype 892: Label = 2, Class Size = 1\n",
      "Prototype 893: Label = 1, Class Size = 1\n",
      "Prototype 894: Label = 0, Class Size = 1\n",
      "Prototype 895: Label = 0, Class Size = 1\n",
      "Prototype 896: Label = 1, Class Size = 1\n",
      "Prototype 897: Label = 1, Class Size = 2\n",
      "Prototype 898: Label = 2, Class Size = 1\n",
      "Prototype 899: Label = 2, Class Size = 2\n",
      "Prototype 900: Label = 1, Class Size = 1\n",
      "Prototype 901: Label = 2, Class Size = 1\n",
      "Prototype 902: Label = 2, Class Size = 1\n",
      "Prototype 903: Label = 1, Class Size = 1\n",
      "Prototype 904: Label = 0, Class Size = 1\n",
      "Prototype 905: Label = 2, Class Size = 1\n",
      "Prototype 906: Label = 0, Class Size = 1\n",
      "Prototype 907: Label = 1, Class Size = 1\n",
      "Prototype 908: Label = 0, Class Size = 1\n",
      "Prototype 909: Label = 0, Class Size = 1\n",
      "Prototype 910: Label = 0, Class Size = 1\n",
      "Prototype 911: Label = 2, Class Size = 1\n",
      "Prototype 912: Label = 2, Class Size = 2\n",
      "Prototype 913: Label = 0, Class Size = 1\n",
      "Prototype 914: Label = 1, Class Size = 1\n",
      "Prototype 915: Label = 0, Class Size = 1\n",
      "Prototype 916: Label = 0, Class Size = 1\n",
      "Prototype 917: Label = 2, Class Size = 1\n",
      "Prototype 918: Label = 2, Class Size = 1\n",
      "Prototype 919: Label = 2, Class Size = 1\n",
      "Prototype 920: Label = 2, Class Size = 1\n",
      "Prototype 921: Label = 2, Class Size = 1\n",
      "Prototype 922: Label = 0, Class Size = 1\n",
      "Prototype 923: Label = 1, Class Size = 1\n",
      "Prototype 924: Label = 2, Class Size = 1\n",
      "Prototype 925: Label = 2, Class Size = 1\n",
      "Prototype 926: Label = 1, Class Size = 1\n",
      "Prototype 927: Label = 1, Class Size = 1\n",
      "Prototype 928: Label = 2, Class Size = 1\n",
      "Prototype 929: Label = 0, Class Size = 2\n",
      "Prototype 930: Label = 2, Class Size = 2\n",
      "Prototype 931: Label = 2, Class Size = 1\n",
      "Prototype 932: Label = 1, Class Size = 1\n",
      "Prototype 933: Label = 2, Class Size = 1\n",
      "Prototype 934: Label = 2, Class Size = 1\n",
      "Prototype 935: Label = 1, Class Size = 2\n",
      "Prototype 936: Label = 0, Class Size = 1\n",
      "Prototype 937: Label = 2, Class Size = 1\n",
      "Prototype 938: Label = 1, Class Size = 1\n",
      "Prototype 939: Label = 2, Class Size = 1\n",
      "Prototype 940: Label = 2, Class Size = 1\n",
      "Prototype 941: Label = 2, Class Size = 1\n",
      "Prototype 942: Label = 2, Class Size = 1\n",
      "Prototype 943: Label = 1, Class Size = 1\n",
      "Prototype 944: Label = 0, Class Size = 1\n",
      "Prototype 945: Label = 0, Class Size = 1\n",
      "Prototype 946: Label = 2, Class Size = 1\n",
      "Prototype 947: Label = 1, Class Size = 1\n",
      "Prototype 948: Label = 1, Class Size = 1\n",
      "Prototype 949: Label = 2, Class Size = 1\n",
      "Prototype 950: Label = 2, Class Size = 1\n",
      "Prototype 951: Label = 2, Class Size = 1\n",
      "Prototype 952: Label = 2, Class Size = 1\n",
      "Prototype 953: Label = 2, Class Size = 1\n",
      "Prototype 954: Label = 2, Class Size = 1\n",
      "Prototype 955: Label = 2, Class Size = 1\n",
      "Prototype 956: Label = 2, Class Size = 1\n",
      "Prototype 957: Label = 0, Class Size = 1\n",
      "Prototype 958: Label = 0, Class Size = 1\n",
      "Prototype 959: Label = 2, Class Size = 2\n",
      "Prototype 960: Label = 0, Class Size = 1\n",
      "Prototype 961: Label = 0, Class Size = 1\n",
      "Prototype 962: Label = 0, Class Size = 1\n",
      "Prototype 963: Label = 2, Class Size = 2\n",
      "Prototype 964: Label = 0, Class Size = 1\n",
      "Prototype 965: Label = 1, Class Size = 1\n",
      "Prototype 966: Label = 2, Class Size = 1\n",
      "Prototype 967: Label = 2, Class Size = 1\n",
      "Prototype 968: Label = 1, Class Size = 1\n",
      "Prototype 969: Label = 1, Class Size = 1\n",
      "Prototype 970: Label = 1, Class Size = 1\n",
      "Prototype 971: Label = 0, Class Size = 1\n",
      "Prototype 972: Label = 2, Class Size = 1\n",
      "Prototype 973: Label = 2, Class Size = 1\n",
      "Prototype 974: Label = 1, Class Size = 1\n",
      "Prototype 975: Label = 1, Class Size = 1\n",
      "Prototype 976: Label = 0, Class Size = 1\n",
      "Prototype 977: Label = 1, Class Size = 1\n",
      "Prototype 978: Label = 1, Class Size = 1\n",
      "Prototype 979: Label = 2, Class Size = 1\n",
      "Prototype 980: Label = 2, Class Size = 1\n",
      "Prototype 981: Label = 0, Class Size = 1\n",
      "Prototype 982: Label = 0, Class Size = 1\n",
      "Prototype 983: Label = 0, Class Size = 1\n",
      "Prototype 984: Label = 2, Class Size = 1\n",
      "Prototype 985: Label = 2, Class Size = 1\n",
      "Prototype 986: Label = 0, Class Size = 1\n",
      "Prototype 987: Label = 2, Class Size = 1\n",
      "Prototype 988: Label = 0, Class Size = 1\n",
      "Prototype 989: Label = 2, Class Size = 1\n",
      "Prototype 990: Label = 2, Class Size = 1\n",
      "Prototype 991: Label = 0, Class Size = 1\n",
      "Prototype 992: Label = 1, Class Size = 2\n",
      "Prototype 993: Label = 2, Class Size = 1\n",
      "Prototype 994: Label = 1, Class Size = 1\n",
      "Prototype 995: Label = 1, Class Size = 1\n",
      "Prototype 996: Label = 1, Class Size = 1\n",
      "Prototype 997: Label = 2, Class Size = 1\n",
      "Prototype 998: Label = 1, Class Size = 1\n",
      "Prototype 999: Label = 1, Class Size = 1\n",
      "Prototype 1000: Label = 2, Class Size = 1\n",
      "Prototype 1001: Label = 2, Class Size = 1\n",
      "Prototype 1002: Label = 2, Class Size = 1\n",
      "Prototype 1003: Label = 2, Class Size = 1\n",
      "Prototype 1004: Label = 2, Class Size = 2\n",
      "Prototype 1005: Label = 0, Class Size = 1\n",
      "Prototype 1006: Label = 2, Class Size = 1\n",
      "Prototype 1007: Label = 2, Class Size = 1\n",
      "Prototype 1008: Label = 0, Class Size = 1\n",
      "Prototype 1009: Label = 2, Class Size = 1\n",
      "Prototype 1010: Label = 1, Class Size = 1\n",
      "Prototype 1011: Label = 2, Class Size = 2\n",
      "Prototype 1012: Label = 0, Class Size = 1\n",
      "Prototype 1013: Label = 1, Class Size = 1\n",
      "Prototype 1014: Label = 0, Class Size = 2\n",
      "Prototype 1015: Label = 0, Class Size = 1\n",
      "Prototype 1016: Label = 1, Class Size = 1\n",
      "Prototype 1017: Label = 2, Class Size = 1\n",
      "Prototype 1018: Label = 2, Class Size = 1\n",
      "Prototype 1019: Label = 2, Class Size = 1\n",
      "Prototype 1020: Label = 2, Class Size = 1\n",
      "Prototype 1021: Label = 2, Class Size = 1\n",
      "Prototype 1022: Label = 1, Class Size = 1\n",
      "Prototype 1023: Label = 0, Class Size = 1\n",
      "Prototype 1024: Label = 2, Class Size = 1\n",
      "Prototype 1025: Label = 2, Class Size = 1\n",
      "Prototype 1026: Label = 1, Class Size = 2\n",
      "Prototype 1027: Label = 0, Class Size = 1\n",
      "Prototype 1028: Label = 2, Class Size = 1\n",
      "Prototype 1029: Label = 2, Class Size = 1\n",
      "Prototype 1030: Label = 1, Class Size = 1\n",
      "Prototype 1031: Label = 1, Class Size = 1\n",
      "Prototype 1032: Label = 2, Class Size = 1\n",
      "Prototype 1033: Label = 2, Class Size = 1\n",
      "Prototype 1034: Label = 1, Class Size = 1\n",
      "Prototype 1035: Label = 1, Class Size = 1\n",
      "Prototype 1036: Label = 2, Class Size = 1\n",
      "Prototype 1037: Label = 1, Class Size = 1\n",
      "Prototype 1038: Label = 2, Class Size = 1\n",
      "Prototype 1039: Label = 1, Class Size = 1\n",
      "Prototype 1040: Label = 1, Class Size = 1\n",
      "Prototype 1041: Label = 0, Class Size = 2\n",
      "Prototype 1042: Label = 1, Class Size = 1\n",
      "Prototype 1043: Label = 0, Class Size = 1\n",
      "Prototype 1044: Label = 1, Class Size = 1\n",
      "Prototype 1045: Label = 2, Class Size = 1\n",
      "Prototype 1046: Label = 2, Class Size = 1\n",
      "Prototype 1047: Label = 0, Class Size = 1\n",
      "Prototype 1048: Label = 0, Class Size = 1\n",
      "Prototype 1049: Label = 0, Class Size = 1\n",
      "Prototype 1050: Label = 0, Class Size = 1\n",
      "Prototype 1051: Label = 0, Class Size = 1\n",
      "Prototype 1052: Label = 2, Class Size = 1\n",
      "Prototype 1053: Label = 2, Class Size = 1\n",
      "Prototype 1054: Label = 1, Class Size = 1\n",
      "Prototype 1055: Label = 1, Class Size = 1\n",
      "Prototype 1056: Label = 1, Class Size = 1\n",
      "Prototype 1057: Label = 0, Class Size = 1\n",
      "Prototype 1058: Label = 2, Class Size = 1\n",
      "Prototype 1059: Label = 2, Class Size = 1\n",
      "Prototype 1060: Label = 1, Class Size = 1\n",
      "Prototype 1061: Label = 0, Class Size = 1\n",
      "Prototype 1062: Label = 2, Class Size = 1\n",
      "Prototype 1063: Label = 2, Class Size = 1\n",
      "Prototype 1064: Label = 1, Class Size = 1\n",
      "Prototype 1065: Label = 2, Class Size = 1\n",
      "Prototype 1066: Label = 2, Class Size = 1\n",
      "Prototype 1067: Label = 1, Class Size = 1\n",
      "Prototype 1068: Label = 1, Class Size = 1\n",
      "Prototype 1069: Label = 2, Class Size = 1\n",
      "Prototype 1070: Label = 0, Class Size = 1\n",
      "Prototype 1071: Label = 1, Class Size = 2\n",
      "Prototype 1072: Label = 2, Class Size = 1\n",
      "Prototype 1073: Label = 2, Class Size = 1\n",
      "Prototype 1074: Label = 1, Class Size = 1\n",
      "Prototype 1075: Label = 0, Class Size = 1\n",
      "Prototype 1076: Label = 1, Class Size = 1\n",
      "Prototype 1077: Label = 2, Class Size = 1\n",
      "Prototype 1078: Label = 2, Class Size = 2\n",
      "Prototype 1079: Label = 1, Class Size = 1\n",
      "Prototype 1080: Label = 2, Class Size = 1\n",
      "Prototype 1081: Label = 2, Class Size = 2\n",
      "Prototype 1082: Label = 2, Class Size = 1\n",
      "Prototype 1083: Label = 0, Class Size = 1\n",
      "Prototype 1084: Label = 1, Class Size = 1\n",
      "Prototype 1085: Label = 2, Class Size = 1\n",
      "Prototype 1086: Label = 0, Class Size = 1\n",
      "Prototype 1087: Label = 2, Class Size = 1\n",
      "Prototype 1088: Label = 0, Class Size = 1\n",
      "Prototype 1089: Label = 1, Class Size = 1\n",
      "Prototype 1090: Label = 1, Class Size = 2\n",
      "Prototype 1091: Label = 1, Class Size = 1\n",
      "Prototype 1092: Label = 2, Class Size = 1\n",
      "Prototype 1093: Label = 2, Class Size = 1\n",
      "Prototype 1094: Label = 2, Class Size = 1\n",
      "Prototype 1095: Label = 1, Class Size = 1\n",
      "Prototype 1096: Label = 1, Class Size = 1\n",
      "Prototype 1097: Label = 2, Class Size = 1\n",
      "Prototype 1098: Label = 2, Class Size = 1\n",
      "Prototype 1099: Label = 2, Class Size = 1\n",
      "Prototype 1100: Label = 0, Class Size = 1\n",
      "Prototype 1101: Label = 1, Class Size = 1\n",
      "Prototype 1102: Label = 2, Class Size = 2\n",
      "Prototype 1103: Label = 2, Class Size = 1\n",
      "Prototype 1104: Label = 0, Class Size = 1\n",
      "Prototype 1105: Label = 2, Class Size = 1\n",
      "Prototype 1106: Label = 1, Class Size = 2\n",
      "Prototype 1107: Label = 2, Class Size = 1\n",
      "Prototype 1108: Label = 1, Class Size = 1\n",
      "Prototype 1109: Label = 1, Class Size = 1\n",
      "Prototype 1110: Label = 1, Class Size = 1\n",
      "Prototype 1111: Label = 1, Class Size = 1\n",
      "Prototype 1112: Label = 1, Class Size = 1\n",
      "Prototype 1113: Label = 0, Class Size = 1\n",
      "Prototype 1114: Label = 0, Class Size = 2\n",
      "Prototype 1115: Label = 1, Class Size = 2\n",
      "Prototype 1116: Label = 2, Class Size = 1\n",
      "Prototype 1117: Label = 0, Class Size = 1\n",
      "Prototype 1118: Label = 2, Class Size = 1\n",
      "Prototype 1119: Label = 2, Class Size = 2\n",
      "Prototype 1120: Label = 2, Class Size = 2\n",
      "Prototype 1121: Label = 2, Class Size = 1\n",
      "Prototype 1122: Label = 1, Class Size = 1\n",
      "Prototype 1123: Label = 2, Class Size = 1\n",
      "Prototype 1124: Label = 0, Class Size = 1\n",
      "Prototype 1125: Label = 2, Class Size = 1\n",
      "Prototype 1126: Label = 2, Class Size = 1\n",
      "Prototype 1127: Label = 2, Class Size = 1\n",
      "Prototype 1128: Label = 2, Class Size = 1\n",
      "Prototype 1129: Label = 2, Class Size = 1\n",
      "Prototype 1130: Label = 2, Class Size = 2\n",
      "Prototype 1131: Label = 0, Class Size = 1\n",
      "Prototype 1132: Label = 1, Class Size = 1\n",
      "Prototype 1133: Label = 2, Class Size = 1\n",
      "Prototype 1134: Label = 2, Class Size = 1\n",
      "Prototype 1135: Label = 2, Class Size = 1\n",
      "Prototype 1136: Label = 0, Class Size = 1\n",
      "Prototype 1137: Label = 2, Class Size = 1\n",
      "Prototype 1138: Label = 2, Class Size = 1\n",
      "Prototype 1139: Label = 2, Class Size = 1\n",
      "Prototype 1140: Label = 2, Class Size = 2\n",
      "Prototype 1141: Label = 2, Class Size = 1\n",
      "Prototype 1142: Label = 2, Class Size = 1\n",
      "Prototype 1143: Label = 0, Class Size = 1\n",
      "Prototype 1144: Label = 2, Class Size = 1\n",
      "Prototype 1145: Label = 1, Class Size = 1\n",
      "Prototype 1146: Label = 2, Class Size = 1\n",
      "Prototype 1147: Label = 1, Class Size = 1\n",
      "Prototype 1148: Label = 2, Class Size = 1\n",
      "Prototype 1149: Label = 2, Class Size = 1\n",
      "Prototype 1150: Label = 2, Class Size = 1\n",
      "Prototype 1151: Label = 0, Class Size = 1\n",
      "Prototype 1152: Label = 1, Class Size = 1\n",
      "Prototype 1153: Label = 0, Class Size = 1\n",
      "Prototype 1154: Label = 1, Class Size = 1\n",
      "Prototype 1155: Label = 1, Class Size = 1\n",
      "Prototype 1156: Label = 2, Class Size = 1\n",
      "Prototype 1157: Label = 1, Class Size = 1\n",
      "Prototype 1158: Label = 2, Class Size = 1\n",
      "Prototype 1159: Label = 2, Class Size = 1\n",
      "Prototype 1160: Label = 0, Class Size = 1\n",
      "Prototype 1161: Label = 2, Class Size = 1\n",
      "Prototype 1162: Label = 1, Class Size = 1\n",
      "Prototype 1163: Label = 1, Class Size = 1\n",
      "Prototype 1164: Label = 2, Class Size = 1\n",
      "Prototype 1165: Label = 0, Class Size = 1\n",
      "Prototype 1166: Label = 2, Class Size = 2\n",
      "Prototype 1167: Label = 1, Class Size = 1\n",
      "Prototype 1168: Label = 0, Class Size = 1\n",
      "Prototype 1169: Label = 2, Class Size = 1\n",
      "Prototype 1170: Label = 2, Class Size = 1\n",
      "Prototype 1171: Label = 2, Class Size = 1\n",
      "Prototype 1172: Label = 2, Class Size = 1\n",
      "Prototype 1173: Label = 2, Class Size = 1\n",
      "Prototype 1174: Label = 1, Class Size = 1\n",
      "Prototype 1175: Label = 2, Class Size = 1\n",
      "Prototype 1176: Label = 2, Class Size = 1\n",
      "Prototype 1177: Label = 1, Class Size = 1\n",
      "Prototype 1178: Label = 2, Class Size = 1\n",
      "Prototype 1179: Label = 0, Class Size = 1\n",
      "Prototype 1180: Label = 1, Class Size = 1\n",
      "Prototype 1181: Label = 2, Class Size = 1\n",
      "Prototype 1182: Label = 1, Class Size = 2\n",
      "Prototype 1183: Label = 0, Class Size = 1\n",
      "Prototype 1184: Label = 2, Class Size = 1\n",
      "Prototype 1185: Label = 2, Class Size = 1\n",
      "Prototype 1186: Label = 1, Class Size = 1\n",
      "Prototype 1187: Label = 1, Class Size = 1\n",
      "Prototype 1188: Label = 2, Class Size = 1\n",
      "Prototype 1189: Label = 1, Class Size = 1\n",
      "Prototype 1190: Label = 2, Class Size = 1\n",
      "Prototype 1191: Label = 2, Class Size = 1\n",
      "Prototype 1192: Label = 2, Class Size = 1\n",
      "Prototype 1193: Label = 0, Class Size = 1\n",
      "Prototype 1194: Label = 2, Class Size = 1\n",
      "Prototype 1195: Label = 0, Class Size = 1\n",
      "Prototype 1196: Label = 2, Class Size = 1\n",
      "Prototype 1197: Label = 2, Class Size = 1\n",
      "Prototype 1198: Label = 1, Class Size = 1\n",
      "Prototype 1199: Label = 2, Class Size = 1\n",
      "Prototype 1200: Label = 1, Class Size = 1\n",
      "Prototype 1201: Label = 1, Class Size = 1\n",
      "Prototype 1202: Label = 1, Class Size = 1\n",
      "Prototype 1203: Label = 1, Class Size = 1\n",
      "Prototype 1204: Label = 1, Class Size = 1\n",
      "Prototype 1205: Label = 2, Class Size = 2\n",
      "Prototype 1206: Label = 2, Class Size = 1\n",
      "Prototype 1207: Label = 0, Class Size = 1\n",
      "Prototype 1208: Label = 2, Class Size = 1\n",
      "Prototype 1209: Label = 0, Class Size = 1\n",
      "Prototype 1210: Label = 2, Class Size = 1\n",
      "Prototype 1211: Label = 2, Class Size = 1\n",
      "Prototype 1212: Label = 2, Class Size = 1\n",
      "Prototype 1213: Label = 2, Class Size = 1\n",
      "Prototype 1214: Label = 2, Class Size = 1\n",
      "Prototype 1215: Label = 1, Class Size = 1\n",
      "Prototype 1216: Label = 1, Class Size = 1\n",
      "Prototype 1217: Label = 0, Class Size = 1\n",
      "Prototype 1218: Label = 1, Class Size = 1\n",
      "Prototype 1219: Label = 0, Class Size = 1\n",
      "Prototype 1220: Label = 2, Class Size = 1\n",
      "Prototype 1221: Label = 1, Class Size = 1\n",
      "Prototype 1222: Label = 2, Class Size = 1\n",
      "Prototype 1223: Label = 2, Class Size = 1\n",
      "Prototype 1224: Label = 1, Class Size = 1\n",
      "Prototype 1225: Label = 0, Class Size = 1\n",
      "Prototype 1226: Label = 2, Class Size = 1\n",
      "Prototype 1227: Label = 2, Class Size = 1\n",
      "Prototype 1228: Label = 2, Class Size = 1\n",
      "Prototype 1229: Label = 1, Class Size = 1\n",
      "Prototype 1230: Label = 0, Class Size = 1\n",
      "Prototype 1231: Label = 1, Class Size = 1\n",
      "Prototype 1232: Label = 0, Class Size = 1\n",
      "Prototype 1233: Label = 1, Class Size = 1\n",
      "Prototype 1234: Label = 0, Class Size = 1\n",
      "Prototype 1235: Label = 1, Class Size = 1\n",
      "Prototype 1236: Label = 0, Class Size = 1\n",
      "Prototype 1237: Label = 2, Class Size = 1\n",
      "Prototype 1238: Label = 2, Class Size = 1\n",
      "Prototype 1239: Label = 1, Class Size = 1\n",
      "Prototype 1240: Label = 2, Class Size = 1\n",
      "Prototype 1241: Label = 2, Class Size = 1\n",
      "Prototype 1242: Label = 1, Class Size = 1\n",
      "Prototype 1243: Label = 2, Class Size = 1\n",
      "Prototype 1244: Label = 1, Class Size = 1\n",
      "Prototype 1245: Label = 1, Class Size = 1\n",
      "Prototype 1246: Label = 1, Class Size = 1\n",
      "Prototype 1247: Label = 2, Class Size = 1\n",
      "Prototype 1248: Label = 2, Class Size = 1\n",
      "Prototype 1249: Label = 2, Class Size = 1\n",
      "Prototype 1250: Label = 2, Class Size = 1\n",
      "Prototype 1251: Label = 0, Class Size = 1\n",
      "Prototype 1252: Label = 2, Class Size = 1\n",
      "Prototype 1253: Label = 2, Class Size = 1\n",
      "Prototype 1254: Label = 1, Class Size = 1\n",
      "Prototype 1255: Label = 1, Class Size = 1\n",
      "Prototype 1256: Label = 2, Class Size = 1\n",
      "Prototype 1257: Label = 2, Class Size = 1\n",
      "Prototype 1258: Label = 0, Class Size = 1\n",
      "Prototype 1259: Label = 0, Class Size = 1\n",
      "Prototype 1260: Label = 2, Class Size = 1\n",
      "Prototype 1261: Label = 1, Class Size = 1\n",
      "Prototype 1262: Label = 1, Class Size = 1\n",
      "Prototype 1263: Label = 2, Class Size = 1\n",
      "Prototype 1264: Label = 1, Class Size = 1\n",
      "Prototype 1265: Label = 1, Class Size = 1\n",
      "Prototype 1266: Label = 1, Class Size = 2\n",
      "Prototype 1267: Label = 0, Class Size = 1\n",
      "Prototype 1268: Label = 2, Class Size = 1\n",
      "Prototype 1269: Label = 0, Class Size = 1\n",
      "Prototype 1270: Label = 1, Class Size = 2\n",
      "Prototype 1271: Label = 0, Class Size = 1\n",
      "Prototype 1272: Label = 2, Class Size = 1\n",
      "Prototype 1273: Label = 1, Class Size = 1\n",
      "Prototype 1274: Label = 2, Class Size = 1\n",
      "Prototype 1275: Label = 1, Class Size = 1\n",
      "Prototype 1276: Label = 2, Class Size = 1\n",
      "Prototype 1277: Label = 1, Class Size = 1\n",
      "Prototype 1278: Label = 1, Class Size = 1\n",
      "Prototype 1279: Label = 2, Class Size = 1\n",
      "Prototype 1280: Label = 1, Class Size = 1\n",
      "Prototype 1281: Label = 2, Class Size = 1\n",
      "Prototype 1282: Label = 0, Class Size = 1\n",
      "Prototype 1283: Label = 1, Class Size = 1\n",
      "Prototype 1284: Label = 2, Class Size = 1\n",
      "Prototype 1285: Label = 0, Class Size = 1\n",
      "Prototype 1286: Label = 2, Class Size = 1\n",
      "Prototype 1287: Label = 1, Class Size = 1\n",
      "Prototype 1288: Label = 0, Class Size = 1\n",
      "Prototype 1289: Label = 2, Class Size = 1\n",
      "Prototype 1290: Label = 1, Class Size = 1\n",
      "Prototype 1291: Label = 0, Class Size = 1\n",
      "Prototype 1292: Label = 1, Class Size = 1\n",
      "Prototype 1293: Label = 0, Class Size = 1\n",
      "Prototype 1294: Label = 1, Class Size = 1\n",
      "Prototype 1295: Label = 2, Class Size = 1\n",
      "Prototype 1296: Label = 2, Class Size = 1\n",
      "Prototype 1297: Label = 1, Class Size = 1\n",
      "Prototype 1298: Label = 2, Class Size = 1\n",
      "Prototype 1299: Label = 2, Class Size = 1\n",
      "Prototype 1300: Label = 2, Class Size = 1\n",
      "Prototype 1301: Label = 1, Class Size = 1\n",
      "Prototype 1302: Label = 2, Class Size = 1\n",
      "Prototype 1303: Label = 0, Class Size = 1\n",
      "Prototype 1304: Label = 2, Class Size = 1\n",
      "Prototype 1305: Label = 2, Class Size = 1\n",
      "Prototype 1306: Label = 2, Class Size = 1\n",
      "Prototype 1307: Label = 2, Class Size = 1\n",
      "Prototype 1308: Label = 0, Class Size = 1\n",
      "Prototype 1309: Label = 1, Class Size = 1\n",
      "Prototype 1310: Label = 0, Class Size = 1\n",
      "Prototype 1311: Label = 2, Class Size = 1\n",
      "Prototype 1312: Label = 2, Class Size = 1\n",
      "Prototype 1313: Label = 1, Class Size = 1\n",
      "Prototype 1314: Label = 1, Class Size = 1\n",
      "Prototype 1315: Label = 1, Class Size = 1\n",
      "Prototype 1316: Label = 1, Class Size = 1\n",
      "Prototype 1317: Label = 2, Class Size = 1\n",
      "Prototype 1318: Label = 1, Class Size = 1\n",
      "Prototype 1319: Label = 1, Class Size = 1\n",
      "Prototype 1320: Label = 2, Class Size = 1\n",
      "Prototype 1321: Label = 2, Class Size = 1\n",
      "Prototype 1322: Label = 2, Class Size = 1\n",
      "Prototype 1323: Label = 1, Class Size = 1\n",
      "Prototype 1324: Label = 2, Class Size = 1\n",
      "Prototype 1325: Label = 2, Class Size = 1\n",
      "Prototype 1326: Label = 2, Class Size = 1\n",
      "Prototype 1327: Label = 0, Class Size = 1\n",
      "Prototype 1328: Label = 1, Class Size = 1\n",
      "Prototype 1329: Label = 0, Class Size = 1\n",
      "Prototype 1330: Label = 2, Class Size = 1\n",
      "Prototype 1331: Label = 2, Class Size = 1\n",
      "Prototype 1332: Label = 0, Class Size = 1\n",
      "Prototype 1333: Label = 0, Class Size = 1\n",
      "Prototype 1334: Label = 2, Class Size = 1\n",
      "Prototype 1335: Label = 1, Class Size = 1\n",
      "Prototype 1336: Label = 0, Class Size = 1\n",
      "Prototype 1337: Label = 0, Class Size = 1\n",
      "Prototype 1338: Label = 1, Class Size = 1\n",
      "Prototype 1339: Label = 1, Class Size = 1\n",
      "Prototype 1340: Label = 2, Class Size = 1\n",
      "Prototype 1341: Label = 1, Class Size = 1\n",
      "Prototype 1342: Label = 2, Class Size = 1\n",
      "Prototype 1343: Label = 2, Class Size = 1\n",
      "Prototype 1344: Label = 2, Class Size = 1\n",
      "Prototype 1345: Label = 1, Class Size = 1\n",
      "Prototype 1346: Label = 1, Class Size = 1\n",
      "Prototype 1347: Label = 1, Class Size = 1\n",
      "Prototype 1348: Label = 0, Class Size = 1\n",
      "Prototype 1349: Label = 2, Class Size = 1\n",
      "Prototype 1350: Label = 2, Class Size = 1\n",
      "Prototype 1351: Label = 1, Class Size = 1\n",
      "Prototype 1352: Label = 0, Class Size = 2\n",
      "Prototype 1353: Label = 0, Class Size = 1\n",
      "Prototype 1354: Label = 2, Class Size = 2\n",
      "Prototype 1355: Label = 0, Class Size = 1\n",
      "Prototype 1356: Label = 2, Class Size = 1\n",
      "Prototype 1357: Label = 1, Class Size = 1\n",
      "Prototype 1358: Label = 1, Class Size = 1\n",
      "Prototype 1359: Label = 0, Class Size = 1\n",
      "Prototype 1360: Label = 1, Class Size = 1\n",
      "Prototype 1361: Label = 2, Class Size = 1\n",
      "Prototype 1362: Label = 1, Class Size = 1\n",
      "Prototype 1363: Label = 1, Class Size = 1\n",
      "Prototype 1364: Label = 2, Class Size = 1\n",
      "Prototype 1365: Label = 2, Class Size = 1\n",
      "Prototype 1366: Label = 1, Class Size = 1\n",
      "Prototype 1367: Label = 1, Class Size = 1\n",
      "Prototype 1368: Label = 0, Class Size = 1\n",
      "Prototype 1369: Label = 1, Class Size = 1\n",
      "Prototype 1370: Label = 2, Class Size = 1\n",
      "Prototype 1371: Label = 0, Class Size = 1\n",
      "Prototype 1372: Label = 2, Class Size = 1\n",
      "Prototype 1373: Label = 0, Class Size = 1\n",
      "Prototype 1374: Label = 1, Class Size = 1\n",
      "Prototype 1375: Label = 0, Class Size = 1\n",
      "Prototype 1376: Label = 1, Class Size = 1\n",
      "Prototype 1377: Label = 2, Class Size = 1\n",
      "Prototype 1378: Label = 1, Class Size = 1\n",
      "Prototype 1379: Label = 2, Class Size = 1\n",
      "Prototype 1380: Label = 2, Class Size = 1\n",
      "Prototype 1381: Label = 2, Class Size = 1\n",
      "Prototype 1382: Label = 2, Class Size = 1\n",
      "Prototype 1383: Label = 2, Class Size = 1\n",
      "Prototype 1384: Label = 0, Class Size = 1\n",
      "Prototype 1385: Label = 1, Class Size = 1\n",
      "Prototype 1386: Label = 2, Class Size = 1\n",
      "Prototype 1387: Label = 0, Class Size = 1\n",
      "Prototype 1388: Label = 0, Class Size = 1\n",
      "Prototype 1389: Label = 0, Class Size = 1\n",
      "Prototype 1390: Label = 2, Class Size = 1\n",
      "Prototype 1391: Label = 0, Class Size = 1\n",
      "Prototype 1392: Label = 2, Class Size = 1\n",
      "Prototype 1393: Label = 1, Class Size = 1\n",
      "Prototype 1394: Label = 0, Class Size = 1\n",
      "Prototype 1395: Label = 2, Class Size = 1\n",
      "Prototype 1396: Label = 1, Class Size = 1\n",
      "Prototype 1397: Label = 2, Class Size = 1\n",
      "Prototype 1398: Label = 1, Class Size = 1\n",
      "Prototype 1399: Label = 2, Class Size = 1\n",
      "Prototype 1400: Label = 2, Class Size = 1\n",
      "Prototype 1401: Label = 2, Class Size = 1\n",
      "Prototype 1402: Label = 1, Class Size = 1\n",
      "Prototype 1403: Label = 1, Class Size = 1\n",
      "Prototype 1404: Label = 0, Class Size = 1\n",
      "Prototype 1405: Label = 2, Class Size = 1\n",
      "Prototype 1406: Label = 1, Class Size = 1\n",
      "Prototype 1407: Label = 2, Class Size = 1\n",
      "Prototype 1408: Label = 1, Class Size = 1\n",
      "Prototype 1409: Label = 0, Class Size = 1\n",
      "Prototype 1410: Label = 2, Class Size = 1\n",
      "Prototype 1411: Label = 2, Class Size = 1\n",
      "Test Accuracy: 0.9101449275362319\n",
      "Weighted F1 Score: 0.91055045635266\n",
      "Classification Report:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.int64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[202], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeighted F1 Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, f1)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification Report:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2598\u001b[0m, in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2596\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2597\u001b[0m     longest_last_line_heading \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted avg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2598\u001b[0m     name_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2599\u001b[0m     width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(name_width, \u001b[38;5;28mlen\u001b[39m(longest_last_line_heading), digits)\n\u001b[1;32m   2600\u001b[0m     head_fmt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m:>\u001b[39m\u001b[38;5;132;01m{width}\u001b[39;00m\u001b[38;5;124ms} \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{:>9}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(headers)\n",
      "File \u001b[0;32m~/Multi Modal Project/multimodal_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2598\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2596\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2597\u001b[0m     longest_last_line_heading \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted avg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2598\u001b[0m     name_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcn\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m cn \u001b[38;5;129;01min\u001b[39;00m target_names)\n\u001b[1;32m   2599\u001b[0m     width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(name_width, \u001b[38;5;28mlen\u001b[39m(longest_last_line_heading), digits)\n\u001b[1;32m   2600\u001b[0m     head_fmt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m:>\u001b[39m\u001b[38;5;132;01m{width}\u001b[39;00m\u001b[38;5;124ms} \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{:>9}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(headers)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.int64' has no len()"
     ]
    }
   ],
   "source": [
    "# Step 5: Compute Prototype Vectors and Assign Labels via Majority Voting\n",
    "rep_vectors_list = []\n",
    "rep_labels_list = []\n",
    "class_sizes = []\n",
    "\n",
    "for cls in tolerance_classes:\n",
    "    idx_list = list(cls)\n",
    "    # Extract the corresponding normalized FTTransformer features.\n",
    "    class_feats = X_train_norm[idx_list]  # shape: (num_samples_in_class, 192)\n",
    "    # Compute the prototype (mean vector) for the class.\n",
    "    rep_vector = class_feats.mean(axis=0)\n",
    "    rep_vectors_list.append(rep_vector)\n",
    "    \n",
    "    # Use majority voting for the label (even if the class is impure).\n",
    "    cls_labels = y_train[idx_list]\n",
    "    label_counts = Counter(cls_labels)\n",
    "    majority_label, majority_count = label_counts.most_common(1)[0]\n",
    "    rep_labels_list.append(majority_label)\n",
    "    class_sizes.append(len(idx_list))\n",
    "\n",
    "rep_vectors = np.vstack(rep_vectors_list)  # shape: (num_tolerance_classes, 192)\n",
    "rep_labels = np.array(rep_labels_list)\n",
    "\n",
    "print(\"Number of prototypes (tolerance classes):\", rep_vectors.shape[0])\n",
    "for i in range(rep_vectors.shape[0]):\n",
    "    print(f\"Prototype {i+1}: Label = {rep_labels[i]}, Class Size = {class_sizes[i]}\")\n",
    "\n",
    "# Step 6: Testing Phase – Classify Test Samples Using the Prototypes\n",
    "# Compute cosine similarity between each test sample and each prototype.\n",
    "sim_matrix_test = cosine_similarity(X_test_norm, rep_vectors)  # shape: (n_test, num_prototypes)\n",
    "\n",
    "# For each test sample, find the prototype with the highest cosine similarity.\n",
    "predicted_indices = np.argmax(sim_matrix_test, axis=1)\n",
    "predicted_labels = rep_labels[predicted_indices]\n",
    "\n",
    "# Evaluate predictions.\n",
    "acc = accuracy_score(y_test, predicted_labels)\n",
    "f1 = f1_score(y_test, predicted_labels, average='weighted')\n",
    "\n",
    "print(\"Test Accuracy:\", acc)\n",
    "print(\"Weighted F1 Score:\", f1)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predicted_labels, target_names=list(np.unique(y_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "64d69839-409f-4f77-9754-d3ce089b6397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9101449275362319\n",
      "Weighted F1 Score: 0.91055045635266\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.85      0.82        72\n",
      "           1       0.97      0.99      0.98       106\n",
      "           2       0.93      0.89      0.91       167\n",
      "\n",
      "    accuracy                           0.91       345\n",
      "   macro avg       0.90      0.91      0.90       345\n",
      "weighted avg       0.91      0.91      0.91       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Compute cosine similarity between each test sample and each prototype.\n",
    "sim_matrix = cosine_similarity(X_test_norm, rep_vectors)  # shape: (n_test, num_prototypes)\n",
    "\n",
    "# For each test sample, find the prototype with the highest cosine similarity.\n",
    "predicted_indices = np.argmax(sim_matrix, axis=1)\n",
    "\n",
    "# Map these indices to the corresponding labels.\n",
    "predicted_labels = rep_labels[predicted_indices]\n",
    "\n",
    "# Evaluate predictions.\n",
    "acc = accuracy_score(y_test, predicted_labels)\n",
    "f1 = f1_score(y_test, predicted_labels, average='weighted')\n",
    "\n",
    "print(\"Test Accuracy:\", acc)\n",
    "print(\"Weighted F1 Score:\", f1)\n",
    "\n",
    "# Convert target names to strings.\n",
    "target_names = [str(x) for x in np.unique(y_train)]\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predicted_labels, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd5c832-1b5b-423f-a337-5f6b8b4d7f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
