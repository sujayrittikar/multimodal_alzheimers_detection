{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51807170-1d84-4c68-b8ff-6827b6599159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import scipy.ndimage\n",
    "from monai.networks.nets import resnet18\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e579606-2ae8-43dd-95f9-9f9f25530b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_files_with_substring(directory, substring):\n",
    "    matching_files = [f for f in os.listdir(directory) if substring in f]\n",
    "    return matching_files\n",
    "\n",
    "def get_nib_image(adni_file_name):\n",
    "    return nib.load(adni_file_name).get_fdata()\n",
    "\n",
    "def visualize_image(nib_image):\n",
    "    plt.imshow(nib_image[:,:,nib_image.shape[2]//2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52f021e5-6aab-4e1c-89d1-6453028f7d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_file_names_for_subject(subject_id, date=None):\n",
    "    os.path.expanduser(\"~/adni_flat_dataset/adni_flat_dataset\")\n",
    "    dir_ = \"/home/rittikar-s/adni_flat_dataset/adni_flat_dataset\"\n",
    "    files = find_files_with_substring(dir_, subject_id)\n",
    "    if date:\n",
    "        files = [file for file in files if date in file]\n",
    "    file_paths = [f\"{dir_}/{file}\" for file in files]\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f01308a7-4fa8-4e4a-b42a-ccb401de8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"ADNI1_Complete_1Yr_1.5T_1_26_2025.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c4be2a0-6cf7-422c-bb27-23a77776d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets.vitautoenc import ViTAutoEnc\n",
    "\n",
    "vit_model = ViTAutoEnc(in_channels=1, patch_size=(16,16,16), img_size=(128,128,128))\n",
    "\n",
    "def get_vit_embedding(img):\n",
    "    return vit_model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9723c9f4-ec96-41be-b7c8-9f24de00ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NiftiDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, target_shape=(128, 128, 128)):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def preprocess_nifti(self, nifti_path, threshold_value=80):\n",
    "        # Load the NIfTI file\n",
    "        img = nib.load(nifti_path).get_fdata()\n",
    "    \n",
    "        # Resize the image to the target shape\n",
    "        img_resized = scipy.ndimage.zoom(img, np.array(self.target_shape) / np.array(img.shape), order=1)\n",
    "    \n",
    "        # Normalize intensity to [0, 1] for neural network\n",
    "        img_normalized = (img_resized - np.min(img_resized)) / (np.max(img_resized) - np.min(img_resized) + 1e-8)\n",
    "        img_normalized = (img_normalized * 255).astype(np.uint8)  # You can adjust this based on model input needs\n",
    "    \n",
    "        # Create an empty array to store the processed result\n",
    "        processed_img = np.zeros_like(img_normalized)\n",
    "    \n",
    "        # Iterate over each slice in the z-axis (assuming this is a 3D image)\n",
    "        for z in range(img_normalized.shape[2]):\n",
    "            slice_img = img_normalized[:, :, z]\n",
    "            \n",
    "            # Apply Sharpening to the slice using a kernel\n",
    "            sharpening_kernel = np.array([[0, -1, 0], [-1, 7, -1], [0, -1, 0]])\n",
    "            sharpened_slice = cv2.filter2D(slice_img, -1, sharpening_kernel)\n",
    "    \n",
    "            # Threshold the low-intensity regions (set them to black)\n",
    "            thresholded_slice = np.where(sharpened_slice < threshold_value, 0, sharpened_slice)\n",
    "            \n",
    "            # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization) to the thresholded image\n",
    "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n",
    "            clahe_slice = clahe.apply(thresholded_slice)\n",
    "    \n",
    "            # Store the processed slice back into the 3D image\n",
    "            processed_img[:, :, z] = clahe_slice\n",
    "    \n",
    "        # Convert the processed image to a tensor and add a channel dimension (assuming 1 channel)\n",
    "        return torch.tensor(processed_img, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.preprocess_nifti(self.image_paths[idx])\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        embedding = get_vit_embedding(image.reshape(1,1,128,128,128))\n",
    "        return embedding, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62036a9f-e976-4fd0-aef5-4ea71b88072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_label = {\n",
    "    \"CN\": 0,\n",
    "    \"MCI\": 1,\n",
    "    \"AD\": 2\n",
    "}\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    subject = row[\"Subject\"]\n",
    "    date = row[\"Acq Date\"]\n",
    "    date = date.replace(\"/\", \"-\")\n",
    "    image_path = get_image_file_names_for_subject(subject, date)[0]\n",
    "    image_paths.append(image_path)\n",
    "    labels.append(class_to_label[row[\"Group\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc86ce3e-ef67-4492-b81e-4174b4f8084f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2294"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac5306e8-85f3-4a47-9e2d-048b87bdb7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2294"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ff88cf5-1448-4d1f-aa93-964640bfa9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(image_paths, labels, test_size=0.3, random_state=42, stratify=labels)\n",
    "val_paths, test_paths, val_labels, test_labels = train_test_split(test_paths, test_labels, test_size=0.5, random_state=42, stratify=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6107f004-c758-45c2-a4c9-6c352027cb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Batches: 402, Val Batches: 86, Test Batches: 87\n"
     ]
    }
   ],
   "source": [
    "# Create train & test datasets\n",
    "train_dataset = NiftiDataset(train_paths, train_labels)\n",
    "val_dataset = NiftiDataset(val_paths, val_labels)\n",
    "test_dataset = NiftiDataset(test_paths, test_labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, pin_memory=True)\n",
    "\n",
    "print(f\"Train Batches: {len(train_loader)}, Val Batches: {len(val_loader)}, Test Batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "175d94f2-5922-4e14-9b95-f428d1555e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def save_embeddings_hdf5(dataloader, filename):\n",
    "    \"\"\"Save embeddings (from list format) and labels incrementally to an HDF5 file.\"\"\"\n",
    "    with h5py.File(filename, \"w\") as f:\n",
    "        first_batch = True\n",
    "        for i, (embedding_list, label) in enumerate(dataloader):\n",
    "            # Extract the last layer embeddings\n",
    "            embedding_tensor = embedding_list[1][-1]  # Extract final layer embeddings\n",
    "            embedding_tensor = embedding_tensor.cpu().detach()  # Move to CPU\n",
    "            \n",
    "            embedding_numpy = embedding_tensor.numpy()  # Convert to NumPy\n",
    "            label_numpy = label.cpu().numpy()\n",
    "\n",
    "            # Reshape embeddings if needed\n",
    "            embedding_numpy = embedding_numpy.reshape(embedding_numpy.shape[0], -1)  # (4, 1, 512, 768) → (4, 512 * 768)\n",
    "\n",
    "            if first_batch:\n",
    "                # Create expandable datasets with correct shape\n",
    "                f.create_dataset(\"embeddings\", data=embedding_numpy, \n",
    "                                 maxshape=(None, embedding_numpy.shape[1]))  # Now 2D\n",
    "                f.create_dataset(\"labels\", data=label_numpy, maxshape=(None,))\n",
    "                first_batch = False\n",
    "            else:\n",
    "                # Resize and append new embeddings\n",
    "                f[\"embeddings\"].resize((f[\"embeddings\"].shape[0] + embedding_numpy.shape[0]), axis=0)\n",
    "                f[\"embeddings\"][-embedding_numpy.shape[0]:] = embedding_numpy\n",
    "\n",
    "                f[\"labels\"].resize((f[\"labels\"].shape[0] + label_numpy.shape[0]), axis=0)\n",
    "                f[\"labels\"][-label_numpy.shape[0]:] = label_numpy\n",
    "            \n",
    "            print(f\"Saved embeddings for batch: {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a120898e-b143-40a4-a31f-3fabcb857139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings for batch: 1\n",
      "Saved embeddings for batch: 2\n",
      "Saved embeddings for batch: 3\n",
      "Saved embeddings for batch: 4\n",
      "Saved embeddings for batch: 5\n",
      "Saved embeddings for batch: 6\n",
      "Saved embeddings for batch: 7\n",
      "Saved embeddings for batch: 8\n",
      "Saved embeddings for batch: 9\n",
      "Saved embeddings for batch: 10\n",
      "Saved embeddings for batch: 11\n",
      "Saved embeddings for batch: 12\n",
      "Saved embeddings for batch: 13\n",
      "Saved embeddings for batch: 14\n",
      "Saved embeddings for batch: 15\n",
      "Saved embeddings for batch: 16\n",
      "Saved embeddings for batch: 17\n",
      "Saved embeddings for batch: 18\n",
      "Saved embeddings for batch: 19\n",
      "Saved embeddings for batch: 20\n",
      "Saved embeddings for batch: 21\n",
      "Saved embeddings for batch: 22\n",
      "Saved embeddings for batch: 23\n",
      "Saved embeddings for batch: 24\n",
      "Saved embeddings for batch: 25\n",
      "Saved embeddings for batch: 26\n",
      "Saved embeddings for batch: 27\n",
      "Saved embeddings for batch: 28\n",
      "Saved embeddings for batch: 29\n",
      "Saved embeddings for batch: 30\n",
      "Saved embeddings for batch: 31\n",
      "Saved embeddings for batch: 32\n",
      "Saved embeddings for batch: 33\n",
      "Saved embeddings for batch: 34\n",
      "Saved embeddings for batch: 35\n",
      "Saved embeddings for batch: 36\n",
      "Saved embeddings for batch: 37\n",
      "Saved embeddings for batch: 38\n",
      "Saved embeddings for batch: 39\n",
      "Saved embeddings for batch: 40\n",
      "Saved embeddings for batch: 41\n",
      "Saved embeddings for batch: 42\n",
      "Saved embeddings for batch: 43\n",
      "Saved embeddings for batch: 44\n",
      "Saved embeddings for batch: 45\n",
      "Saved embeddings for batch: 46\n",
      "Saved embeddings for batch: 47\n",
      "Saved embeddings for batch: 48\n",
      "Saved embeddings for batch: 49\n",
      "Saved embeddings for batch: 50\n",
      "Saved embeddings for batch: 51\n",
      "Saved embeddings for batch: 52\n",
      "Saved embeddings for batch: 53\n",
      "Saved embeddings for batch: 54\n",
      "Saved embeddings for batch: 55\n",
      "Saved embeddings for batch: 56\n",
      "Saved embeddings for batch: 57\n",
      "Saved embeddings for batch: 58\n",
      "Saved embeddings for batch: 59\n",
      "Saved embeddings for batch: 60\n",
      "Saved embeddings for batch: 61\n",
      "Saved embeddings for batch: 62\n",
      "Saved embeddings for batch: 63\n",
      "Saved embeddings for batch: 64\n",
      "Saved embeddings for batch: 65\n",
      "Saved embeddings for batch: 66\n",
      "Saved embeddings for batch: 67\n",
      "Saved embeddings for batch: 68\n",
      "Saved embeddings for batch: 69\n",
      "Saved embeddings for batch: 70\n",
      "Saved embeddings for batch: 71\n",
      "Saved embeddings for batch: 72\n",
      "Saved embeddings for batch: 73\n",
      "Saved embeddings for batch: 74\n",
      "Saved embeddings for batch: 75\n",
      "Saved embeddings for batch: 76\n",
      "Saved embeddings for batch: 77\n",
      "Saved embeddings for batch: 78\n",
      "Saved embeddings for batch: 79\n",
      "Saved embeddings for batch: 80\n",
      "Saved embeddings for batch: 81\n",
      "Saved embeddings for batch: 82\n",
      "Saved embeddings for batch: 83\n",
      "Saved embeddings for batch: 84\n",
      "Saved embeddings for batch: 85\n",
      "Saved embeddings for batch: 86\n",
      "Saved embeddings for batch: 87\n",
      "Saved embeddings for batch: 88\n",
      "Saved embeddings for batch: 89\n",
      "Saved embeddings for batch: 90\n",
      "Saved embeddings for batch: 91\n",
      "Saved embeddings for batch: 92\n",
      "Saved embeddings for batch: 93\n",
      "Saved embeddings for batch: 94\n",
      "Saved embeddings for batch: 95\n",
      "Saved embeddings for batch: 96\n",
      "Saved embeddings for batch: 97\n",
      "Saved embeddings for batch: 98\n",
      "Saved embeddings for batch: 99\n",
      "Saved embeddings for batch: 100\n",
      "Saved embeddings for batch: 101\n",
      "Saved embeddings for batch: 102\n",
      "Saved embeddings for batch: 103\n",
      "Saved embeddings for batch: 104\n",
      "Saved embeddings for batch: 105\n",
      "Saved embeddings for batch: 106\n",
      "Saved embeddings for batch: 107\n",
      "Saved embeddings for batch: 108\n",
      "Saved embeddings for batch: 109\n",
      "Saved embeddings for batch: 110\n",
      "Saved embeddings for batch: 111\n",
      "Saved embeddings for batch: 112\n",
      "Saved embeddings for batch: 113\n",
      "Saved embeddings for batch: 114\n",
      "Saved embeddings for batch: 115\n",
      "Saved embeddings for batch: 116\n",
      "Saved embeddings for batch: 117\n",
      "Saved embeddings for batch: 118\n",
      "Saved embeddings for batch: 119\n",
      "Saved embeddings for batch: 120\n",
      "Saved embeddings for batch: 121\n",
      "Saved embeddings for batch: 122\n",
      "Saved embeddings for batch: 123\n",
      "Saved embeddings for batch: 124\n",
      "Saved embeddings for batch: 125\n",
      "Saved embeddings for batch: 126\n",
      "Saved embeddings for batch: 127\n",
      "Saved embeddings for batch: 128\n",
      "Saved embeddings for batch: 129\n",
      "Saved embeddings for batch: 130\n",
      "Saved embeddings for batch: 131\n",
      "Saved embeddings for batch: 132\n",
      "Saved embeddings for batch: 133\n",
      "Saved embeddings for batch: 134\n",
      "Saved embeddings for batch: 135\n",
      "Saved embeddings for batch: 136\n",
      "Saved embeddings for batch: 137\n",
      "Saved embeddings for batch: 138\n",
      "Saved embeddings for batch: 139\n",
      "Saved embeddings for batch: 140\n",
      "Saved embeddings for batch: 141\n",
      "Saved embeddings for batch: 142\n",
      "Saved embeddings for batch: 143\n",
      "Saved embeddings for batch: 144\n",
      "Saved embeddings for batch: 145\n",
      "Saved embeddings for batch: 146\n",
      "Saved embeddings for batch: 147\n",
      "Saved embeddings for batch: 148\n",
      "Saved embeddings for batch: 149\n",
      "Saved embeddings for batch: 150\n",
      "Saved embeddings for batch: 151\n",
      "Saved embeddings for batch: 152\n",
      "Saved embeddings for batch: 153\n",
      "Saved embeddings for batch: 154\n",
      "Saved embeddings for batch: 155\n",
      "Saved embeddings for batch: 156\n",
      "Saved embeddings for batch: 157\n",
      "Saved embeddings for batch: 158\n",
      "Saved embeddings for batch: 159\n",
      "Saved embeddings for batch: 160\n",
      "Saved embeddings for batch: 161\n",
      "Saved embeddings for batch: 162\n",
      "Saved embeddings for batch: 163\n",
      "Saved embeddings for batch: 164\n",
      "Saved embeddings for batch: 165\n",
      "Saved embeddings for batch: 166\n",
      "Saved embeddings for batch: 167\n",
      "Saved embeddings for batch: 168\n",
      "Saved embeddings for batch: 169\n",
      "Saved embeddings for batch: 170\n",
      "Saved embeddings for batch: 171\n",
      "Saved embeddings for batch: 172\n",
      "Saved embeddings for batch: 173\n",
      "Saved embeddings for batch: 174\n",
      "Saved embeddings for batch: 175\n",
      "Saved embeddings for batch: 176\n",
      "Saved embeddings for batch: 177\n",
      "Saved embeddings for batch: 178\n",
      "Saved embeddings for batch: 179\n",
      "Saved embeddings for batch: 180\n",
      "Saved embeddings for batch: 181\n",
      "Saved embeddings for batch: 182\n",
      "Saved embeddings for batch: 183\n",
      "Saved embeddings for batch: 184\n",
      "Saved embeddings for batch: 185\n",
      "Saved embeddings for batch: 186\n",
      "Saved embeddings for batch: 187\n",
      "Saved embeddings for batch: 188\n",
      "Saved embeddings for batch: 189\n",
      "Saved embeddings for batch: 190\n",
      "Saved embeddings for batch: 191\n",
      "Saved embeddings for batch: 192\n",
      "Saved embeddings for batch: 193\n",
      "Saved embeddings for batch: 194\n",
      "Saved embeddings for batch: 195\n",
      "Saved embeddings for batch: 196\n",
      "Saved embeddings for batch: 197\n",
      "Saved embeddings for batch: 198\n",
      "Saved embeddings for batch: 199\n",
      "Saved embeddings for batch: 200\n",
      "Saved embeddings for batch: 201\n",
      "Saved embeddings for batch: 202\n",
      "Saved embeddings for batch: 203\n",
      "Saved embeddings for batch: 204\n",
      "Saved embeddings for batch: 205\n",
      "Saved embeddings for batch: 206\n",
      "Saved embeddings for batch: 207\n",
      "Saved embeddings for batch: 208\n",
      "Saved embeddings for batch: 209\n",
      "Saved embeddings for batch: 210\n",
      "Saved embeddings for batch: 211\n",
      "Saved embeddings for batch: 212\n",
      "Saved embeddings for batch: 213\n",
      "Saved embeddings for batch: 214\n",
      "Saved embeddings for batch: 215\n",
      "Saved embeddings for batch: 216\n",
      "Saved embeddings for batch: 217\n",
      "Saved embeddings for batch: 218\n",
      "Saved embeddings for batch: 219\n",
      "Saved embeddings for batch: 220\n",
      "Saved embeddings for batch: 221\n",
      "Saved embeddings for batch: 222\n",
      "Saved embeddings for batch: 223\n",
      "Saved embeddings for batch: 224\n",
      "Saved embeddings for batch: 225\n",
      "Saved embeddings for batch: 226\n",
      "Saved embeddings for batch: 227\n",
      "Saved embeddings for batch: 228\n",
      "Saved embeddings for batch: 229\n",
      "Saved embeddings for batch: 230\n",
      "Saved embeddings for batch: 231\n",
      "Saved embeddings for batch: 232\n",
      "Saved embeddings for batch: 233\n",
      "Saved embeddings for batch: 234\n",
      "Saved embeddings for batch: 235\n",
      "Saved embeddings for batch: 236\n",
      "Saved embeddings for batch: 237\n",
      "Saved embeddings for batch: 238\n",
      "Saved embeddings for batch: 239\n",
      "Saved embeddings for batch: 240\n",
      "Saved embeddings for batch: 241\n",
      "Saved embeddings for batch: 242\n",
      "Saved embeddings for batch: 243\n",
      "Saved embeddings for batch: 244\n",
      "Saved embeddings for batch: 245\n",
      "Saved embeddings for batch: 246\n",
      "Saved embeddings for batch: 247\n",
      "Saved embeddings for batch: 248\n",
      "Saved embeddings for batch: 249\n",
      "Saved embeddings for batch: 250\n",
      "Saved embeddings for batch: 251\n",
      "Saved embeddings for batch: 252\n",
      "Saved embeddings for batch: 253\n",
      "Saved embeddings for batch: 254\n",
      "Saved embeddings for batch: 255\n",
      "Saved embeddings for batch: 256\n",
      "Saved embeddings for batch: 257\n",
      "Saved embeddings for batch: 258\n",
      "Saved embeddings for batch: 259\n",
      "Saved embeddings for batch: 260\n",
      "Saved embeddings for batch: 261\n",
      "Saved embeddings for batch: 262\n",
      "Saved embeddings for batch: 263\n",
      "Saved embeddings for batch: 264\n",
      "Saved embeddings for batch: 265\n",
      "Saved embeddings for batch: 266\n",
      "Saved embeddings for batch: 267\n",
      "Saved embeddings for batch: 268\n",
      "Saved embeddings for batch: 269\n",
      "Saved embeddings for batch: 270\n",
      "Saved embeddings for batch: 271\n",
      "Saved embeddings for batch: 272\n",
      "Saved embeddings for batch: 273\n",
      "Saved embeddings for batch: 274\n",
      "Saved embeddings for batch: 275\n",
      "Saved embeddings for batch: 276\n",
      "Saved embeddings for batch: 277\n",
      "Saved embeddings for batch: 278\n",
      "Saved embeddings for batch: 279\n",
      "Saved embeddings for batch: 280\n",
      "Saved embeddings for batch: 281\n",
      "Saved embeddings for batch: 282\n",
      "Saved embeddings for batch: 283\n",
      "Saved embeddings for batch: 284\n",
      "Saved embeddings for batch: 285\n",
      "Saved embeddings for batch: 286\n",
      "Saved embeddings for batch: 287\n",
      "Saved embeddings for batch: 288\n",
      "Saved embeddings for batch: 289\n",
      "Saved embeddings for batch: 290\n",
      "Saved embeddings for batch: 291\n",
      "Saved embeddings for batch: 292\n",
      "Saved embeddings for batch: 293\n",
      "Saved embeddings for batch: 294\n",
      "Saved embeddings for batch: 295\n",
      "Saved embeddings for batch: 296\n",
      "Saved embeddings for batch: 297\n",
      "Saved embeddings for batch: 298\n",
      "Saved embeddings for batch: 299\n",
      "Saved embeddings for batch: 300\n",
      "Saved embeddings for batch: 301\n",
      "Saved embeddings for batch: 302\n",
      "Saved embeddings for batch: 303\n",
      "Saved embeddings for batch: 304\n",
      "Saved embeddings for batch: 305\n",
      "Saved embeddings for batch: 306\n",
      "Saved embeddings for batch: 307\n",
      "Saved embeddings for batch: 308\n",
      "Saved embeddings for batch: 309\n",
      "Saved embeddings for batch: 310\n",
      "Saved embeddings for batch: 311\n",
      "Saved embeddings for batch: 312\n",
      "Saved embeddings for batch: 313\n",
      "Saved embeddings for batch: 314\n",
      "Saved embeddings for batch: 315\n",
      "Saved embeddings for batch: 316\n",
      "Saved embeddings for batch: 317\n",
      "Saved embeddings for batch: 318\n",
      "Saved embeddings for batch: 319\n",
      "Saved embeddings for batch: 320\n",
      "Saved embeddings for batch: 321\n",
      "Saved embeddings for batch: 322\n",
      "Saved embeddings for batch: 323\n",
      "Saved embeddings for batch: 324\n",
      "Saved embeddings for batch: 325\n",
      "Saved embeddings for batch: 326\n",
      "Saved embeddings for batch: 327\n",
      "Saved embeddings for batch: 328\n",
      "Saved embeddings for batch: 329\n",
      "Saved embeddings for batch: 330\n",
      "Saved embeddings for batch: 331\n",
      "Saved embeddings for batch: 332\n",
      "Saved embeddings for batch: 333\n",
      "Saved embeddings for batch: 334\n",
      "Saved embeddings for batch: 335\n",
      "Saved embeddings for batch: 336\n",
      "Saved embeddings for batch: 337\n",
      "Saved embeddings for batch: 338\n",
      "Saved embeddings for batch: 339\n",
      "Saved embeddings for batch: 340\n",
      "Saved embeddings for batch: 341\n",
      "Saved embeddings for batch: 342\n",
      "Saved embeddings for batch: 343\n",
      "Saved embeddings for batch: 344\n",
      "Saved embeddings for batch: 345\n",
      "Saved embeddings for batch: 346\n",
      "Saved embeddings for batch: 347\n",
      "Saved embeddings for batch: 348\n",
      "Saved embeddings for batch: 349\n",
      "Saved embeddings for batch: 350\n",
      "Saved embeddings for batch: 351\n",
      "Saved embeddings for batch: 352\n",
      "Saved embeddings for batch: 353\n",
      "Saved embeddings for batch: 354\n",
      "Saved embeddings for batch: 355\n",
      "Saved embeddings for batch: 356\n",
      "Saved embeddings for batch: 357\n",
      "Saved embeddings for batch: 358\n",
      "Saved embeddings for batch: 359\n",
      "Saved embeddings for batch: 360\n",
      "Saved embeddings for batch: 361\n",
      "Saved embeddings for batch: 362\n",
      "Saved embeddings for batch: 363\n",
      "Saved embeddings for batch: 364\n",
      "Saved embeddings for batch: 365\n",
      "Saved embeddings for batch: 366\n",
      "Saved embeddings for batch: 367\n",
      "Saved embeddings for batch: 368\n",
      "Saved embeddings for batch: 369\n",
      "Saved embeddings for batch: 370\n",
      "Saved embeddings for batch: 371\n",
      "Saved embeddings for batch: 372\n",
      "Saved embeddings for batch: 373\n",
      "Saved embeddings for batch: 374\n",
      "Saved embeddings for batch: 375\n",
      "Saved embeddings for batch: 376\n",
      "Saved embeddings for batch: 377\n",
      "Saved embeddings for batch: 378\n",
      "Saved embeddings for batch: 379\n",
      "Saved embeddings for batch: 380\n",
      "Saved embeddings for batch: 381\n",
      "Saved embeddings for batch: 382\n",
      "Saved embeddings for batch: 383\n",
      "Saved embeddings for batch: 384\n",
      "Saved embeddings for batch: 385\n",
      "Saved embeddings for batch: 386\n",
      "Saved embeddings for batch: 387\n",
      "Saved embeddings for batch: 388\n",
      "Saved embeddings for batch: 389\n",
      "Saved embeddings for batch: 390\n",
      "Saved embeddings for batch: 391\n",
      "Saved embeddings for batch: 392\n",
      "Saved embeddings for batch: 393\n",
      "Saved embeddings for batch: 394\n",
      "Saved embeddings for batch: 395\n",
      "Saved embeddings for batch: 396\n",
      "Saved embeddings for batch: 397\n",
      "Saved embeddings for batch: 398\n",
      "Saved embeddings for batch: 399\n",
      "Saved embeddings for batch: 400\n",
      "Saved embeddings for batch: 401\n",
      "Saved embeddings for batch: 402\n",
      "Saved embeddings for batch: 1\n",
      "Saved embeddings for batch: 2\n",
      "Saved embeddings for batch: 3\n",
      "Saved embeddings for batch: 4\n",
      "Saved embeddings for batch: 5\n",
      "Saved embeddings for batch: 6\n",
      "Saved embeddings for batch: 7\n",
      "Saved embeddings for batch: 8\n",
      "Saved embeddings for batch: 9\n",
      "Saved embeddings for batch: 10\n",
      "Saved embeddings for batch: 11\n",
      "Saved embeddings for batch: 12\n",
      "Saved embeddings for batch: 13\n",
      "Saved embeddings for batch: 14\n",
      "Saved embeddings for batch: 15\n",
      "Saved embeddings for batch: 16\n",
      "Saved embeddings for batch: 17\n",
      "Saved embeddings for batch: 18\n",
      "Saved embeddings for batch: 19\n",
      "Saved embeddings for batch: 20\n",
      "Saved embeddings for batch: 21\n",
      "Saved embeddings for batch: 22\n",
      "Saved embeddings for batch: 23\n",
      "Saved embeddings for batch: 24\n",
      "Saved embeddings for batch: 25\n",
      "Saved embeddings for batch: 26\n",
      "Saved embeddings for batch: 27\n",
      "Saved embeddings for batch: 28\n",
      "Saved embeddings for batch: 29\n",
      "Saved embeddings for batch: 30\n",
      "Saved embeddings for batch: 31\n",
      "Saved embeddings for batch: 32\n",
      "Saved embeddings for batch: 33\n",
      "Saved embeddings for batch: 34\n",
      "Saved embeddings for batch: 35\n",
      "Saved embeddings for batch: 36\n",
      "Saved embeddings for batch: 37\n",
      "Saved embeddings for batch: 38\n",
      "Saved embeddings for batch: 39\n",
      "Saved embeddings for batch: 40\n",
      "Saved embeddings for batch: 41\n",
      "Saved embeddings for batch: 42\n",
      "Saved embeddings for batch: 43\n",
      "Saved embeddings for batch: 44\n",
      "Saved embeddings for batch: 45\n",
      "Saved embeddings for batch: 46\n",
      "Saved embeddings for batch: 47\n",
      "Saved embeddings for batch: 48\n",
      "Saved embeddings for batch: 49\n",
      "Saved embeddings for batch: 50\n",
      "Saved embeddings for batch: 51\n",
      "Saved embeddings for batch: 52\n",
      "Saved embeddings for batch: 53\n",
      "Saved embeddings for batch: 54\n",
      "Saved embeddings for batch: 55\n",
      "Saved embeddings for batch: 56\n",
      "Saved embeddings for batch: 57\n",
      "Saved embeddings for batch: 58\n",
      "Saved embeddings for batch: 59\n",
      "Saved embeddings for batch: 60\n",
      "Saved embeddings for batch: 61\n",
      "Saved embeddings for batch: 62\n",
      "Saved embeddings for batch: 63\n",
      "Saved embeddings for batch: 64\n",
      "Saved embeddings for batch: 65\n",
      "Saved embeddings for batch: 66\n",
      "Saved embeddings for batch: 67\n",
      "Saved embeddings for batch: 68\n",
      "Saved embeddings for batch: 69\n",
      "Saved embeddings for batch: 70\n",
      "Saved embeddings for batch: 71\n",
      "Saved embeddings for batch: 72\n",
      "Saved embeddings for batch: 73\n",
      "Saved embeddings for batch: 74\n",
      "Saved embeddings for batch: 75\n",
      "Saved embeddings for batch: 76\n",
      "Saved embeddings for batch: 77\n",
      "Saved embeddings for batch: 78\n",
      "Saved embeddings for batch: 79\n",
      "Saved embeddings for batch: 80\n",
      "Saved embeddings for batch: 81\n",
      "Saved embeddings for batch: 82\n",
      "Saved embeddings for batch: 83\n",
      "Saved embeddings for batch: 84\n",
      "Saved embeddings for batch: 85\n",
      "Saved embeddings for batch: 86\n",
      "Saved embeddings for batch: 1\n",
      "Saved embeddings for batch: 2\n",
      "Saved embeddings for batch: 3\n",
      "Saved embeddings for batch: 4\n",
      "Saved embeddings for batch: 5\n",
      "Saved embeddings for batch: 6\n",
      "Saved embeddings for batch: 7\n",
      "Saved embeddings for batch: 8\n",
      "Saved embeddings for batch: 9\n",
      "Saved embeddings for batch: 10\n",
      "Saved embeddings for batch: 11\n",
      "Saved embeddings for batch: 12\n",
      "Saved embeddings for batch: 13\n",
      "Saved embeddings for batch: 14\n",
      "Saved embeddings for batch: 15\n",
      "Saved embeddings for batch: 16\n",
      "Saved embeddings for batch: 17\n",
      "Saved embeddings for batch: 18\n",
      "Saved embeddings for batch: 19\n",
      "Saved embeddings for batch: 20\n",
      "Saved embeddings for batch: 21\n",
      "Saved embeddings for batch: 22\n",
      "Saved embeddings for batch: 23\n",
      "Saved embeddings for batch: 24\n",
      "Saved embeddings for batch: 25\n",
      "Saved embeddings for batch: 26\n",
      "Saved embeddings for batch: 27\n",
      "Saved embeddings for batch: 28\n",
      "Saved embeddings for batch: 29\n",
      "Saved embeddings for batch: 30\n",
      "Saved embeddings for batch: 31\n",
      "Saved embeddings for batch: 32\n",
      "Saved embeddings for batch: 33\n",
      "Saved embeddings for batch: 34\n",
      "Saved embeddings for batch: 35\n",
      "Saved embeddings for batch: 36\n",
      "Saved embeddings for batch: 37\n",
      "Saved embeddings for batch: 38\n",
      "Saved embeddings for batch: 39\n",
      "Saved embeddings for batch: 40\n",
      "Saved embeddings for batch: 41\n",
      "Saved embeddings for batch: 42\n",
      "Saved embeddings for batch: 43\n",
      "Saved embeddings for batch: 44\n",
      "Saved embeddings for batch: 45\n",
      "Saved embeddings for batch: 46\n",
      "Saved embeddings for batch: 47\n",
      "Saved embeddings for batch: 48\n",
      "Saved embeddings for batch: 49\n",
      "Saved embeddings for batch: 50\n",
      "Saved embeddings for batch: 51\n",
      "Saved embeddings for batch: 52\n",
      "Saved embeddings for batch: 53\n",
      "Saved embeddings for batch: 54\n",
      "Saved embeddings for batch: 55\n",
      "Saved embeddings for batch: 56\n",
      "Saved embeddings for batch: 57\n",
      "Saved embeddings for batch: 58\n",
      "Saved embeddings for batch: 59\n",
      "Saved embeddings for batch: 60\n",
      "Saved embeddings for batch: 61\n",
      "Saved embeddings for batch: 62\n",
      "Saved embeddings for batch: 63\n",
      "Saved embeddings for batch: 64\n",
      "Saved embeddings for batch: 65\n",
      "Saved embeddings for batch: 66\n",
      "Saved embeddings for batch: 67\n",
      "Saved embeddings for batch: 68\n",
      "Saved embeddings for batch: 69\n",
      "Saved embeddings for batch: 70\n",
      "Saved embeddings for batch: 71\n",
      "Saved embeddings for batch: 72\n",
      "Saved embeddings for batch: 73\n",
      "Saved embeddings for batch: 74\n",
      "Saved embeddings for batch: 75\n",
      "Saved embeddings for batch: 76\n",
      "Saved embeddings for batch: 77\n",
      "Saved embeddings for batch: 78\n",
      "Saved embeddings for batch: 79\n",
      "Saved embeddings for batch: 80\n",
      "Saved embeddings for batch: 81\n",
      "Saved embeddings for batch: 82\n",
      "Saved embeddings for batch: 83\n",
      "Saved embeddings for batch: 84\n",
      "Saved embeddings for batch: 85\n",
      "Saved embeddings for batch: 86\n",
      "Saved embeddings for batch: 87\n"
     ]
    }
   ],
   "source": [
    "save_embeddings_hdf5(train_loader, \"train_embeddings.h5\")\n",
    "save_embeddings_hdf5(val_loader, \"val_embeddings.h5\")\n",
    "save_embeddings_hdf5(test_loader, \"test_embeddings.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39ae1d98-9812-4221-afa4-17432bf7a7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "504d8865-b072-44d8-bec1-2d1af005019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_hdf5(filename, batch_size=4):\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        num_samples = f[\"embeddings\"].shape[0]  # Total samples\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            X_batch = torch.tensor(f[\"embeddings\"][i : i + batch_size], dtype=torch.float32).to(DEVICE)\n",
    "            y_batch = torch.tensor(f[\"labels\"][i : i + batch_size], dtype=torch.long).to(DEVICE)\n",
    "            yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ebd750f-ea7f-4567-a9ba-681ba901667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1024)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(1024)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(512)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(256)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc4 = nn.Linear(256, num_classes)  # Final layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        if x.shape[0] > 1:  # Apply BatchNorm only if batch size > 1\n",
    "            x = self.batch_norm1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "    \n",
    "        x = self.fc2(x)\n",
    "        if x.shape[0] > 1:\n",
    "            x = self.batch_norm2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "    \n",
    "        x = self.fc3(x)\n",
    "        if x.shape[0] > 1:\n",
    "            x = self.batch_norm3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9af434c0-713f-4747-add1-0dab59e5a131",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = next(load_embeddings_hdf5(\"train_embeddings.h5\"))[0].shape[1]  # Get feature size\n",
    "num_classes = 3  # Adjust based on labels\n",
    "model = MLPClassifier(input_dim, num_classes).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "376a2515-00cd-49fb-8c3d-a5d6a333e8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(\n",
       "  (fc1): Linear(in_features=393216, out_features=1024, bias=True)\n",
       "  (batch_norm1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (batch_norm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (batch_norm3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU()\n",
       "  (dropout3): Dropout(p=0.5, inplace=False)\n",
       "  (fc4): Linear(in_features=256, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53eb668e-d4e1-4963-b601-55afd0f07f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights\n",
    "# classes = np.unique(train_labels)\n",
    "# class_weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=train_labels)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights).to(DEVICE)).to(DEVICE)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e834bcc3-7120-45dd-bfd3-bbae86e4d540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "#     model = torch.nn.DataParallel(model)\n",
    "\n",
    "# model = model.to(\"cuda\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a8a0a0e-c841-46df-9644-88bf0b8a4420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "`torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "`torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.                         | 0/401 [00:00<?, ?it/s]\n",
      "Epoch 1/100 Train: 402it [00:41,  9.60it/s, acc=35.5, loss=86.4]                                                                                            \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 113.00it/s, acc=48.3, loss=0.992]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]\n",
      "  Train Loss: 0.3447 | Train Acc: 35.45%\n",
      "  Val Loss: 0.2701 | Val Acc: 48.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 Train: 402it [00:42,  9.54it/s, acc=38.4, loss=44.3]                                                                                            \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 122.26it/s, acc=47.7, loss=0.994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100]\n",
      "  Train Loss: 0.3108 | Train Acc: 38.44%\n",
      "  Val Loss: 0.2680 | Val Acc: 47.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 Train: 402it [00:42,  9.54it/s, acc=40, loss=148]                                                                                               \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 117.28it/s, acc=50.9, loss=0.929]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100]\n",
      "  Train Loss: 0.3725 | Train Acc: 40.00%\n",
      "  Val Loss: 0.2659 | Val Acc: 50.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 Train: 402it [00:42,  9.54it/s, acc=41.4, loss=168]                                                                                             \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 119.32it/s, acc=51.2, loss=0.922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100]\n",
      "  Train Loss: 0.3794 | Train Acc: 41.37%\n",
      "  Val Loss: 0.2660 | Val Acc: 51.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 Train: 402it [00:42,  9.52it/s, acc=44.1, loss=0]                                                                                               \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 122.96it/s, acc=52.3, loss=0.892]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100]\n",
      "  Train Loss: 0.2695 | Train Acc: 44.11%\n",
      "  Val Loss: 0.2650 | Val Acc: 52.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 Train: 402it [00:42,  9.52it/s, acc=44.7, loss=29.8]                                                                                            \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 115.52it/s, acc=53.5, loss=0.885]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100]\n",
      "  Train Loss: 0.2857 | Train Acc: 44.67%\n",
      "  Val Loss: 0.2631 | Val Acc: 53.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 Train: 402it [00:42,  9.49it/s, acc=46.4, loss=9.54e-7]                                                                                         \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 117.70it/s, acc=52.3, loss=0.833]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100]\n",
      "  Train Loss: 0.2601 | Train Acc: 46.42%\n",
      "  Val Loss: 0.2636 | Val Acc: 52.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 Train: 402it [00:42,  9.51it/s, acc=51.1, loss=0]                                                                                               \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 116.57it/s, acc=52.6, loss=0.816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100]\n",
      "  Train Loss: 0.2542 | Train Acc: 51.09%\n",
      "  Val Loss: 0.2617 | Val Acc: 52.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 Train: 402it [00:42,  9.53it/s, acc=49.8, loss=57.9]                                                                                            \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 119.78it/s, acc=51.2, loss=0.748]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100]\n",
      "  Train Loss: 0.2895 | Train Acc: 49.78%\n",
      "  Val Loss: 0.2619 | Val Acc: 51.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 Train: 402it [00:42,  9.53it/s, acc=52.9, loss=419]                                                                                            \n",
      "Validation:  21%|██████████████████▏                                                                  | 86/401 [00:00<00:02, 118.70it/s, acc=52, loss=0.758]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100]\n",
      "  Train Loss: 0.5048 | Train Acc: 52.90%\n",
      "  Val Loss: 0.2626 | Val Acc: 52.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 Train: 402it [00:42,  9.53it/s, acc=51.2, loss=171]                                                                                            \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 120.42it/s, acc=52.3, loss=0.797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100]\n",
      "  Train Loss: 0.3544 | Train Acc: 51.21%\n",
      "  Val Loss: 0.2613 | Val Acc: 52.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 Train: 402it [00:42,  9.53it/s, acc=51.6, loss=578]                                                                                            \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 117.21it/s, acc=53.8, loss=0.737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100]\n",
      "  Train Loss: 0.6055 | Train Acc: 51.59%\n",
      "  Val Loss: 0.2594 | Val Acc: 53.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 Train: 402it [00:42,  9.54it/s, acc=55, loss=0]                                                                                                \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 119.88it/s, acc=54.4, loss=0.757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100]\n",
      "  Train Loss: 0.2420 | Train Acc: 55.02%\n",
      "  Val Loss: 0.2584 | Val Acc: 54.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 Train: 402it [00:42,  9.52it/s, acc=56.7, loss=159]                                                                                            \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 115.83it/s, acc=54.1, loss=0.726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100]\n",
      "  Train Loss: 0.3335 | Train Acc: 56.70%\n",
      "  Val Loss: 0.2576 | Val Acc: 54.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100 Train: 402it [00:42,  9.52it/s, acc=56.1, loss=0]                                                                                              \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 120.69it/s, acc=54.4, loss=0.726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100]\n",
      "  Train Loss: 0.2335 | Train Acc: 56.14%\n",
      "  Val Loss: 0.2554 | Val Acc: 54.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 Train: 402it [00:42,  9.49it/s, acc=56.7, loss=0]                                                                                              \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 117.56it/s, acc=55.2, loss=0.759]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100]\n",
      "  Train Loss: 0.2307 | Train Acc: 56.70%\n",
      "  Val Loss: 0.2529 | Val Acc: 55.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100 Train: 402it [00:42,  9.50it/s, acc=59.7, loss=0]                                                                                              \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 118.32it/s, acc=55.2, loss=0.688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100]\n",
      "  Train Loss: 0.2227 | Train Acc: 59.69%\n",
      "  Val Loss: 0.2538 | Val Acc: 55.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100 Train: 402it [00:42,  9.49it/s, acc=59.9, loss=0]                                                                                              \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 120.49it/s, acc=55.5, loss=0.732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100]\n",
      "  Train Loss: 0.2245 | Train Acc: 59.94%\n",
      "  Val Loss: 0.2530 | Val Acc: 55.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 Train: 402it [00:42,  9.54it/s, acc=62.5, loss=0]                                                                                              \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 120.17it/s, acc=56.1, loss=0.679]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100]\n",
      "  Train Loss: 0.2178 | Train Acc: 62.49%\n",
      "  Val Loss: 0.2514 | Val Acc: 56.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 Train: 402it [00:42,  9.55it/s, acc=64.8, loss=238]                                                                                            \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 116.33it/s, acc=55.8, loss=0.713]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100]\n",
      "  Train Loss: 0.3581 | Train Acc: 64.80%\n",
      "  Val Loss: 0.2516 | Val Acc: 55.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100 Train: 402it [00:42,  9.52it/s, acc=65, loss=0]                                                                                                \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 118.51it/s, acc=58.4, loss=0.686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100]\n",
      "  Train Loss: 0.2068 | Train Acc: 65.05%\n",
      "  Val Loss: 0.2485 | Val Acc: 58.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100 Train: 402it [00:42,  9.52it/s, acc=65.2, loss=0]                                                                                              \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 117.96it/s, acc=58.4, loss=0.697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100]\n",
      "  Train Loss: 0.2070 | Train Acc: 65.17%\n",
      "  Val Loss: 0.2485 | Val Acc: 58.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100 Train: 402it [00:42,  9.51it/s, acc=66.2, loss=0]                                                                                              \n",
      "Validation:  21%|██████████████████▏                                                                  | 86/401 [00:00<00:02, 118.37it/s, acc=57, loss=0.598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100]\n",
      "  Train Loss: 0.1984 | Train Acc: 66.17%\n",
      "  Val Loss: 0.2478 | Val Acc: 56.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100 Train: 402it [00:42,  9.52it/s, acc=67.5, loss=0]                                                                                              \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 120.44it/s, acc=57.8, loss=0.657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100]\n",
      "  Train Loss: 0.1926 | Train Acc: 67.48%\n",
      "  Val Loss: 0.2461 | Val Acc: 57.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100 Train: 402it [00:42,  9.52it/s, acc=68.8, loss=525]                                                                                            \n",
      "Validation:  21%|██████████████████▋                                                                    | 86/401 [00:00<00:02, 119.30it/s, acc=57, loss=0.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100]\n",
      "  Train Loss: 0.5180 | Train Acc: 68.85%\n",
      "  Val Loss: 0.2453 | Val Acc: 56.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100 Train: 402it [00:42,  9.51it/s, acc=71.4, loss=0]                                                                                              \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 120.79it/s, acc=57.3, loss=0.594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100]\n",
      "  Train Loss: 0.1834 | Train Acc: 71.40%\n",
      "  Val Loss: 0.2482 | Val Acc: 57.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100 Train: 402it [00:42,  9.51it/s, acc=70.3, loss=0]                                                                                              \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 115.30it/s, acc=57.8, loss=0.624]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100]\n",
      "  Train Loss: 0.1822 | Train Acc: 70.28%\n",
      "  Val Loss: 0.2456 | Val Acc: 57.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100 Train: 402it [00:42,  9.52it/s, acc=71.6, loss=0]                                                                                              \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 122.21it/s, acc=57.6, loss=0.629]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100]\n",
      "  Train Loss: 0.1817 | Train Acc: 71.59%\n",
      "  Val Loss: 0.2444 | Val Acc: 57.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100 Train: 402it [00:42,  9.52it/s, acc=72.1, loss=0]                                                                                              \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 122.01it/s, acc=57.3, loss=0.587]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100]\n",
      "  Train Loss: 0.1765 | Train Acc: 72.09%\n",
      "  Val Loss: 0.2442 | Val Acc: 57.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100 Train: 402it [00:42,  9.52it/s, acc=73.8, loss=0]                                                                                              \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 121.41it/s, acc=57.6, loss=0.589]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100]\n",
      "  Train Loss: 0.1743 | Train Acc: 73.77%\n",
      "  Val Loss: 0.2458 | Val Acc: 57.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100 Train: 402it [00:42,  9.51it/s, acc=73.2, loss=0]                                                                                              \n",
      "Validation:  21%|█████████████████▊                                                                 | 86/401 [00:00<00:02, 121.66it/s, acc=58.1, loss=0.577]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 31. No improvement in validation accuracy for 10 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm  # Progress tracking\n",
    "\n",
    "NUM_GPUS = 1\n",
    "\n",
    "# Compute class weights\n",
    "def compute_class_weights(y_train):\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "    return torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "y_train = np.concatenate([y.cpu().numpy() for _, y in load_embeddings_hdf5(\"train_embeddings.h5\", batch_size=4)])\n",
    "class_weights = compute_class_weights(y_train)\n",
    "\n",
    "# Define optimizer, scheduler, and scaler\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "num_epochs = 100\n",
    "patience = 10  # Early stopping patience\n",
    "best_val_acc = 0.0\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    # Training loop\n",
    "    train_loader = load_embeddings_hdf5(\"train_embeddings.h5\", batch_size=4 * NUM_GPUS)  # Adjust batch size\n",
    "    train_bar = tqdm(train_loader, total=len(y_train) // (4 * NUM_GPUS), desc=f\"Epoch {epoch+1}/{num_epochs} Train\")\n",
    "    for X_batch, y_batch in train_bar:\n",
    "        X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():  # Mixed precision\n",
    "            outputs = model(X_batch)\n",
    "            loss = F.cross_entropy(outputs, y_batch, weight=class_weights)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "        \n",
    "        train_bar.set_postfix(loss=loss.item(), acc=100 * correct / total)\n",
    "\n",
    "    train_accuracy = 100 * correct / total\n",
    "    avg_train_loss = train_loss / total\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "    val_loader = load_embeddings_hdf5(\"val_embeddings.h5\", batch_size=4 * NUM_GPUS)\n",
    "    val_bar = tqdm(val_loader, total=len(y_train) // (4 * NUM_GPUS), desc=\"Validation\")\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_bar:\n",
    "            X_val, y_val = X_val.to(DEVICE), y_val.to(DEVICE)\n",
    "            outputs = model(X_val)\n",
    "            loss = F.cross_entropy(outputs, y_val, weight=class_weights)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_correct += predicted.eq(y_val).sum().item()\n",
    "            val_total += y_val.size(0)\n",
    "\n",
    "            val_bar.set_postfix(loss=loss.item(), acc=100 * val_correct / val_total)\n",
    "    \n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    avg_val_loss = val_loss / val_total\n",
    "\n",
    "    # Check for early stopping based on accuracy\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        epochs_without_improvement = 0  # Reset counter\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")  # Save best model\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    # Stop if no improvement for 10 epochs\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}. No improvement in validation accuracy for {patience} epochs.\")\n",
    "        break\n",
    "\n",
    "    # Learning rate scheduler step based on validation accuracy\n",
    "    scheduler.step(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}%\")\n",
    "    print(f\"  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d024b1eb-13bb-41ee-8713-81f9db73ccc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5507\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in load_embeddings_hdf5(\"test_embeddings.h5\", batch_size=4):\n",
    "        outputs = model(X_batch)\n",
    "        predicted_labels = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        y_true.extend(y_batch.cpu().numpy())\n",
    "        y_pred.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "# Compute accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
